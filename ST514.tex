%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 1.4 (12/4/14)
%
% This template has been downloaded from:
% http://www.LaTeXTembplates.com
%
% Original author:
% Mathias Legrand (legrand.mathias@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\usepackage[top=3cm,bottom=3cm,left=3.2cm,right=3.2cm,headsep=10pt,a4paper]{geometry} % Page margins

\usepackage{xcolor} % Required for specifying colors by name
\definecolor{ocre}{RGB}{243,102,25} % Define the orange color used for highlighting throughout the book
\usepackage{amsmath} %Required for some mathmatical fomulas 
\usepackage{enumitem} %lists
% Font Settings
\usepackage{avant} % Use the Avantgarde font for headings
%\usepackage{times} % Use the Times font for headings
\usepackage{mathptmx} % Use the Adobe Times Roman as the default text font together with math symbols from the Sym­bol, Chancery and Com­puter Modern fonts

\usepackage{bm}
\usepackage{bbm}
%\usepackage{parskip}

\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs

% Bibliography
\usepackage[style=alphabetic,sorting=nyt,sortcites=true,autopunct=true,babel=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
\addbibresource{bibliography.bib} % BibTeX bibliography file
\defbibheading{bibempty}{}

% Index
\usepackage{calc} % For simpler calculation - used for spacing the index letter headings correctly
\usepackage{makeidx} % Required to make an index
\makeindex % Tells LaTeX to create the files required for indexing

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\AddToShipoutPicture*{\put(6,5){\includegraphics[scale=1]{background}}} % Image background
\centering
\vspace*{9cm}
\par\normalfont\fontsize{35}{35}\sffamily\selectfont
STAT 514 Lecture Notes\par % Book title
\vspace*{1cm}
{\Huge Dr. David Hunter}\par % Author name
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2014 John Smith\\ % Copyright notice

\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \textsc{book-website.com}\\ % URL

\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Point Estimation - Chapter 7 Casella \& Berger}

\section{Introduction}\index{Paragraphs of Text}

	In the simplest case, we have $n$ observations of data that we believe follow the same distribution.
	$$
		X_1, \dots, X_n \stackrel{iid}{\sim} f_\theta(x)
	$$

	where $f_\theta(x)$ is a density function involving a parameter $\theta$. Our goal is to learn something about $\theta$, which could be real or vector valued.\\

	\begin{definition}[Estimator]
		An \emph{estimator} of $\theta$ is any function $W(X_1, \dots, X_n)$ of the data. That is, an estimator is a \emph{statistic}.
	\end{definition}

	Note:
	\begin{enumerate}
		\item $W(\bm{X})$ may not depend on $\theta$.
		\item $W(\bm{X})$ should resemble or ``be close'' to $\theta$.
		\item An estimator is \emph{random}.
		\item $W(X_1, \dots, X_n)$ is the estimator, $W(x_1, \dots, x_n)$ is the fixed estimate.
	\end{enumerate}

	\begin{example}
		Suppose we have $n$ observations from an exponential distribution,
		$$
			X_1, \dots, X_n \stackrel{iid}{\sim} f_\theta(x)=\frac{1}{\theta}\exp\left\{-\frac{x}{\theta}\right\}\mathbbm{1}\{x>0\}
		$$
		for some $\theta > 0$. The \textbf{likelihood function} is equivalent to the joint density function, expressed as a function of $\theta$ rather than the data:
		$$
			L(\theta) = \prod_{i=1}^n \frac{1}{\theta}\exp\left\{-\frac{x}{\theta}\right\} = \frac{1}{\theta^n}\exp\left\{-\frac{1}{\theta}\sum_{i=1}^n x_i\right\}
		$$
		This function represents the \emph{likelihood} of observing the data we observed assuming the parameter was a particular value of $\theta$. If we can maximize this function, we can determine the $\hat{\theta}$ for which the likelihood of observing $\bm{X}$ was the highest. This might tell us something about the true value of $\theta$.

		\indent To maximize $L(\theta)$, we want to take the derivative, set it equal to 0, and solve for $\theta$. However, in many cases taking the derivative of the likelihood function will be very hard, if not impossible. We can use the fact that taking the logarithm does not change the location of extrema. The \textbf{log-likelihood function} in this case is
		\begin{equation*}
			\ell(\theta) = \log L(\theta) = -n \log \theta - \frac{1}{\theta} \sum_{i=1}^n x_i
		\end{equation*}
		Take the derivative with respect to the parameter and set equal to 0:
		\begin{eqnarray*}
			\ell'(\theta) &=& -\frac{n}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^n x_i \enspace \stackrel{\textrm{set}}{=} \enspace 0\\
			\hat{\theta} &=& \frac{1}{n} \sum_{i=1}^n x_i
		\end{eqnarray*}
		Here $\hat{\theta}$ is an estimator (the sample mean). Since it maximizes $L(\theta)$, we call it the \textbf{maximum likelihood estimator} (MLE).
	\end{example}


\begin{section}{Mean Squared Error}

\begin{remark} 
	Read Casella \& Berger Chapter 7.3 - Methods of Evaluating Estimation
\end{remark} 


\begin{definition}[Mean Squared Error]

If $W(\bm{X})$ is an estimator of $\theta$, then the \textbf{mean squared error} (MSE) is defined as
\[
	E_\theta\left[(W(\bm{X}) - \theta)^2\right].
\]

\end{definition}

\begin{definition}[Unbiased estimator]

If $W(\bm{X})$ is an estimator of $\theta$, we say that $W(\bm{X})$ is \textbf{unbiased} if
\[
	E_\theta[W(\bm{X})] = \theta \quad \forall \theta.
\]
Furthermore, the \textbf{bias} of $W(\bm{X})$ is
\[
	E_\theta[W(\bm{X})] - \theta.
\]

\end{definition}

\begin{example}
	For $\theta$ > 0, let
	$$
		X_1, \dots, X_n \stackrel{iid}{\sim} f_\theta(x)=\theta x^{-2} \mathbbm{1}\{x>\theta\}
	$$
Find the MLE of $\theta$.
$$
	L(\theta)=\theta^n \prod_{i=1}^n x_i^{-2} \prod_{i=1}^n \mathbbm{1}\{x>\theta\}$$
	\\
	$$\hat{\theta}=\text{minimum of }x_i
$$
\end{example}

\begin{theorem}
$\textrm{MSE}(W) = \textrm{bias}^2 + \textrm{Var}(W)$
\end{theorem}

Proof:
\begin{eqnarray*}
	E[(W(\bm{X})-\theta)^2] &=& E[(W-E[W]+E[W]-\theta)^2]\\
							&=& E[(W-E[W])^2]+E[(E[W]-\theta)^2]+2E[(W-E[W])(E[W]-\theta)]\\
							&=& \textrm{Var}(W) + \textrm{bias}^2(W) + 0
\end{eqnarray*}

\end{section}

\begin{section}{Best Unbias Estimator}
	What does best mean? Answer: Minimum variance. \\
\\
	Recall: Given an esitmator $W(\underline{X})$ for $\theta$,
	\[
		MSE(\theta)= E(W(\underline{X})-\theta)^2)
	\]
\begin{definition}[Best Unbiased Estimator]
	An estimator W* is a \textbf{best unbiased estimator}* of $\tau(\theta)$ if it satisties
	\[
		E_\theta (W*) = \tau(\theta)
	\] 
	for all $\theta$ and, for any other estimator W with 
	\[
		E_\theta (W) = \tau(\theta)
	\]
	we have 
	\[
		\text{Var}_\theta (W*) \le \text{Var}_\theta (W)
	\]
	for all $\theta$. W* is also called a $\textit{uniform minimum variance unbiased estimator}$ (UMVUE) of $\tau(\theta)$.
\end{definition}
Main Result: Under some assumptions we can establish a lower bound on Var(W(X)). 

\begin{theorem}[Cremer-Rao Inequality]
	Also: Information Inequality. 
	$$\text{Var}(W(\underline{X})) \ge \frac{(\Psi^\prime(\theta))^2}{I(\theta)}$$
	where, 
	$\Psi(\theta)=E_\theta(W(\underline{X}))$\\
	and, 
	the Fisher/Expected information, $I(\theta)=E\left((\frac{\delta}{\delta \theta}\log f_\theta(\underline{X}))^2 \right)$.
	
\end{theorem}
Proof: Follows from Cauchy-Schwarz Inequality

$$Cov(W(\underline{x}), \frac{\delta}{\delta\theta}\log f_\theta(\underline{X})))^2 \le \text{Var}(W(\underline{X}))*\text{Var}(\frac{\delta}{\delta\theta}\log f_\theta(\underline{X})) $$

Assumptions:
\begin{enumerate}
	\item I($\theta)=E_\theta \left((\frac{\delta}{\delta \theta}\log f_\theta(\underline{X})^2)\right) = Var((\frac{\delta}{\delta \theta}\log f_\theta(\underline{X})^2))$ is well defined and I($\theta)>0$.
	\item $E \left((\frac{\delta}{\delta \theta}\log f_\theta(\underline{X}))^2 \right)=0$ Thus, I($\theta$) is just varience.
	\item $E \left((W(\underline{X})\frac{\delta}{\delta \theta}\log f_\theta(\underline{X}))^2 \right)=\Psi^\prime(\theta)$
\end{enumerate}
So, \\
$Cov(W(\underline{x}), \frac{\delta}{\delta\theta}\log f_\theta(\underline{X})))^2 \le \text{Var}(W(\underline{X}))*\text{Var}(\frac{\delta}{\delta\theta}\log f_\theta(\underline{X})) \simeq \text{Var}(W(\underline{X})) \ge \frac{(\Psi^\prime(\theta))^2}{I(\theta)}$
\begin{exercise}
	Let $\underline{X} \sim Poi(\theta)$, Y=$\sum x_i$, and Y $\sim Poi(n\theta)$. What is $I(\theta)$?

	\begin{align*}
		I(\theta) &= E\left((\frac{\delta}{\delta \theta}\log f_\theta(\underline{X})^2) \right)\\
				&=E\left((\frac{\delta}{\delta \theta}\log \prod \frac{\theta^x e^{-\theta}}{x!})^2) \right)\\
				&= E\left((\frac{\delta}{\delta \theta}\log \frac{\theta^{\sum x_i} e^{-\theta n}}{\sum x_i!})^2 \right)\\
				&= E\left(\frac{\delta}{\delta \theta}(-n\theta + \sum x_i \log \theta - \sum \log x_i!)^2  \right)\\
				&= E\left((-n+\frac{\sum x_i}{\theta})^2 \right)\\
				&= E\left( n^2 - 2(\frac{\sum x_i}{\theta})(n)+(\frac{\sum x_i}{\theta})^2 \right)\\
				&= n^2 -\frac{2n * E(\sum x_i)}{\theta} + \frac{E((\sum x_i)^2)}{\theta^2}\\
				\\
				&=n^2 - \frac{2n*E(Y)}{\theta} + \frac{E(Y)^2}{\theta}\\
				&=\frac{n}{\theta}
	\end{align*}
\end{exercise}

Note: I($\theta$) is equal to information in the whole sample, but sometimes it's just one sample (based on context).

If we assume (as in any exponential family):

$$E_\theta \left(\frac{\delta^2}{\delta \theta^2}\log f_\theta(\underline{x})\right) = \frac{\delta^2}{\delta \theta^2}\int{\log f_{\theta}(\underline{x})}f_\theta(\underline{x})dx$$
  
then, the observed information is 
$$I(\theta) = - E_\theta \left(\frac{\delta^2}{\delta \theta^2}\log f_\theta(\underline{x})\right) $$

\begin{example}
	Let $X_i \dots X_n \stackrel{iid}{\sim} N(\mu,\sigma^2)$
Find the information based on $\sigma^2$.

	Write 
\begin{align*}
	\ell(\mu,\sigma^2) &=\log\left(\prod (2\pi\sigma^2)^{\frac{1}{2}} \exp\left\{\frac{-(x-\mu)^2}{2\sigma^2}\right\}\right)\\
	&= \frac{-n}{2} \log(2\pi\sigma^2) +\sum \left(\frac{-(x_i-\mu)^2}{2\sigma^2} \right)
	\end{align*}

If we try to use the expected information: 

$I(\sigma^2) = E\left(\frac{\delta}{\delta \theta}\left(\frac{-n}{2} \log(2\pi\sigma^2) +\sum \left(\frac{-(x_i-\mu)^2}{2\sigma^2}\right)\right)^2 \right)$
which is a mess. 

However, using the observed information:

\begin{align*}
I(\sigma^2) &= \frac{-n}{2\sigma^4} + \frac{1}{\sigma^6} E\left(\sum(X_i-\mu)^2\right)\\
	&= \frac{-n}{2\sigma^4}+\frac{n\sigma^2}{\sigma^6}\\
	&= \frac{n}{2\sigma^4}
\end{align*}

\end{example}

\begin{example}
	Continued from previous example. 

	Define $S^2 = \frac{1}{n-1}\sum (X_i - \bar{X})^2$

	Note: $\frac{n-1}{\sigma^2}S^2 \sim \chi^2_{n-1}$

	Does $S^2$ acheive the C-R lower bound?

	\begin{align*}
	Var(S^2) &\ge \frac{\Psi^\prime(\theta)}{I(\theta)}\\
		\frac{2\sigma^4}{n-1} &\ge \frac{1}{I(\theta)}\\
		&\ge \frac{2\sigma^4}{n}
	\end{align*}
\end{example}
\begin{remark}
	Reread 7.3 and 6.2 in Casella and Berger
\end{remark}
What would give equality? When W($\underline{X}$) is a linear function of $\frac{\delta}{\delta \theta} \log f_\theta(\underline{X})$ which leads to... 
\begin{corollary}[Attainment]
	Let $X_1,\dots,X_n$ be iid f(x|$\theta$), where f(x|$\theta$) satisfies the conditions of the Cramer-Rao Theorem. Let $L(\theta|x) = \prod f(x_i|\theta)$ denote the likelihood function. If $W(X) = W(X_1,\dots,X_n)$ is any unbiased estimator of $\tau(\theta)$, then W(X) attains the Cramer-Rao Lower Bound iff
	$$ a(\theta)\left(W(x) - \tau(\theta)\right) = \frac{\delta}{\delta \theta} \log L(\theta|x) $$
	for some function a($\theta$).
\end{corollary}
\end{section}

\begin{section}{Lost Function Optimality}
	
	\begin{definition}[Loss] L($\theta$, W($\underline{x}$)) assigns a nonnegative real value called the \textbf{loss} to our decision to estimate $\theta$ by W($\underline{X}$). General context: Decision Theory. 
	\end{definition}

Typically L($\theta$,$\theta$) = 0 because nothing is lost if your decision is exactly correct. 

\begin{example}
	\begin{eqnarray*}
		L(\theta, W(\underline{X})) &=& (\theta-W(X)^2) \text{\indent square error loss}\\
\\
		&=& | \theta -W(X)| \text{   \indent absolute error loss}\\
\\
		&=& \frac{W(X)}{\theta}-1-\log \frac{W(X)}{\theta} \text{  \indent Stein's loss}
	\end{eqnarray*}
\end{example}

\begin{definition}[Risk] \textbf{Risk} of estimating $\theta$ by W($\underline{X}$) is
	\[
	R(\theta,W)=E\left(\theta, W(\underline{X} \right) )
	\]
	
\end{definition}

\begin{exercise}
If $X_1, \dots, X_n$ are iid with mean $\mu$ and varience $\sigma^2$ what is
$$
E \left(\sum_{i=1}^n (X_i-\bar{X})^2 \right)?
$$

Note:
\begin{enumerate}
		\item $\sum{i=i}^n \left (X_i - \bar{X})^2 \right) = \sum (X_i^2) - n\bar{X}$
		\item Var(X) = $E(X^2)-E(X)^2$
		\item Var$(\frac{X_i}{n})=\frac{\sigma^2}{n^2}$
		\item Var$(\bar{X})=\frac{\sigma^2}{n}$
	\end{enumerate}	

Thus,
$$E (\sum X_i^2) = (\sigma^2 + \mu^2)n$$
$$-n E(\bar{X}^2)= -n(\mu^2+\frac{\sigma^2}{n}) $$
We may conclude, $\frac{1}{n-1}\sum(X_i-\bar{X})^2$ is an unbiased estimator of $\sigma^2$ called $S^2$. 
\end{exercise}

\begin{theorem}[Cauchy-Scwarz Inequality]
	$$||<x,y>||^2 \le ||<x,x>|| * ||<y,y>||$$
In terms of E(X) if A \& B have $\mu$ = 0:
$$(E(AB))^2 \le E(A^2) * E(B^2)$$

Or in terms of covariance:
$$Cov^2(AB) \le Var(A)*Var(B)$$
\end{theorem}
Proof: Let $D = B - \frac{E(AB)}{E(A^2)}A$, given $D^2 \ge 0$.
	\begin{align*}
		E(D^2) &= E(B^2 - 2(B)(\frac{E(AB)}{E(A^2)}A)+(\frac{E(AB)}{E(A^2)}A)^2)\\
			&= E(B^2)-2\left( \frac{E(AB)^2}{E(A^2)} \right) + \frac{E(AB)}{E(A^2)}E(A^2)\\
			&= E(B^2) - \frac{E(AB)^2}{E(A^2)} \ge 0
	\end{align*}

\end{section}

\begin{section}{Practice Problems}

	\begin{problem} (C\&B 7.2) Let $X_1, \dots, X_n$ be a random sample from a Gamma($\alpha, \beta$) population.

		\begin{enumerate}[label=(\alph*)] 
			\item Find the MLE of $\beta$, assuming $\alpha$ is known.
			\item If $\alpha$ and $\beta$ are both unknown, there is no explicit formula for the MLEs of $\alpha$ and $\beta$, but the maximum can be found numberically. The result in part (a) can be used to reduce the problem to the maximization of a univariate function. Find the MLEs for $\alpha$and $\beta$ for the data in Exercise 7.10(c).
		\end{enumerate}

			Solution.

			(a) Gamma distribution pdf: $\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$\\
			\\
			$L(\beta) = \frac{\beta^{n\alpha}}{\Gamma(\alpha)^n}[\prod x]^{\alpha-1}e^{-\beta \sum x}$\\

			$\ell(\beta) = n\alpha\log\beta - n\log\Gamma(\alpha) +(\alpha-1)\log(\prod x_i) - \beta \sum x_i $\\

			$\frac{\delta \ell}{\delta \beta} = \frac{n\alpha}{\beta} - \sum x_i$\\
			\\
			$\hat{\beta} = \frac{n\alpha}{\sum x_i}$

			\begin{remark}
				The density of Gamma can also be in the form with $\frac{1}{\beta}$ which would mean $\hat{\beta}$ would be $\frac{\sum x_i}{n\alpha} $.
			\end{remark} 

			(b) Meh. You do it. 

	\end{problem}
	
	\begin{problem} (C\&B 7.6) Let $X_1, \dots, X_n$ be a random sample from the pdf
	$$f(x|\theta) = \theta x^{-2}, 0<\theta\le x < \infty $$

	\begin{enumerate}[label=(\alph*)]
		\item What is a sufficient statistic for $\theta$?
		\item Find the MLE of $\theta$.
		\item Find the method of moments estimator of $\theta$?
	\end{enumerate}
		

		Solution. 

		(a) Use Factorization Theorem.\\ 
		$L(\theta) = \theta^n \prod(x_i^2) \prod I\{\theta \le x < \infty\}$ where $\prod I\{\theta \le x < \infty = x_{(1)}$\\
		Thus the sufficient statistic is the minimum x. \\
		\\
		(b) L($\theta$|x) = $\theta^n \prod(x_i^2) \prod I\{\theta \le x < \infty \}$. $\theta^n$ is increasing in $\theta$. The second term does not involve $\theta$. So to maximize $L(\theta|x)$, we want to make $\theta$ as large as possible. But because of the indicator function, $L(\theta|x) = 0 if \theta > x(1)$. Thus, $\hat{\theta} = x_{(1)}$.


	\end{problem}

\end{section}

%------------------------------------------------

% \section{Citation}\index{Citation}

% This statement requires citation \cite{book_key}; this one is more specific \cite[122]{article_key}.

% %------------------------------------------------

% \section{Lists}\index{Lists}

% Lists are useful to present information in a concise and/or ordered way\footnote{Footnote example...}.

% \subsection{Numbered List}\index{Lists!Numbered List}

% \begin{enumerate}
% \item The first item
% \item The second item
% \item The third item
% \end{enumerate}

% \subsection{Bullet Points}\index{Lists!Bullet Points}

% \begin{itemize}
% \item The first item
% \item The second item
% \item The third item
% \end{itemize}

% \subsection{Descriptions and Definitions}\index{Lists!Descriptions and Definitions}

% \begin{description}
% \item[Name] Description
% \item[Word] Definition
% \item[Comment] Elaboration
% \end{description}

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapter{Principles of Data Reduction}

\section{Sufficiency Principle}

\begin{definition}{sufficient}
	T(X) is \textbf{sufficient} for $\theta$ if the distribution of X|T($\underline{X}$) does not depend on $\theta$.  
\end{definition}

\begin{theorem}{Factorization Theorem}
	If we have $\underline{X} \sim f_\theta(\underline{x}$ then T is sufficient iff we can write $f_\theta$ as
	$$g(T(x),\theta)h(x)$$
	for some g and h.
\end{theorem}

\begin{example}
	$X_1, \dots, X_n \stackrel{iid}{\sim}$ Bern($\theta$)\\
	pdf: $\theta^x(1-\theta)^{1-x}$

	Joint pdf = $f_\theta(x) = \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}$

	So, T($\underline{x}$) = $\sum x_i$
\end{example}

Q: What is the connection to Section 7.3?\\
A: \begin{align*}
Var_\theta(W(\underline{X})) &= Var\left[E(W(X)|T(X))\right] + E\left[Var(W(X)|T(X)) \right]\\
&\ge Var\left[E(W(X)|T(X))\right]	
\end{align*} 
Note: $E\theta \left\{E_\theta[W(X)|T(X)] \right\} = E_\theta(W(X))$

$E_\theta[W(X)|T(X)]$ does not depend on $\theta$. So this is a legitimate estimator since T(X) is sufficient and its varience is smaller than any estimator with same mean. \\
\\
General Idea of Sufficiency: If T(X) is sufficient for $\theta$, then all information in X about $\theta$ is captured in T(X). \\
\\
More technically, conditioning T(X), the remaining randomness does not depend on $\theta$.

\begin{remark}
	Know Thm 6.2.6 - Factorization Theorem
\end{remark}

\begin{definition}{minimal sufficient}
	 T(X) is $\textbf{minimal sufficient}$ if 
	 \begin{itemize}
	 	\item T(X) is sufficient
	 	\item is a function of any sufficient statistic
	 \end{itemize}
	 Note: "T(X) is a function S(X)" means that S(x) = S(y) implies T(x) = T(y).
\end{definition}

\begin{example}
	 $X_1, \dots, X_n \stackrel{iid}{\sim} N(\mu,\sigma^2)$. Find possible T($\mu,\sigma^2$).\\
	 $f(\underline{x})= \prod (2\pi\sigma^2)^{\frac{1}{2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}$
\end{example}
%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Presenting Information}

\section{Table}\index{Table}

\begin{table}[h]
\centering
\begin{tabular}{l l l}
\toprule
\textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
\midrule
Treatment 1 & 0.0003262 & 0.562 \\
Treatment 2 & 0.0015681 & 0.910 \\
Treatment 3 & 0.0009271 & 0.296 \\
\bottomrule
\end{tabular}
\caption{Table caption}
\end{table}

%------------------------------------------------

\section{Figure}\index{Figure}

\begin{figure}[h]
\centering\includegraphics[scale=0.5]{placeholder}
\caption{Figure caption}
\end{figure}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}}
\section*{Books}
\addcontentsline{toc}{section}{Books}
\printbibliography[heading=bibempty,type=book]
\section*{Articles}
\addcontentsline{toc}{section}{Articles}
\printbibliography[heading=bibempty,type=article]

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage
\phantomsection
\setlength{\columnsep}{0.75cm}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}}
\printindex

%----------------------------------------------------------------------------------------

\end{document}