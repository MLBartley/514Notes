%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.0 (9/2/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\usepackage{mathrsfs}
\usepackage{amsbsy}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{bm}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
\coordinate [below=12cm] (midpoint) at (current page.north);
\node at (current page.north west)
{\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{background}}; % Background image
\draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering Linear Models\\[15pt] % Book title
{\Large STAT 551}\\[20pt] % Subtitle
{\huge Course Notes by Meridith Bartley}}}; % Author name
\end{tikzpicture}};
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2013 John Smith\\ % Copyright notice

\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \textsc{book-website.com}\\ % URL

\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Part One}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Linear Regression}


\begin{itemize}
	\item projection
	\item orthongonal decomposition
	\item Gaussian Linear Regression
	\item prediction (generally of $\hat{y}$)
	\item different types of errors
	\item  influence 
	\item lack of fit
	\item $R^2$
	\item Multicollinearity

\end{itemize}

\section{Projection in Euclidean Space}\index{Projection}

\textbf{Monday August 22}

\begin{definition}[Euclidian Space]
	One way to think of the Euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angle. \textbf{Euclidean space} is an abstraction detached from actual physical locations, specific reference frames, measurement instruments, and so on.\\
\\
	Let Euclidian Space be denoted by $\mathbb{R}^P$. \\ 

	$\mathbb{R}X \dots X\mathbb{R} = \{(x_1, \dots, x_p): x_1 \in \mathbb{R} \dots, x_p \in \mathbb{R}^P\} $
\end{definition}


\begin{definition}[Inner Product]
In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. \textbf{Inner products} allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product).\\
\\
	Let $a \in \mathbb{R}^P,  b \in \mathbb{R}^P$\\
$$ a^Tb = \displaystyle\sum^{P}_{i = 1} a_i b_i$$\\
$$a^Tb =<a,b>$$
\end{definition}

 \begin{definition}[Hilbert Space]
 The mathematical concept of a Hilbert space generalizes the notion of Euclidean space. It extends the methods of vector algebra and calculus from the two-dimensional Euclidean plane and three-dimensional space to spaces with any finite or infinite number of dimensions. A Hilbert space is an abstract vector space possessing the structure of an inner product that allows length and angle to be measured. Furthermore, Hilbert spaces are complete: there are enough limits in the space to allow the techniques of calculus to be used.

 \textit{Hilbert Inner Product Space }$\{\mathbb{R}^P, <a, b>\} $ 

 	
 \end{definition}


\textbf{General Inner Product}

Let $\Sigma \in \mathbb{R}^{PxP} $ set of all $PxP$ matrices. Assume $\Sigma$  is a positive definite matrix. 

$$x^T \Sigma x <0 $$
$$\forall x \in \mathbb{R}^P$$
$$ x \neq 0$$

Then $a^T \Sigma b$ also satisfies the conditions for inner product. 

$$a^T \Sigma b = <a,b>_\Sigma$$

$$a^Tb = a^TIb = <a,b>_I$$

$\{ \mathbb{R}^P, <,>_\Sigma \}$ is a more general inner product space. \\
\\ 

\textbf{Linear Transformation}

A matrix, $A, \in \mathbb{R}^{PxP}$ can be viewed as linear transformation

$T_A: \mathbb{R}^P \rightarrow \mathbb{R}^P, x \mapsto Ax$

\begin{remark}
	Bing Li will denote $T_A$ as $A$. \\
	$\rightarrow$ means maps to for a domain.\\
	$\mapsto$ means maps to for a value.\\
	$\Rightarrow$ means implies. \\
	
\end{remark}



If $A: \mathbb{R}^P \rightarrow \mathbb{R}^P$, 

	$$ker(A) = \{x \in \mathbb{R}^P, Ax = 0 \}$$
	$$ran(A) = \{ Ax: x \in \mathbb{R}^P \}$$

\begin{definition}[Kernel]
 In linear algebra, the kernel, or sometimes the null space, is the set of all elements v of V for which L(v) = 0, where 0 denotes the zero vector in W.

 In coordinate plane, think of a function that crosses the x-axis. The kernel would be all points on x where $y=0$.
	
\end{definition}

\begin{definition}[Range]

In coordinate plane, how much of the y axis is reached with the function? Now extend this idea to more dimensions. 
	
\end{definition}

A linear transformation is \textbf{idempotent} if 

	$$A = A^2$$
	$$Ax = A(A(x)) $$
	$$\forall x \in \mathbb{R}^P$$


If A were a number it could only be 1 or 0. \\
 % get from photo
 % get arrow defintions
\\
\textbf{Wednesday August 24}

Let $T \in \mathbb{R}^{PxP}$ then there exists a unique operator $R \in \mathbb{R}^{PxP}$ such that $\forall x, y \in \mathbb{R}^P$,\\
 $$<x, Ty> = <Rx, y> $$ (general inner product, $a^T\Sigma b$). Aside: What this states is that if you give me any operator in the first you can find one in the second.\\ 
\\
$R$ is called the \textbf{adjoint operator} of T. Written as $T^*$, that is, 
 $$<x, Ty> = <T^*x, y>$$
\\
\textbf{Derived Facts}

\begin{multicols}{2}

 \begin{align*}
	<x, Ty> &= <T^*, y>\\
	&= <y, T^*x>\\
	&= <(T^*)^*y, x>\\
	&= <x, (T^*)^*y>\\
\end{align*}

\columnbreak


		(by the definition)\\
    (inner products the order doesn't matter)\\
    (Use the definition again)\\
    (swap order)
	
  	
\end{multicols}

So, $T = (T^*)^*$. \\
\\
It is easy to see in our case 



 \begin{align*}
	<x, Ty>_\Sigma &= x^T\Sigma Ty \\
	&= x^T \Sigma T \Sigma^{-1} \Sigma y\\
	&= (\Sigma^{-1} T^T \Sigma x)^T \Sigma y\\
	&= <\Sigma^{-1} T^T \Sigma x, y>_\Sigma\\
\end{align*}

So, $T^* = \Sigma^{-1} T^T \Sigma$ when $\Sigma = I_P$ (identity) 
and $T^* = T^T$. \\
\\
\textbf{Derived Facts}

An operator is \textbf{self adjoint} if its adjoint is itself. (i.e. if $T = T^*$ or $<x, Ty> = <Tx, y>$). In the case of $<,>_\Sigma$, 

$$T = \Sigma^{-1} T^T \Sigma$$
 if 
$$ \Sigma = I_P\text{, }T = T^T$$ 

\begin{remark}
	Self adjoint implies symmetric. It's a more general case, hence the use of $\Sigma$ vs $I$. Useful to remember in following two Theorems
\end{remark}

\begin{theorem}
	If $\bm{A} \in \mathbb{R}^{PxP}$ is symmetric, then there exists \textbf{eigenvalue-eigenvector pairs}.\\
	 $(\lambda_1, v_1), \dots (\lambda_P, v_P)$ such that $v_1 \bot \dots \bot v_P$. Orthoginal basis (ONB) such that $$\bm{A} = \displaystyle \sum^P_{i=1} \lambda_i v_i v_i^T \text{(spectral decomposition)}$$ 
\end{theorem}
 
 More generally, if $\bm{A}$ is a linear operator in $\mathscr{H}$ (finite dimential inner product such as \\ ($\mathbb{R}^P, <,>_\Sigma$)). its eigen pair (linear operator now) ($\lambda, v$) is defined by 

$$\begin{cases}
 \bm{A}\underline{v} = \underline{\lambda} \underline{v}\\
 <\underline{v},\underline{v}> = 1
 \end{cases}$$

\begin{definition}[Orthogonal Basis]
In the following, $(\mathbb{R}^P, <,>_\Sigma) = \mathscr{H}$ (H for Hilbert)

ONB is defined by:	

	\begin{enumerate}
		\item $v_i \perp v_j, <v_i, v_j> = 0$ 
		\item $||v_i || = 1$
		\item $\text{span}\{v_1, \dots, v_P \} = \mathscr{H}$
	\end{enumerate}
\end{definition}



\begin{theorem}
	Suppose $\bm{A}: \mathscr{H} \rightarrow \mathscr{H}$ is a self adjoint linear operator. Then $\bm{A}$ has eigen pairs:

	$(\lambda_1, v_1, \dots, (\lambda_P, v_P)$ where $\{v_1, \dots, v_P \}$ is ONB of $\mathbb{R}$ such that

	$$\bm{A} = \displaystyle \sum^P_{i=1} \lambda_i v_i v_i^T \Sigma$$



\end{theorem}

\begin{proof}
	($\lambda, v$) is eigen pair of $\bm{A}$, which means
	 	$$\bm{A}v = \lambda v $$
	 	$$<v,v> = 1$$
	 	$$v^T\Sigma v = 1$$

	  Let $u = \Sigma^{\frac{1}{2}} v$. 

\begin{remark}
		Aside: $\Sigma^\alpha = \Sigma \lambda_i^\alpha v_i v_i^T$

\end{remark}


	Let $v = \Sigma^{-\frac{1}{2}} u$.

	\begin{align*}
		\bm{A} \Sigma^{-\frac{1}{2}} u &= \lambda \Sigma^{-\frac{1}{2}} u\\
		\Sigma^{-\frac{1}{2}} u &= \lambda u\\
	\end{align*}

So, ($\lambda, v$) is  an eigen pair of $\bm{A}$ in ($\mathbb{R}, <,>_\Sigma$) $\Leftrightarrow$ ($\lambda, u$) '$\dots$' of $\Sigma^{\frac{1}{2}} \bm{A} \Sigma^{-\frac{1}{2}}$ in ($\mathbb{R}, <,>_I$).

Note that, $\bm{A}$ is self adjoint in ($\mathbb{R}, <,>_\Sigma$). So, $\bm{A} = \Sigma^{-1} \bm{A}^T \Sigma$

\begin{align*}
	\Sigma^{\frac{1}{2}} \bm{A} \Sigma^{- \frac{1}{2}} &= \Sigma^{\frac{1}{2}} \bm{A}^T \Sigma \Sigma^{-\frac{1}{2}}\\
	&= \Sigma^{-\frac{1}{2}} \bm{A}^T \Sigma^{\frac{1}{2}} \\
	&= (\Sigma^{\frac{1}{2}} \bm{A} \Sigma^{-\frac{1}{2}})^T
\end{align*}

Note: $\Sigma^{\frac{1}{2}} \bm{A} \Sigma^{-\frac{1}{2}}$ is symmetric!! So by Theorem 1.1, $\Sigma^{\frac{1}{2}} A \Sigma^{-\frac{1}{2}} = \sum \lambda_i v_i v_i^T$ where ($\lambda_i, v_i$) eigenpairs of $\Sigma^{\frac{1}{2}} \bm{A} \Sigma^{-\frac{1}{2}}$. 

That means $(\lambda_i, \Sigma^{\frac{1}{2}} v_i)$ are eigen pairs of $A$. 

So, $\Sigma^{\frac{1}{2}} \bm{A} \Sigma^{-\frac{1}{2}} = \displaystyle \sum ^P_{i=1} \Sigma^{\frac{1}{2}} u_i u_i^T \Sigma^{\frac{1}{2}} \Rightarrow A = \displaystyle \sum ^P_{i=1} \lambda u_i u_i^T \Sigma$ 
\end{proof}


\begin{definition}[Projection]
	If $P$ is an operator in $(\mathbb{R}^{P}, <,>)$ then $P$ is called a \textbf{projection} if it is both idempotent ($P = P^2$) and self adjoint ($P = P^*)$. 
\end{definition}

\textbf{Preposition 1.1} If $\bm{A}$ is a linear operator then $ker(\bm{A}) = ran(\bm{A}^*)^\bot$

\begin{proof}
	Take $x \in ker(\bm{A}) (\Rightarrow \bm{A}x = 0)$. 

	$\forall y \in ran(\bm{A}^*), x \perp y$

	$\Rightarrow x \perp y \forall y = \bm{A}^*z, z \in \mathbb{R}^P$

	Hence, 
	\begin{align*}
		<x,y> &= <x, \bm{A}^*z>\\
			&= <\bm{A}x, z>\\
			&= <0, z>\\
			&= 0\\
			\\
			&\Rightarrow x \perp y\\
			&\Rightarrow x \in ran(\bm{A}^*)^\perp
	\end{align*}
Or vice versa.
\end{proof}

\textbf{Friday August 26}

\begin{remark}
	$\perp$ means orthogonal complement. 

	$$\mathscr{S}^\perp = \{v \in \mathbb{R}^P, v \perp \mathscr{S} \} $$

	$$ v \perp w \forall w \in \mathscr{S} $$

	$$<v,w> = 0 \forall w \in \mathscr{S} $$

	$$ = \{v \in \mathbb{R}^P, <v,w> = 0 \forall w \in \mathscr{S} \} $$
\end{remark}

Recall, 
$ker(A) = ran(A^*)^\perp$

So, if A is self adjoint then this is true and $ran(A) $ is also $span(A)$ which is the subspace spanned all columns of A. 

\begin{theorem}
	If $P$ is a projection, then 

	\begin{enumerate}
		\item $Pv = v,\text{ } \forall v \in ran(P) $
		\item $Pv = 0,\text{ } \forall v \perp ran(P)$ 
		\item If $Q$ is another projections such that the $ran(Q) = ran(P)$ then $Q=P$. (The range determines the operator, because it is what decomposes the operator.)

	\end{enumerate}

	Asside: $P$ acts like one on some spaces, and zero on orthogonal space.

\end{theorem}

\begin{proof}
	\begin{enumerate}
		\item Let $v \in ran(P)$. Since $P^2 = P$ (idempotent) then\\ 
			$\begin{aligned}
					P^2 v &= Pv\\
					&\Rightarrow P^2 v - PV = 0\\
					&\Rightarrow P(Pv-v) = 0 \\
					&\Rightarrow Pv - v \in ker(P)\\
					&\Rightarrow Pv - v \perp ran(P)\\
					&\Rightarrow <Pv - v, Pv - v> = 0\\
					&\Rightarrow ||Pv - v || = 0\\
					&\Rightarrow Pv - v = 0\\
					&\Rightarrow Pv = v\\	
			\end{aligned}$
	\item If\\ 
		$\begin{aligned}
				v &\perp ran(P)\\
				\Rightarrow v &\in ker(P)\\
				\Rightarrow Pv &= 0
		\end{aligned}$

	\item If $Q$ is another operator with $ran(Q) = ran(P) = \mathscr{S}$ then $\forall v \in \mathscr{S}$

		$\begin{aligned}
			Qv &= v = Pf (\forall v \perp \mathscr{S})\\
			Qv &= 0 = Pv\\
			Qv &= Pv \forall,\text{ } v \in \mathscr{S}\\
			Q &= P
		\end{aligned}$
	
	\end{enumerate}
\end{proof}


\begin{theorem}
	Suppose $\mathscr{S}$ is a subspace of $\mathbb{R}^P$, R $V_1, \dots, V_m$ is a basis of $\mathscr{S}$. 

	Let $V = (V_1, \dots, V_m) \in \mathbb{R}^{xM}$. 

	Then, 

	\begin{enumerate}
		\item $A = V(V^T\Sigma V)^{-1}V^T \Sigma$ is a projection.
		\item $ran(A) = \mathscr{S}$ 
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}
		\item idempotent.\\
		$\begin{aligned}
			A^2 &= V(V^T\Sigma V)^{-1} V^t \Sigma V (V^T \Sigma V)^{-1} V^T \Sigma\\
				&= V(V^T \Sigma V) ^{-1}V^T \Sigma\\
				&= A
		\end{aligned}$

		\item Self adjoint.

		Let $x, y \in \mathbb{R}^P$

		$\begin{aligned}
					<x,Ay> &= x^T \Sigma v(v^T \Sigma v)^{-1}v^ \Sigma y\\
					&= (v(v^T \Sigma v)^{-1} v^T \Sigma x)^T \Sigma y\\
					&= <Ax, y>
				\end{aligned}$

		\item $ran(A)= \mathscr{S}$?

		Let $x \in \mathbb{R}^P$. 

		$Ax = v(v^T \Sigma v)^{-1} v^T \Sigma x$
		$\in span(v) = \mathscr{S}$ 

		So let $x \in \mathscr{S}$, 

		$$x \in ran(v)$$

		$$x = vy $$ for some $y \in \mathbb{R}^P$

		$$= v(v^T \Sigma v)^{-1} v^T \Sigma vy $$

		$\in ran(A)$

		So, $\mathscr{S} \subseteq ran(A)$ and then $\mathscr{S} = ran(A)$.
	\end{enumerate}
\end{proof}

We write $A$ as $P_\mathscr{S} (\Sigma)$ (orthogonal projection on to $\mathscr{S}$ with respect to $\Sigma$ - product). \\ 

In the following, let $I: \mathbb{R}^P \rightarrow \mathbb{R}^P$ be the identity mapping. ($x \mapsto x$)

Let $\mathscr{S}$ be a subspace in $\mathbb{R}^P$. 

Let $Q_\mathscr{S} (\Sigma) = I - P_\mathscr{S} (\Sigma)$\\

\textbf{Proprosition 1.2} $Q_\mathscr{S} (\Sigma) = P_{\mathscr{S}^\perp} (\Sigma)$

\begin{proof}
	 Show $Q_\mathscr{S} (\Sigma)$ is projection. \\
		
	 	\begin{enumerate}
	 		\item \textit{Idempotent}\\
				$\begin{aligned}	
							Q_\mathscr{S}^2 (\Sigma) &= Q_\mathscr{S} (\Sigma) Q_\mathscr{S} (\Sigma) \\
							&= (I - P_\mathscr{S} (\Sigma))(I - P_\mathscr{S} (\Sigma))\\
							&= I - P_\mathscr{S} (\Sigma) - P_\mathscr{S}(\Sigma) + P_\mathscr{S}P_\mathscr{S}\\
							&= Q_\mathscr{S} (\Sigma)
				\end{aligned}$

			\item \textit{Self-adjoint}\\
					$$x, y \in \mathbb{R}^P$$

					$\begin{aligned}
						<x, Q_\mathscr{S}(\Sigma) y> &= <x, (I - P_\mathscr{S} (\Sigma)) y>\\
						&= <x,y> - <x, P_\mathscr{S} (\Sigma)) y>\\
						&= <x,y> - <P_\mathscr{S} (\Sigma)) x,  y>\\
						&= <(I - P_\mathscr{S} (\Sigma))x, y>\\
						&= <Q_\mathscr{S}(\Sigma) x, y>				
					\end{aligned}$

			\item \textit{Range}

				$ran(Q_\mathscr{S} (\Sigma)) = \mathscr{S}^\perp$. Take $x \perp \mathscr{S} = ran(P_\mathscr{S}(\Sigma))^\perp = ker(P_\mathscr{S}(\Sigma)).$\\

				$\begin{aligned}
						&\Rightarrow P_\mathscr{S}(\Sigma) = 0\\
						&\Rightarrow Q_\mathscr{S}(\Sigma) x = x - P_\mathscr{S}(\Sigma) x  = x\\
						X \in ran(Q_\mathscr{S}(\Sigma))\\
						&\Rightarrow \mathscr{S}^\perp \subseteq ran(Q_\mathscr{S}(\Sigma))\\
						\text{Take } x\in ran(Q_\mathscr{S}(\Sigma)), \text{ } \forall y \in \mathscr{S} = ran(P_\mathscr{S}(\Sigma))\\
						y = P_\mathscr{S}(\Sigma)z \text{ for some } z \in \mathbb{R}^P\\
						<x, y> = <x, P_\mathscr{S}(\Sigma) z> = <P_\mathscr{S}(\Sigma)x,  z> = 0\\
						&\Rightarrow x \in \mathscr{S}^\perp\\
						&\Rightarrow ran(Q_\mathscr{S}(\Sigma)) = \mathscr{S}^\perp
				\end{aligned}$	
	 	\end{enumerate}	


\end{proof}


\section{Cochran's Theorem}\index{Cochran's Theorem}

This section will be about the distribution of the squared norm of a projection of a Gaussian random vector. 

\textbf{Preposition 1.3} If $A$ is idempotent, then its eigenvalues are either 0 or 1. 

\begin{proof}
	$\lambda$ is eigenvalue of $A$. 

	$$\Rightarrow Av = \lambda v (||v|| = 1)$$ 

	$$ \lambda = Av = A^2v = \lambda Av = \lambda^2$$

	So, $\lambda$ is 0 or 1.
\end{proof}

\textbf{Monday August 29}\\


 \textbf{Lemma 1.1}
 	Suppose $V \sim N(0, \sigma^2 I_P)$. 

 	P is projection with $I_P$- inner product. 
 	Then $V^T PV \sim \sigma^2 \chi^2_S$ where df = rank(P).

 	\begin{proof}
 		P is symmetric, and it has spectral decomposisition, 

 		$$A R A^T $$
 		where the A's are orthogonal and R is diagonal with diagonal entries 0 or 1.\\

 		Then, 

 		$$A^T V \sim N_P (0, A^T (\sigma^2 I_P)A) = N_P (0, \sigma I_P) $$
 		
 		Let, 

 		$$Z = R A^T V$$

 		then,

 		$$Z \sim N_P (0, \sigma^2 R^2) = N_P (0, \sigma^2 R) $$

 		That means among the components of Z, some are distributied as N(0, 1) and the rest are zero and they are independant. So, 
 		$$Z^T Z \sim \chi^2_S = V^T P V $$
  	\end{proof}

  	\begin{corollary}
  		Suppose $X \sim N(0, \Sigma)$. Consider the Hilbert space ($\mathbb{R}^P, <,>_{\Sigma^{-1}}$). 

  	$$<a,b>_{\Sigma^{-1}} = a^T \Sigma^{-1} b$$

  	Let $\mathscr{S}$ be a subspace of $\mathbb{R}^P$ and $P_\mathscr{S}(\sigma^{-1})$ be the projection onto $\mathscr{S}$ with respect to $<,>_\Sigma^{-1}$ (special case of Fisher information inner product)

  	Then, 

  	$$||P_\mathscr{S}(\Sigma^{-1})x||^2_{\Sigma^{-1}} \sim \chi^2_r $$

  	where $r = dim(\mathscr{S})$.
  	\end{corollary}

  	\begin{proof}
  		Let V be a basis matrix of $\mathscr{S}$ (i.e. the col of V form basis in $\mathscr{S}$).

  		$\begin{aligned}
  			||P_\mathscr{S}(\Sigma^{-1})X||^2_{\Sigma^{-1}} &= <P_\mathscr{S} (\Sigma^{-1}) X, P_\mathscr{S} (\Sigma^{-1}) X>\\
  				&= X^T P_\mathscr{S} (\Sigma^{-1}) \Sigma^{-1} P_\mathscr{S} (\Sigma^{-1}) X\\
  				&= X^T (V(V^T \Sigma^{-1} V)^{-1} V^T \Sigma^{-1})^T \Sigma^{-1} (V(V^T \Sigma^{-1} V)^{-1} V^T \Sigma^{-1}) X\\
  				&= X^T \Sigma^{-1} V(V^T \Sigma^{-1} V)^{-1} v^T \Sigma^{-1} V(V^T \Sigma^{-1} V)^{-1} V^T \Sigma^{-1}) X\\
  				&= (\Sigma^{-\frac{1}{2}} X)^T[\Sigma^{-\frac{1}{2}} V(V^T \Sigma^{-1} V)^{-1} (\Sigma^{-\frac{1}{2}} V)^T] (\Sigma^{-\frac{1}{2}} X)
  		\end{aligned}$

  		But, 

  		$$\Sigma^{-\frac{1}{2}} x \sim N(0, I_P) $$

  		So, 
  		$$ \Sigma^{-\frac{1}{2}} V(V^T \Sigma^{-1} V)^{-1} (V^T \Sigma^{-\frac{1}{2}})^T \quad (*)$$

  		is a projection with repect to $I_P$-inner producted (idempotent, self adjoint, YES). 

  		By Lemme 1.1, $$ (*) \sim \chi^2_r$$.

  		
  	\end{proof}
 
 It is then easy to derive Cocharan's Theorem. (see proof in Homework 1)

\begin{theorem}
	Let $X \sim N(0, \Sigma)$ and $\mathscr{H} = \{\mathbb{R}^P, <,>_{\Sigma^{-1}} \}$. Let $\mathscr{S}_1, \ dots, \mathscr{S}_k$ be linear subspaces of $\mathbb{R}^P$ such that $\mathscr{S}_i\perp\mathscr{S}_j$ in $<,>_{\Sigma^{-1}}$ 

	Let $r_i = dim(\mathscr{S}_i)$.

	Let $W_i = || P_{\mathscr{S}_i} (\Sigma^{-1}) X||^2_{\Sigma{-1}}$

	Then, 

	\begin{enumerate}
		\item $W_i \sim \chi^2_{r_i}$
		\item $W_1 \indep, \dots, \indep W_k$ where $\indep$ indicates independence.
	\end{enumerate}
\end{theorem}


\section{Gaussian Linear Regresson Model}\index{Gaussian Linear Regresson Model}

$Y =
\begin{pmatrix} y_1  \\  
\vdots \\
y_n
 \end{pmatrix}$

$X = \begin{pmatrix}
	x_{11} & \dots & x_{1P} \\
	\vdots & \ddots & \vdots\\
	x_{n1} & \dots & x_{np}\\
\end{pmatrix} \in \mathbb{R}^{n x p}$

\vspace{5mm}

Consider the linear model, 

$$y = X\beta  + \epsilon$$

$$\epsilon \sim N(0, \sigma^2 I_n) $$

where X has full column rank ($n \geq p$).\\

Here X is treated as fixed. \\

\textbf{Maximum Likelihood Estimator} 
		$$\text{E}(y) = X\beta \in \mathbb{R}^n $$
		$$\text{Var}(y) = \sigma^2 I_n $$
	    $$y \sim \text{N}_p(X\beta, \sigma^2 I_n)$$


\textbf{Multivariate Normal Density}

$$y \sim \text{N}(\mu, \Sigma) $$

$$f_Y(y) = \frac{1}{(2\pi)^{\frac{n}{2}} [det(\Sigma)]^{\frac{1}{2}}} e^{-\frac{1}{2} (y - \mu)^T \Sigma^{- 1} (y - \mu)}$$\\
\\
In our case, 

$$\Sigma = \sigma I_n $$

$$\text{det}(\Sigma) = \text{det}(\sigma^2 I_n) = \sigma^2 \text{det}( I_n) = \sigma^{2n} $$

So, 

$$f_Y(y) = \frac{1}{(2\pi)^{\frac{n}{2}} \sqrt{\sigma^{2n}}} e^{-\frac{1}{2\sigma^2} ||y - \mu ||^2}$$

\vspace{5mm}

To find the log likelihood and subsequently take the partial derivatives for MLE,  \\

$\log(f_y(\eta)) = \frac{n}{2} \log(\sigma^2) -\frac{1}{2\sigma^2} ||y - \mu ||^2 = \ell(\beta, \sigma^2, y)$\\
\\

$\frac{\partial}{\partial \beta} = \dots = -\frac{1}{2 \sigma^2} 2 X^T (y - X\beta) = 0 $

$$\hat{\beta} = (X^T X)^{-1} X^T Y \in \mathbb{R}^P$$

$\frac{\partial}{\partial \sigma^2} l(\beta, \sigma^2, y) = \dots = -\frac{n}{2\sigma^2} + \frac{1}{2 \sigma^4} ||y - X\beta ||^2 = 0 $

$$\hat{\sigma^2} = \frac{1}{n} ||y-X\hat{\beta}||^2 $$

In summary, the MLE for $(\beta, \sigma^2)$ in Gaussian Linear Model are

$$\hat{\beta} = (X^T x)^{-1} X^T Y $$
$$\hat{\sigma^2} = \frac{1}{n} ||y-X\hat{\beta}||^2 $$

Note that 

$$X\hat{\beta} = X(X^T X)^{-1} X^T y = \hat{y}$$

So, 
$$\hat{y} = P_{\text{span}(x)} (I_P) = P_X y$$

Now, 

\begin{align*}
	\hat{\sigma^2} &= \frac{1}{n} ||y - \hat{y}||^2\\
	&= \frac{1}{n} ||y - P_X y||^2\\
	&= \frac{1}{n} ||(I_n - P_X) y||^2 \\
	&= \frac{1}{n} ||Q_X y ||^2
\end{align*}


where $Q_X = (I_n - P_X)$ is projection on to $\text{span}(X)^\perp$.\\

It turns out that $(X^T y, y^T y)$ is complete, sufficient statistic for this Gaussian linear model (see homework). \\


\textbf{Wednesday August 31}\\ 

Recall,

\begin{align*}
	\hat{\beta} &= (X^T x)^{-1} X^T Y\\
	\hat{\sigma^2} &= \frac{1}{n} ||y-X\hat{\beta}||^2\\
	Q_x &= I_n - P_x\\
	P_X &+ X(X^TX)^{-1}X^T
\end{align*}

Several properties, 

$$\text{E}(\hat{\beta}) = \beta \quad \text{(unbiased)}$$

$$\text{Var}(\hat{\beta}) = (X^TX)^{-1} X^T (\sigma^2 I_n) X (X^TX)^{-1} = \sigma^2 (X^TX)^{-1} $$

Thus, 
$$\hat{\beta} \sim N(\beta, \sigma^2 (X^TX)^{-1})$$

Because $P_x$ has rank $p$ and $Q_x$ has rank $(n-p)$, then

	$$||Q_x y ||^2 \sim \chi^2_{(n-p)} $$

	Let's find an unbiased estimator for $\sigma^2$ (needed for UMVUE), 

	\begin{align*}
		E(\hat{\sigma^2}) &= E(\frac{1}{n}||Q_x y ||^2)\\
			&= \frac{n-p}{n} \sigma^2\\
			E\left(\frac{n}{n-p} \hat{\sigma^2}\right) &= \tilde{\sigma}^2\\
	\end{align*}

	Moreover, $\hat{\beta}$ has one-to-one transformation with 

	$$(X^TX)^{-1} X^T y \leftrightarrow X (X^TX)^{-1} X^T y = P_{Xy}$$

	\begin{align*}
		Cov(P_{Xy}, Q_{Xy}) &= P_X \sigma^2 I_n Q_X\\
		&= \sigma^2 P_X Q_X\\
		&= 0
	\end{align*}

	$$P_{Xy} \indep Q_{Xy} \quad \text{(due to normality)} $$

	$\hat{\beta} \leftrightarrow P_{Xy}$

	$\hat{\sigma^2}$ is a funciton of $Q_{Xy}$, so $\hat{\beta} \indep \hat{\sigma^2}$\\

	In your homework, $\hat{\beta}, \hat{\sigma^2} \leftrightarrow$ complete sufficient.\\

	$\hat{\beta}, \tilde{\sigma^2}$ is UMVUE (Lehmann-Sheffe).



\begin{theorem}[ Gaussian Regression Model] Under this model:\\

	\begin{enumerate}
		\item $\hat{\beta}, \tilde{\sigma^2}$ UMVUE for $\beta, \sigma^2$
		\item $\hat{\beta} \sim N(\beta, \sigma^2 (X^TX)^{-1})$
		\item $(n-p)\tilde{\sigma^2} \sim \sigma^2 \chi^2_{(n-p)}$
		\item $\hat{\beta} \indep \tilde{\sigma^2}$
	\end{enumerate}

\end{theorem}


\section{Statistical Inference for $\beta$, $\sigma^2$}\index{Statistical Inference for $\beta$, $\sigma^2$}

Suppose we want to test 

$H_0: \beta_1 = \beta_{i0}$

Let $M = (X^TX)^{-1}$.\\
\\
Then, 

	$$\hat{\beta} \sim N(\beta_i0, \sigma^2 M_{ii}) $$

where, $M_{ii} \leftarrow (i,i)^{th}$ entry of M\\
\\

Also,  $\frac{(n-p)\tilde{\sigma^2}}{\sigma^2} \sim \chi^2_{(n-p)}$

$$\hat{\beta} \indep \tilde{\sigma^2} $$


$$\frac{\frac{\hat{\beta_i}- \beta_{i0}}{\sqrt{\sigma^2 M_{ii}}} \sim N(0,1) }{\sqrt{\frac{(n-p) \tilde{\sigma}^2 / \sigma^2\cap^\infty_{k=n} A_k^C )}{n-p}}} \sim t_{(n-p)}$$

$$T = \frac{\hat{\beta_i} - \beta_{i0}}{\tilde{\sigma} \sqrt{M_{ii}}} \sim t_{(n-p)} = (*)$$

Reject $H_0$ if 

$$\left|\frac{\hat{\beta_i} - \beta_{i0}}{\tilde{\sigma} \sqrt{M_{ii}}} \right| > t_{\frac{\alpha}{2}(n-p)} $$

% PHOTO

Recall, 


$$X \sim N(\mu, 1)$$
$$y \sim \chi^2_r$$
$$X \indep y$$

$$\frac{X}{\sqrt{\frac{y}{r}}} \sim t_n(\mu)$$\\
\\

\textbf{Power at $\beta_{i1}$}

$$\hat{\beta_i} \sim N(\beta_{i1}, \sigma^2 M_{i1})$$
So, 

$$ \frac{\hat{\beta_i} - \beta_{i0}}{\tilde{\sigma} \sqrt{M_{ii}}} \sim t_{(n-p)}(\frac{\beta_{i1} - \beta_{i0}}{\sigma \sqrt{M_{ii}}})$$

(alternative distrabution of T)\\
\\
By this (*), 

$$P(\frac{}{} \in (-t_{\frac{\alpha}{2}(n-p)}, t_{\frac{\alpha}{2}(n-p)})) $$



Convert this to put $\beta_{i0}$ in between $(1-\alpha)100$ percent C.I. for $\beta_{i.}$. 

$$(\hat{\beta_1} - t_{\frac{n}{2}(n-p)} \hat{\sigma} \sqrt{M_{ii}}, \hat{\beta_1} + t_{\frac{n}{2}(n-p)} \hat{\sigma} \sqrt{M_{ii}}) $$


\section{Delete One Prediciton}\index{Delete One Prediciton}

Very useful in variable selection, cross validation, diagnostics.\\
\\ 
Prediction: $\hat{y} = X \hat{\beta}$
	$= P_x y$\\
\\
But this has a drawback as it favors overfitting. Projectioning onto larger spaces will always decrease the norm,  $||Q_Xy ||^2$. (This can decrease errors which would cause you to think it's better, even though it's not.)

To prevent overfitting, try to be objective, withhold $y_i$ when predicting $y_i$ (inverse of a matrix, rank 1 perpendicular)

\begin{theorem}[Theorem 1.7]
	Suppose $A \in \mathbb{R}^{PxP}$ is a symmetric, nonsingular matrix. and $v \in \mathbb{R}^P$. 

	Then, 

	$$(A \pm vv^T)^{-1} = A^{-1} \pm \frac{A^{-1}vv^t A^{-1}}{1 \pm v^TA^{-1}v}$$
\end{theorem}

\begin{figure}[h]
	\centering\includegraphics[scale=0.05]{wedAug31-1.jpg}
	\caption{Theorem 1.7 Visualization}
	\end{figure}

Use what is left to compute $\hat{\beta_{-i}}$.

$$\hat{\beta_{-i}} = (X^T_{-1}X_{-i})^{-1}X^T_{-i} y_{-i}$$

This can be expanded in simple sum, so that you don't have to do n regressions.

\begin{align*}
	(X^T_{-i} X_{-i})^{-1} &= (X^TX-X_iX_i^T)^{-1}\\
	&=A^{-1} + \frac{A^{-1}v v^T A^{-1}}{1 - v^t A^{-1} v}\\
	&=(X^T X)^{-1} + \frac{(X^TX)^{-1}X_i X_i^T (X^TX)^{-1}}{1 - X^T_i M X_i}\\
	X_i^T M X_i &= X_i^T (X^T X)^{-1}  \\
	&= (P_x)_{ii}\\
	&= P_i\\
	\\
	\hat{\beta}_i &= (X^TX - X_iX_i^T)^{-1} (X^T y - X_i y_i)\\
	&= [M + \frac{M X_i X_i^T M}{1 - P_i}] (X^T y - X_i y_i)\\
	&= M X^T y + \frac{M X_i X_i^T M X^T y}{1 - P_i} - M X_iy_i - \frac{M X_i X_i^T M X_i y_i}{1 - P_i}\\
	&= \dots\\
	&= \hat{\beta} - \frac{M X_i}{1 - P_i}(y_i - X_i^T \hat{\beta})
\end{align*}

Delete-one regression. 

$X_i\hat{\beta_{-i}} = \hat{y}_i - \frac{P_i}{1 - P_i} (y_i - \hat{y_i})$

\textbf{Friday September 2}\\
\\

Delete- one error

$$y_i - \hat{y}_i^{(-i)}$$

\begin{remark}
	Recall, you want to leave out $y^{i}$ so you don't overfit. 
\end{remark}


The above is equivalent to 

\begin{align*}
	y_i - X_i^T \hat{\beta}_{-i}\\
	y_i - \hat{y}_i - \frac{P_i}{1 - P_i} (y_i - \hat{y_i})\\
	(y_i - \hat{y}_i) (1 - \frac{P_i}{1 - P_i}))\\
	\frac{1}{1 - P_i} (y_i - \hat{y}_i)
\end{align*}

% GET INUITION ABOUT ABOVE

Delete-one cross validation 

$\displaystyle \sum^n_{i=1} (y_i - \hat{y}_i^{(-i)})^2$

This method is not affected by over fitting. 

The following is often used for "tuning" or variable selection (i.e. penalty, bandwidth, regularization, etc).
$ \displaystyle \sum^n_{i=1} \frac{1}{(1 - P_i)^2}(y_i - \hat{y}_i)^2$

Note: we will come back to variable selection later. 

$\beta = \begin{pmatrix} \beta_1  \\  
\vdots \\
\beta_n
 \end{pmatrix}$

$ A \subseteq \{1, \dots, P \}$

Cross validation of $A$ minimizes over $A \in 2^{\{1, \dots, P \}}$. Best cross validation set. 




\section{Residuals}\index{Residuals}

\begin{itemize}
	\item Residual
		$$\hat{e}_i = y_i - \hat{y}_i $$
	\item Standardized Residual
		$$\text{Var}(\hat{e}_i) = \text{Var}(y_i - \hat{y}_i) = \text{Var}((Q_X)_{ii}y_i)$$
		$$= ((Q_X)_{ii}y_i) \sigma^2 $$
		$$= (1 - P_i)\sigma^2 $$
		$$ sd(\hat{e}_i) = \sqrt{1-P_i} \sigma $$
		$$\hat{sd}(\hat{e}_i) = \sqrt{1-P_i} \tilde{\sigma} $$
		$$ \tilde{\sigma} = \frac{\displaystyle \sum^n_{i=1} (y_i - \hat{y}_i^{(-i)})^2}{n-p}$$
	\item Standardized residual
		$$E^*_i = \frac{\hat{e}_i}{\tilde{\sigma} \sqrt{1 - P_i}} $$
	\item Prediction Error Sum of Squares (PRESS) Residual
		$$y_i - \tilde{y}_i^{(-i)} = \frac{1}{1-P_i} \hat{e}_i = \hat{e}_{iP} $$

		$$ \hat{e}_{iP} \sim N(0, \frac{\sigma^2}{1 - P_i})$$
	\item Standardized PRESS Error
		$$\frac{\hat{e}_{iP}}{\tilde{\sigma}/ \sqrt{1-{_i}}} = \frac{\frac{1}{1-P_i}\hat{e}_i}{\tilde{\sigma}\\(\sqrt{1 - P_i})} = \frac{\hat{e}_i}{\tilde{\sigma}\\(\sqrt{1 - P_i})} = e^*_i$$
\end{itemize}

\section{Influence and Cook's Distance}\index{Influence, Cook's Distance}
% PHOTO
\begin{definition}[Influence] The difference between predictions with and without a data point.
	$$\hat{y}_i - \hat{y}_i^{(-i)} $$

\end{definition}

$$\hat{y}_i - \hat{y}_i^{(-i)} $$

$$X_i\hat{\beta} - X_i\hat{\beta}_{-i} $$

	$$\propto || X_i\hat{\beta} - X_i\hat{\beta}_{-i} ||^2	$$
	$$= (X(\hat{\beta} - \hat{\beta}_{-i}))^T (X(\hat{\beta} - \hat{\beta}_{-i})) $$
	$$(\hat{\beta} \hat{\beta}_{-i})^T X^TX (\hat{\beta} \hat{\beta}_{-i}) $$


Recall, 

$\hat{\beta}_{-i} - \hat{\beta} = -\frac{MX_i (y_i - \hat{y}_i)}{1 - P_i} = - \frac{MX_i \hat{e}_i}{1 - P_i}$

$|| X_i\hat{\beta} - X_i\hat{\beta}_{-i} ||^2 = \frac{}{} $

$= $


Cook's Distance (Technometrics, 1976?)

$$||\frac {\hat{y} - \hat{y}^{(-i)} ||^2}{\mathscr{p} \tilde{\sigma}^2} = \frac{|i \hat{e}_i^2}{(1 - P_i)^2 \mathscr{p} \tilde{\sigma}^2} $$

\begin{definition}[Cook's Distance] Cook's distance measures the influence of the $i^{th}$ deservation.
\end{definition}

\section{Orthogonal Decomposition}\index{Orthogonal Decomposition}

Recall,
$\mathbb{R}^n$ is Euclidean Space.\\

$\mathscr{S}$ is a subspace ($\mathscr{S} \leq \mathbb{R}^n$)

\begin{remark}
	$\leq$ is subspace\\
	$\subseteq$ is a subset
\end{remark}

For 

$\mathscr{S}_1 \leq \mathscr{S}_1 \mathscr{S}_2 \leq \mathscr{S}$

$$\mathscr{S}_1 + \mathscr{S}_2 = \{x + y: x \in \mathscr{S}_1, y \in \mathscr{S}_2 \} $$

% PHOTO

Suppose $\mathscr{S}_1, \mathscr{S}_2 \leq \mathscr{S}$, 

$\mathscr{S}_1 + \mathscr{S}_2 = \mathscr{S}$, $\mathscr{S}_1 \perp \mathscr{S}_2$

then, 

	$$\{\mathscr{S}_1, \mathscr{S}_2 \} $$ is called an orthogonal decomposition of $\mathscr{S}$

% PHOTO

In this case, 

	$$\mathscr{S}_1 \oplus \mathscr{S}_2 = \mathscr{S} $$

More generally, 

\begin{definition}[Orthogonal Decompositon (O.D.)]
	Let $\mathscr{S}_1, \dots, \mathscr{S}_k$ be subspaces of $\mathscr{S}$ such that

	\begin{enumerate}
		\item $\mathscr{S}_1, \dots, \mathscr{S}_k = \{v_1 + \dots + v_k : v_1 \in \mathscr{S}_1, \dots, v_k \in \mathscr{S}_k\}$
		\item $\mathscr{S}_i \perp \mathscr{S}_j \quad \forall i\neq j$
	\end{enumerate}
	
	Then, $\{\mathscr{S}_1, \mathscr{S}_2, \dots, \mathscr{S}_k\}$ is an \textbf{orthogonal decomposition} of $\mathscr{S}$. We may write $\mathscr{S} = \mathscr{S}_1 \oplus \mathscr{S}_2 \oplus \dots \oplus \mathscr{S}_k$.
\end{definition}

\textbf{Proposition 1.5} If $\mathscr{S}_1, \dots, \mathscr{S}_k$ is an O.D. of $\mathscr{S}$, then any $v \in \mathscr{S}$ can be uniquely written as

$$v_1 + \dots + v_k$$, where $v_1 \in \mathscr{S}_1, \dots v_k \in \mathscr{S}_k$. \\

\textbf{Wednesday September 7}

\begin{definition}[Direct Difference]
	Let $\mathscr{S}_1 \leq \mathscr{S}_2 \leq \mathbb{R}^n$. Then, \\

	$$\mathscr{S}_2 \cap \mathscr{S}_1^\perp \equiv \mathscr{S}_2 \ominus \mathscr{S}_1 $$ 

	is called \textbf{direct difference}. This is almost the same as orthogonal complement, except it is within $\mathscr{S}_2$. 

\end{definition}

\textbf{Proposition 1.6} If $\mathscr{S}_1 \leq \mathscr{S}_2$, then 

$$\mathscr{S}_2 = \mathscr{S}_1 \oplus (\mathscr{S}_2 \ominus \mathscr{S}_1) $$

\textbf{Proposition 1.7 - Orthogonal Decomposition and Projection} Consider a Hilbert Space, $\mathscr{H} = \{\mathbb{R}^n, <,>_A \}$, 

\begin{enumerate}
	\item If $\mathscr{S} \leq \mathscr{S}_1 \perp \mathscr{S}_2$ in $\mathscr{H}$, then 

	$$P_{\mathscr{S}_1} (A) P_{\mathscr{S}_2} (A) = 0  $$

	\item If $\mathscr{S} \leq \mathscr{H}, \dots, \mathscr{S}_k \leq \mathscr{H}$, and $\mathscr{S}_1 \perp \dots \perp \mathscr{S}_k$, then

	$$P_{\mathscr{S}_1, \oplus \dots \oplus \mathscr{S}_k} (A) = P_{\mathscr{S}_1} (A) + \dots + P_{\mathscr{S}_k} (A) $$

	\item If $\mathscr{S}_1 \leq \mathscr{S}_2 \leq \mathbb{R}^n$, then

	$$P_{\mathscr{S}_2 \ominus \mathscr{S}_1} (A) = P_{\mathscr{S}_2} (A) - P_{\mathscr{S}_1} (A)$$
\end{enumerate}


\begin{theorem}[Generalization of the earlier Cochran's Theorem]

Suppose $X \sim N(0, \Sigma)$ where $\Sigma \in \mathbb{R}^{nxn}$ is positive definite. 

Let $\mathscr{H} = \{ <,>_{\Sigma^{-1}}\}$. Suppose $\mathscr{S}_1, \dots \mathscr{S}_k, \mathscr{S} \leq \mathscr{H}$ such that $\mathscr{S} = \mathscr{S}_1 \oplus \dots \oplus \mathscr{S}_k$.\\

Let 

	$$w_i = ||P_{\mathscr{S}_i} (\Sigma^{-1}) X ||^2_{\Sigma^{-1}} $$
	$$w = ||P_{\mathscr{S}} (\Sigma^{-1}) X ||^2_{\Sigma^{-1}} $$

Then, 

\begin{enumerate}
	\item $ w = w_1 + \dots + w_k$
	\item $ w_1 \indep \dots \indep w_k$
	\item $w_i \sim \chi^2_{r_i}$\\
		$w \sim \chi^2_{r}$\\
		where $r_i$ is the $dim(\mathscr{S}_i)$, $r$ is the $dim(\mathscr{S})$, and $r = r_1 + \dots + r_k$.
\end{enumerate}
	
\end{theorem}

\begin{notation}
	We use $\oplus$ for spaces. We can also use $\oplus$ function to stack up matrices. Let $A_1, \dots, A_k$ be matrices with arbitrary dimensions. 

	$$A_1 \oplus \dots \oplus A_k = \begin{pmatrix}
		A_1 & \dots & 0\\
		    & \ddots & \\
		0 & \dots & A_k
	\end{pmatrix} $$
\end{notation}

\section{Lack of Fit Test}\index{Lack of Fit Test}

Goodness of Fit

% GET PHOTO

At each $x_i$ you have multiple observations, say $y_{i1}, \dots, y_{im_i}$. In this case, you may test to see if a linear model,$y_i = x_i^T \beta + \epsilon_i$ , is the correct choice for fitting the data. In general, lack of fit refers to testing whether any (linear, generalized, etc) model is adequately describing the data. 

Denote 

$y_i = \begin{pmatrix}
	y_{i1}\\
	\vdots\\
	y_{im_i}
\end{pmatrix}$

$1_{m_i} = \begin{pmatrix}
	1 \\
	\vdots\\
	1
\end{pmatrix}$

$X = \begin{pmatrix}
	X_{1}^T\\
	\vdots\\
	X_{m}^T
\end{pmatrix}$

Assume

$$y_{ij} = X_i^T \beta + \epsilon_{ij} $$

where $\epsilon \sim^{iid} N(0, \sigma^2)$. 

The point is that you have $y_{i1} \dots y_{jm}$ for each $X_i$.\\

In matrix form, 

% GET PHOTO

$$(1_{m_1} \oplus \dots \oplus 1_{m_n}) X\beta + \epsilon $$

So, let $N$ denote a full sample size. 

$$N = m_1 + \dots + m_n$$

this is a special case of linear model, except the design matrix is structured $(1_{m_1} \oplus \dots \oplus 1_{m_n})X $ instead of X. So the formula for MLE (and so on) is the same. \\

$$X \leftrightarrow (1_{m_1} \oplus \dots \oplus 1_{m_n})X$$

So, 

$$\hat{\beta} = ([(1_{m_1} \oplus \dots \oplus 1_{m_n})X])^T ([(1_{m_1} \oplus \dots \oplus 1_{m_n})X])^{-1} [(1_{m_1} \oplus \dots \oplus 1_{m_n})X]^T y$$

\begin{align*}
\hat{y} &= (1_{m_1} \oplus \dots \oplus 1_{m_n})X \hat{\beta} \\
	&= (1_{m_1} \oplus \dots \oplus 1_{m_n})X ([(1_{m_1} \oplus \dots \oplus 1_{m_n})X])^T ([(1_{m_1} \oplus \dots \oplus 1_{m_n})X])^{-1} [(1_{m_1} \oplus \dots \oplus 1_{m_n})X]^T y\\
	&=  (1_{m_1} \oplus \dots \oplus 1_{m_n})X [X^T \begin{pmatrix}
		m_1 & \dots & 0\\
		  & \ddots  & \\
		  0 & \dots & m_n
	\end{pmatrix} 	X]^{-1} X^T (1_{m_1} \oplus \dots \oplus 1_{m_n})	
\end{align*}

So, in linear model with replication we have our hypotheses for lack of fit test, 

$$H_O: E(y_i) = 1_{m_i} X_i^T \beta$$
$$H_1: E(y_i) = 1_{m_i} \mu_i $$

% GET PHOTO 

We are testing whether the arbitrary means, $\mu_1, \dots \mu_n$ sit on the same line. \\


\textbf{Friday September 9}

Under $H_1$, 

$$ y = (1_{m_1} \oplus \dots \oplus 1_{m_n}) \begin{pmatrix}
	\mu_1\\
	\vdots\\
	\mu_n
\end{pmatrix}
+ \epsilon$$

So the $\hat{y}$ under this model, 

$$\hat{y}_{H_1} = P_{1_{m_1} \oplus \dots \oplus 1_{m_n}} y = (1_{m_1} \oplus \dots \oplus 1_{m_n}) \begin{pmatrix}
	m_1 & & \\
	 & \ddots & \\
	  &  & m_n
\end{pmatrix} (1_{m_1} \oplus \dots \oplus 1_{m_n})^T y$$

but under $H_0$, 

$$ \hat{y}_{H_0} = P_{(1_{m_1} \oplus \dots \oplus 1_{m_n})X} y$$

$$\mathscr{S}_1 = \text{span}\{(1_{m_1} \oplus \dots \oplus 1_{m_n})X \} \quad \text{(p-dim)}$$

$$\mathscr{S}_2 = \text{span}\{(1_{m_1} \oplus \dots \oplus 1_{m_n}) \} \quad \text{(n-dim)}$$

$$\mathscr{S}_3 = \mathbb{R}^N \quad (N = m_1 + \dots + m_n)$$

$$ \mathscr{S}_1 \leq \mathscr{S}_2 \leq \mathscr{S}_3 $$

\begin{remark}
	Above used the fact that span(AB) $\subseteq$ span(A)
\end{remark}

\textbf{Lemma 1.1} If $ \mathscr{S}_1 \leq \mathscr{S}_2 \leq \mathscr{S}_3$ then

\begin{enumerate}
	\item $\mathscr{S}_3 \ominus \mathscr{S}_2 \leq \mathscr{S}_3 \ominus \mathscr{S}_1$		  \item $(\mathscr{S}_3 \ominus \mathscr{S}_1) \ominus \mathscr{S}_2 =  \mathscr{S}_3 \mathscr{S}_2$
	\item $(\mathscr{S}_3 \ominus \mathscr{S}_1) = (\mathscr{S}_3 \ominus \mathscr{S}_2) \oplus (\mathscr{S}_2 \ominus \mathscr{S}_1)$
 
\end{enumerate}

Go back to lack of fit, 

$$(\mathscr{S}_3 \ominus \mathscr{S}_1) = (\mathscr{S}_3 \ominus \mathscr{S}_2) \oplus (\mathscr{S}_2 \oplus \mathscr{S}_1)$$

$$P_{\mathscr{S}_3 \ominus \mathscr{S}_1}y = P_{\mathscr{S}_3 \ominus \mathscr{S}_3}y + P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y \quad \text{(Orthogonal Decomposition)} $$

$$||P_{\mathscr{S}_3 \ominus \mathscr{S}_1}y||^2 = ||P_{\mathscr{S}_3 \ominus \mathscr{S}_3}y||^2 + ||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2 $$

$$dim(\mathscr{S}_2 \ominus \mathscr{S}_1) = n-p$$

$$dim(\mathscr{S}_3 \ominus \mathscr{S}_2) = N-n$$

Now, 

$$E(P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y) = P_{\mathscr{S}_3 \ominus \mathscr{S}_2}E(y) =P_{\mathscr{S}_3 \ominus \mathscr{S}_2} \mu  = 0 $$

But, 

$\mu = \begin{pmatrix}
	\mu_1\\
	\vdots\\
	\mu_n
\end{pmatrix} \in \mathscr{S}_2$

and, 

$(1_{m_1} \oplus \dots \oplus 1_{m_n})\underline{\mu}$

$$Var(P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y) = P_{\mathscr{S}_3 \ominus \mathscr{S}_2}Var(y)P_{\mathscr{S}_3 \ominus \mathscr{S}_2} = \sigma^2 P^2_{\mathscr{S}_3 \ominus \mathscr{S}_2} = \sigma^2 P_{\mathscr{S}_3 \ominus \mathscr{S}_2} $$

We know that $y \sim N(\mu, \sigma^2 I_n)$. So, 

$$P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y \sim N(0, \sigma^2 P_{\mathscr{S}_3 \ominus \mathscr{S}_2} $$

Similarly, 

$$ E(P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y) = P_{\mathscr{S}_2 \ominus \mathscr{S}_1}E(y)$$

which under $H_0$ is, 

$$P_{\mathscr{S}_2 \ominus \mathscr{S}_1} (1_{m_1} \oplus \dots \oplus 1_{m_n})X\beta = 0 $$

$$Var(P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y) = \sigma^2 P_{\mathscr{S}_2 \ominus \mathscr{S}_1} $$

$$P_{\mathscr{S}_2 \ominus \mathscr{S}_1} y \sim N(0, \sigma^2 P_{\mathscr{S}_2 \ominus \mathscr{S}_1})$$

By Chochran's Theorem: 

	Under $H_O$, 

	$$||P_{\mathscr{S}_3 \ominus \mathscr{S}_2} y||^2 \sim \chi^2_{(N-n)} $$
	$$||P_{\mathscr{S}_2 \ominus \mathscr{S}_1} y||^2 \sim \chi^2_{(n-p)} $$

	$$||P_{\mathscr{S}_3 \ominus \mathscr{S}_2} y||^2 \indep  ||P_{\mathscr{S}_2 \ominus \mathscr{S}_1} y||^2$$

So our lack of fit test is: 

	$$\frac{||P_{\mathscr{S}_2 \ominus \mathscr{S}_1} y||^2 / (n-p)}{||P_{\mathscr{S}_3 \ominus \mathscr{S}_2} y||^2 / (N - n)} \sim F _{n-p, N-n} $$



 \section{Explicit Intercept}\index{Explicit Intercept}


We now apply this $\mathscr{S}_1, dots $ argument to another problem: special linear model. 

$$y_i = \alpha + \beta^TX_i + \epsilon_i 	\quad i = 1, \dots, n $$

$y = \begin{pmatrix}
	y_1\\
	\vdots\\
	y_n
\end{pmatrix}$

$X = \begin{pmatrix}
	X_1\\
	\vdots\\
	X_n
\end{pmatrix}$

$\epsilon = \begin{pmatrix}
	\epsilon_1\\
	\vdots\\
	\epsilon_n
\end{pmatrix}$

$$Y = 1_n \alpha + X\beta + \epsilon = (1_n X)\begin{pmatrix}
	\alpha\\
	\beta
\end{pmatrix} + \epsilon = U\eta + \epsilon$$

Let $P_{1_n} = 1_n(1_n^T 1_n)^{-1} 1_n^T = \frac{1_n1_n^T}{n}$.\\

Note that for all $a = \begin{pmatrix}
	a_1\\
	\vdots\\
	a_n
\end{pmatrix} \in \mathbb{R}^n$, 

	$$P_{1_n}a = \frac{1_n 1_n^T a}{n} = 1_n \bar{a}, \quad \bar{a}= \frac{1}{n}\displaystyle\sum^n_{i=1}a_i $$

	which is a mean projection. (?)

	$Q_{1_n} = I_n - P_{1_n} \quad \text{(projection on}1_n^\perp\text{)}$\\

	$$ Q_{1_n} a = \begin{pmatrix}
		a_1 - \bar{a}\\
		\vdots \\
		a_n - \bar{a}

	\end{pmatrix}$$

	Decompose X:

	$$X = P_{1_n}X + Q_{1_n}X $$

	$$U \eta = 1_n \alpha + X\beta = 1_n \alpha + P_{1_n}X\beta + Q_{1_n}X\beta = 1_n (\alpha + \frac{1_n^T X\beta}{n}) + Q_{1_n}X\beta = (1_n Q_{1_n}X)\begin{pmatrix}
		\alpha^\star\\
		\beta
	\end{pmatrix} = (1_n Q_{1_n}X)\eta^\star = U^\star\eta^\star $$

	So we do least squres of

	$$(y - U^\star\eta^\star)^T(y - U^\star\eta^\star)$$

	and minimize this over all $\eta^\star \in \mathbb{R}^{Px1}$


	$$\hat{\eta}^\star = (U^{\star T} U^\star)U^{\star T} y $$

	$$U^{\star T} U^\star = \begin{pmatrix}
		1_n^T\\
		(Q_{1_n}X)^T
	\end{pmatrix} (1_n Q_{1_n}X) = \begin{pmatrix}
		1_n^t1_n & Q_{1_n}X1_n\\
		1_n^TQ_{1_n}X & Q_{1_n}X Q_{1_n}X\\
	\end{pmatrix}  = \begin{pmatrix}
		n & 0 \\
		0 & X^TQ_{1_n}X
	\end{pmatrix}$$

	$$\hat{\eta}^\star = \begin{pmatrix}
		n^{-1} & 0 \\
		0 & (X^T Q_{1_n}X)^{-1}
	\end{pmatrix} \begin{pmatrix}
		1_n \\
		(Q_{1_n}X)^T
	\end{pmatrix} y$$

	\textbf{Monday September 12}

	So 

	$$\hat{\alpha}^\star = n^{-1}1_n^T y $$

	$$\hat{\beta} = (X^TQX)^{-1} X^TQ y $$

	$$\hat{\alpha} = n^{-1} 1^T_n y - n^{-1}X \hat{\beta}^\star $$

	For statistical inference, we want to make a decomposition of $\mathbb{R}^n$.

	Let, $\mathscr{S}_1 = \text{span}(1_n), \mathscr{S}_2 = \text{span}(1_n, X), \mathscr{S}_3 = \mathbb{R}^n$. \\

	Then,

	$$(\mathscr{S}_3 \ominus \mathscr{S}_1) = (\mathscr{S}_3 \ominus \mathscr{S}_2) \oplus (\mathscr{S}_2 \ominus \mathscr{S}_1) $$

	Then, 

	$$||P_{\mathscr{S}_3 \ominus \mathscr{S}_1}y||^2 = ||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2 + ||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2 $$

	Or, 

	$$SSTotal = SSError + SSRegression $$ 

	We may compute these terms, 

	\begin{align*}
		P_{\mathscr{S}_3 \ominus \mathscr{S}_1} =  P_{\mathscr{S}_3} - P_{\mathscr{S}_1}\\
			&= I_n - \frac{1_n1_n}{1_n^T1_n}\\
			&= Q_1n\\
		\mathscr{S}_2 \ominus \mathscr{S}_1	&= \text{span}(Q_{1_n}X)\\
		P_{\mathscr{S}_2 \ominus \mathscr{S}_1} &= QX(X^TQX)^{-1}QX^T\\
		P_{\mathscr{S}_3 \ominus \mathscr{S}_2}&= Q - QX(X^TQX)^{-1}X^TQ
	\end{align*}

By Cochran's Theorem, (these are orthogonalized projections, etc), 

$$ ||P_{\mathscr{S}_3 \ominus \mathscr{S}_1}y||^2 \sim \chi^2{(n-1)} $$
$$ ||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2 \sim \chi^2_{(p-1)} $$
$$ ||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2 \sim \chi^2_{(n - p - 1)} $$

\begin{remark}
	$$dim(\mathscr{S}_3) = n$$
	$dim(\mathscr{S}_2) = p+1$
	$dim(\mathscr{S}_3) = 1$
\end{remark}

We also know that these are all independent of each other. So we can test regression effect with the following hypothesis:

$$H_0: \beta - 0 $$


$$\frac{||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2 / (p-1)}{||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2 / (n-p - 1)} = \frac{y^T QX(X^TQX)^{-1}QX^T y /(p-1)}{y^T (Q - QX(X^TQX)^{-1}X^TQ) y / (n-p - 1)} \sim F_{p-1, n - p - 1} $$

Distributions\\

$$\hat{\beta} (X^TQX)^{-1}X^TQy$$

$E(\hat{\beta}) = (X^TQX)^{-1}X^TQ(1_{n\alpha} + X\beta = (X^TQX)^{-1}X^TQX \beta = \beta$

$\text{Var}(\hat{\beta}) =  (X^TQX)^{-1}X^TQ (\sigma^2 I_n) QX (X^TQX)^{-1} = \sigma^s(X^TQX)^{-1} $

$\hat{\alpha} = \hat{\alpha}^\star - X^T \hat{\beta}$

Because $\hat{\beta}$ is a function of $Qy$ and $\hat{\alpha}^\star$ is a function of $P_{1_n}y$ (and these are orthogonal to each other and thus by normality also independent). \\

$\text{Var}(\hat{\alpha} = \text{Var}(\hat{\alpha}^\star) + \text{Var}( \bar{X}^T \hat{\beta}) = \text{Var}(\bar{y}) + \text{Var}( \bar{X}^T \hat{\beta}) = \frac{\sigma^2}{n} + \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X} $

$$\hat{\alpha} ~ N( \alpha, \frac{\sigma^2}{n} + \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X}) $$

\begin{align*}
	Cov(\hat{\alpha}, \hat{\beta}) &= Cov(\hat{\alpha}^\star - \bar{X}^T\hat{\beta}, \hat{\beta})\\
		&=-\bar{X}^T Var(\hat{\beta})\\
		&= =\bar{X}^T \sigma^2 (X^TQX)^{-1}
\end{align*}

$$ \begin{pmatrix}
	\hat{alpha}\\
	\hat{\beta}
\end{pmatrix} \sim N[\begin{pmatrix}
	\alpha\\
	\beta
\end{pmatrix}, \begin{pmatrix}
	\frac{\sigma^2}{n} + \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X} & - \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X}\\
	- \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X} & \frac{\sigma^2}{n} + \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X}
\end{pmatrix}]$$

Estimate $\sigma^2$\\


$$ ||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2 = \displaystyle \sum^n_{i=1} (y_i - \hat{\alpha} - \hat{\beta}^TX_i)^2 \sim \sigma^2 \chi^1_{n-p-1}$$

So, 

$$E(||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2 ) = \sigma^2 (n-p-1)$$

Thus, 

$$\hat{\sigma}^2 = \frac{||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2 }{n-p-1}$$

\begin{theorem}
	Under the explicit intercept model, 

	\begin{enumerate}
		\item $(\hat{\alpha}, \hat{\beta}, \hat{\sigma}^1)$ is UMVUE of $(\alpha, \beta, \sigma^2)$ by Lehmann-Sheffe.
		\item $$ \begin{pmatrix}
	\hat{\alpha}\\
	\hat{\beta}
\end{pmatrix} \sim N[\begin{pmatrix}
	\alpha\\
	\beta
\end{pmatrix}, \begin{pmatrix}
	\frac{\sigma^2}{n} + \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X} & - \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X}\\
	- \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X} & \frac{\sigma^2}{n} + \sigma^2 \bar{X}^T(X^TQX)^{-1} \bar{X}
\end{pmatrix}]$$
	\item $(n-p-1) \hat{\sigma}^2 \sim \sigma^2 \chi^2_{(n-p-1)}$
	\item $\begin{pmatrix}
			\hat{\alpha}\\
			\hat{\beta}
		\end{pmatrix} \indep \hat{\sigma}^2$
	\end{enumerate}
\end{theorem}



% %------------------------------------------------




\section{$R^2$}\index{$R^2$}

Proportion of Sum of Squares (SS) explained by regression (i.e. by $\beta$). 

$$R^2 = \frac{SSR}{SST} = \frac{||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2}{||P_{\mathscr{S}_3 \ominus \mathscr{S}_1}y||^2}$$

But we know that, 

$$R^2 = \frac{||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2}{||P_{\mathscr{S}_2 \ominus \mathscr{S}_1}y||^2 + ||P_{\mathscr{S}_3 \ominus \mathscr{S}_2}y||^2} = \frac{SSR}{SSR + SSE} = \frac{SSR/SSE}{SSR/SSE + 1} $$

$$ F = \frac{SSR/p}{SSE/(n-p-1)} = \frac{(n-p-1)}{p} \frac{SSR}{SSE} $$

$$ \frac{SSR}{SSE} = \frac{p}{(n-p-1)} F$$

$$R^2 = \frac{\alpha F}{\alpha F + 1}$$
where $\alpha = \frac{p}{n-p-1}$

This is how we compute the null distribution of $R^2$. \\

\section{Multicollinearity}\index{Multicollinearity}

\textbf{Wednesday September 14}\\

$y = C_1 \beta_1 + \dots + C_p \beta_p$

$$ X = (C_1, \dots, C_P) = \begin{matrix}
	X^T_1\\
	\vdots\\
	X^T_n
\end{matrix}$$

In an extreme case, multicolinearity simply means that the $C_1, \dots, C_p$ are linearly dependent. In this case $\beta$ is not identifiable. 

	We have $C_1, C_2, C_3$.\\

	\begin{align*}
		C_1 &= a\\
		C_2 &= 2a\\
		C_3 &= b
	\end{align*}

	\begin{align*}
		y &= a\beta_1 + 2a \beta_2 + b\beta_3 + \epsilon\\
		&= a(\beta_1 + 2\beta_2) + b\beta_3 + \epsilon
	\end{align*}
	
	$\beta_1 \& \beta_2$ cannot be split. \\

	In the less extreme case, $X^TX$ is nearly singular, meaning it has small eigenvalues. In this case, although $\beta$ is identifiable, they have large variance For example, if $C_1 = a C_2 x 2a$ then $\beta_1, \beta_2$ have large variance which means your parameterization is not good. So may define new parameterization.

	$$ \gamma_1 = \beta_1 + 2\beta_2$$
	$$ \gamma_2 = \beta_3$$ 

	If you run regression against these then the variance would be 'normal'.\\

	S0, how to wee out redundant varaibles? One way Variance Inflation Factor (VIF) which for each $ i = 1, 2, \dots, p$ regresses $C_i$ on $\{C_1, \dots, C_p \setminus C_i \}$
then you get $R^2$ for this regression call it $R_i^2$.\\

If $C_i$ is redunent then $R_i^2$ would be close to 1. 

$$VIF_i = \frac{1}{1 - R_i^2} $$





% %------------------------------------------------





\section{Variable Selection}\index{Variable Selection}

$$ y = C_1\beta_1 + \dots + C_p\beta_p + \epsilon$$

Some of these $\beta$'s are zero. \\

Let us define an active set of parameters, 

$$A_0 = \{i: \beta_i \neq 0 \}$$

To estimate $A_0$ is the goal of variable selction. \\

\textbf{Mallow's $C_p$ criterion}\\

The fundamental issue is variable selction, penalty - penalizing the number of parameters, so you cannot use something like $y - \hat{y}$ as criterion. The more variables you have the smaller $||\hat{y} - y ||^2$ is. So we want to penalize the number of parameters in a reasonable way. \\

Let any subset $A \subset \{1, \dots, p\}$, 

$$X_A = \{C_i: i \in A \}$$

\begin{notation}
	While we often use X  for iid variables (a vector), but here X is a matrix and $X_i$ were referring to its columns. We've changed $X_i$ to $C_i$ to better reflect that we are dealing with columns of X. 
\end{notation}

So, $A = \{1, 3, 5\}$, 

$$X_A = \begin{pmatrix}
	C_1\\
	C_3\\
	C_5
\end{pmatrix}$$

Let $P_{X_A}, Q_{X_A}$ be the projection on to span($X_A$), span$(X_A)^\perp$. For example, 

$$P_{X_A} = X_A(X_A^T X_A)^{-1}X_A^T $$

Let $\mu = E(y) = X\beta = X_{A_0}\beta_{A_0}$.\\

\textbf{Mallow} says we minimize

$$\frac{E||P_{A}y - \mu ||^2}{\sigma^2} $$

among all $A \subset \{1, \dots, p \}$.\\

But we do not know what $\sigma^2$ or $\mu$ are. If so, we would already know $A_0$. We must estimate these.\\

$$E || P_{X_A} y - \mu ||^2 = tr(E(P_{X_A} y - \mu)(P_{X_A} y - \mu)^T) $$

\begin{align*}
	E(P_{X_A} y - \mu)(P_{X_A} y - \mu)^T &= E[(P_{X_a}y - P_{X_a}\mu) + (P_{X_a}\mu - \mu)][(P_{X_a}y - \mu)+ (P_{X_a}\mu - \mu)]^T\\
		&=  \text{expand, two terms are zero}\\
		&= E(P_{X_a}y - P_{X_a}\mu)(P_{X_a}y - P_{X_a}\mu) + (P_{X_a}\mu  - \mu)(\P_{X_a}\mu - \mu)^T\\
		&= Var(P_{X_a}y)\\
		&= P_{X_a} \sigma^2 I_n P_{X_a} = \sigma^2 P_{X_a}\\
		&= tr(\sigma^2 P_{X_a} + Q_{X_a}\mu\mu^TQ_{X_a})\\
		&= \sigma*2 tr(P_{X_a}) + tr(Q_{X_a}\mu\mu^tQ_{X_a})\\
		&= \sigma^2(\#(A)) + tr(\mu^T Q_{X_a} \mu)\\
\end{align*}

$$\Rightarrow E\frac{ || P_{X_A} y - \mu ||^2}{\sigma^2} = \#(A) + \frac{tr(\mu^T Q_{X_a} \mu)}{\sigma^2}$$

Now let's estimate $\frac{\mu^T Q_{X_a} \mu}{\sigma^2}$. \\

Recall, if $U$ is a random vector with multivariate normal distribution so

$$E(U) = e $$
$$Var(U) = Q_{X_A} $$

$$U^TU \sim \chi^2_{(rank(Q)_{X_A})} (||e||^2) $$

Also, $W \sim \chi^2_{(r)} (\delta)$ where $E(W) = r + \delta$.\\

Go back to our problem of estimating $\frac{\mu^T Q_{X_a} \mu}{\sigma^2}$. \\

What about $y^t Q_{X_A} y$? We know that

$$E (\frac{Q_{X_A}y}{\sigma}) = \frac{Q_{X_A}\mu}{\sigma}$$

and

$$Var(\frac{Q_{X_A}y}{\sigma}) = \frac{1}{\sigma^2} Q_{X_A}\sigma^2 I_n = 0$$

So, 

$$ \frac{Q_{X_A}y}{\sigma} \sim N( \frac{Q_{X_A}\mu}{\sigma}, 0)$$

So

$$(\frac{Q_{X_A}y}{\sigma})^T(\frac{Q_{X_A}y}{\sigma}) \sim \chi^2_{(n-\#(A))} ((\frac{Q_{X_A}\mu}{\sigma})^T(\frac{Q_{X_A}\mu}{\sigma})) = \chi^2_{(n-\#(A))} (\frac{\mu^T Q_{X_A} \mu}{\sigma^2}) $$

Thus, 

$$E(\frac{y^T Q_{X_A} y}{\sigma^2} ) = n - \#(A) + \frac{\mu^T Q_{X_A} \mu}{\sigma^2} $$

Which, if you subtract over the n and \#(A) you get an unbiased estimator of $\frac{\mu^T Q_{X_A} \mu}{\sigma^2}$.

But $\sigma^2$ is still unkown, but we use full model, 

$$\hat{\sigma}^2 = \frac{y^T Q_{X_A} y}{n - p} $$

Now we can estimate $\frac{\mu^T Q_{X_A} \mu}{\sigma^2} $ by 

$$\frac{y^T Q_{X_A} y}{\frac{y^T Q_{X} y}{n-p}} - n + 2\#(A) = (n-p) \frac{y^T Q_{X_A} y}{y^T Q_{X} y} - n + 2 \#(A)$$ 

So to recap,
 $$ E\frac{ || P_{X_A} y - \mu ||^2}{\sigma^2} = (n-p)\frac{y^T Q_{X_A} y}{y^T Q_{X} y} - n + 2 \#(A) $$


\textbf{Friday September 16}\\


\textbf{Akaike/Bayesian Information Criteria (AIC/BIC)}\\

Suppose we have some generic (i.e. not related to the design/covariance in regression context) $X_1, \dots, X_n$, a sample of independent random vectors with joint density $f_\theta (x_1, \dots, x_n)$.\\

$\theta \in \Theta \subset \mathbb{R}^P$\\

$\theta = \begin{pmatrix}
	\theta_1\\
	\vdots\\
	\theta_P
\end{pmatrix}$\\

Let $M_0 \subset \{1, \dots, P \}$ be the true active set, that is $\{i: \theta_i \neq 0 \} = M_0$. Also, let $M_0 \subset \{1, \dots, P\}$. We want to recover the true active set $M_0$.\\

Let $\Theta_M$ be the paramter space, corresponding to M. 

$$\Theta_M = \{\theta \in \Theta: \theta_i = 0 iff i \notin M \}$$

Of course we also thave $\Theta_{M_0}$.\\

for each $M \subset \{1, \dots, P \}$ define,

$$L_M = \sup_{\theta \in M} f_\theta (x_1, \dots, x_n) $$

Then, 

$$\text{AIC}(M) = -2 \log L_M + 2(\#M) $$

$$\text{BIC}(M) = -2 \log L_M + (\log n)(\#M) $$

Use them 

$$\hat{M} = \arg\min \{\text{AIC}(M): M \in 2^{\{1, \dots, P\}}\}$$
$$\hat{M} = \arg\min \{\text{BIC}(M): M \in 2^{\{1, \dots, P\}}\}$$

When P is large, this is called \textbf{forward backward selection} instead ov \textbf{Best Set Selection}.\\

\textit{Specialized to Gaussian Linear Regression Model}\\

Here there is no variable selection, 

$\theta = \begin{pmatrix}
	\beta\\
	\sigma^2
\end{pmatrix}$

$\Theta \subset \mathbb{R}^{P+1}$ (M is in $\Theta$)\\
$\beta \in B \\subset \mathbb{R}^P$ (A is in B)\\

In our case, 

$$y \sim N(X_A\beta_A, \sigma^2 I_n)$$

where $A \subseteq B$ which is where $\beta$ is. 

Note we are again using the notation $X_A = \{C_i: i \in \beta \}$ and that 
$$\#A + 1 = \# M $$

If A is an active set of $\beta$ then $M = A \cup \{p+1 \}$ is the active set of $\theta$ because $\sigma^2$ is always active. 

But $L_M = $ ?\\

Recall, MLE for $\beta$ is (under $A$), 

$$\hat{\beta}_A = (X_A^TX_A)^{-1}X^T_A y $$

$$\hat{\sigma}^2_A = \frac{y^T Q_{X_A}y}{n}$$

So the likelihood at $(\hat{\beta}_A, \hat{\sigma}_A^2)$, 

\begin{align*}
	f_{\hat{\theta}_A}(x_1, \dots, x_n) &= \frac{1}{(2\pi)^{\frac{n}{2}} \text{det}(\hat{\sigma}^2_A I_n) } e^{\frac{-1}{2\hat{\sigma}^2_A} ||y - X_A \hat{\beta}_A ||^2}\\
		&= L_M
		&= \dots e^{-\frac{1}{2\hat{\sigma}^2_A} ||y - X_A \hat{\beta}_A ||^2||^2}\\
		&= \dots e^{-\frac{1}{2 \frac{y^T Q_{X_A}y}{n}} ||y - X_A (X_A^TX_A)^{-1}X^T_A y ||^2||^2}\\
		&= \dots e^{-\frac{1}{2 n} ||y - P_{X_A} y ||^2||^2}\\
 		&= \dots e^{-\frac{n}{2 } ||Q_{X_A} y ||^2||^2}\\
 	L_M = \frac{1}{(2\pi)^{\frac{n}{2}} (\frac{y^T Q_{X_A}y}{n})^{\frac{n}{1}} }e^{-\frac{n}{2 }}\\
 	\log L_M = -\frac{n}{2}\log(2\pi) - \frac{n}{2} \log (\frac{y^tQ_A y}{n}) - \frac{n}{2}
\end{align*}

$$\text{AIC}(A) = -2 \log L_M + 2(\#M) =   $$

It is equivalent ti minimize,

$$n\dots $$

For BIC, replace $2(\#A + 1)$ by $(\log n)(\#A + 1)$.


% FINISH NOTES

\textbf{Variable Selection Consistency}\\
(as opposed to estimation consistency)\\

You have data, $(X_1, \dots, X_n) = \mathscr{X}$ (again, generic X, not in regresssion context). An estimator, 

$$\mathscr{X} \rightarrow \Theta $$

Variable selection, 

$$\hat{A}: \mathscr{X} \rightarrow 2^{\{1, \dots, P\}}$$

that is to say, 

$$(x_1, \dots, x_n) \mapsto M $$

\begin{definition}[Variable Selector Consistancy]
	A variable selector, $\hat{A}$ is said to be \textbf{consistant} if 

	$$P(\hat{A} = A_0) \rightarrow 1 $$

	where $A_0$ is the true action set. 
\end{definition}

Next, BIC in variable seleciton consistancy.\\

 Ordering of sequences, $\{a_n\}, \{b_n\}$ 2 sequences in $\mathbb{R}$, positive...

 \begin{notation}[Asymptotic Order of Magnitude]
 	$a_n \prec b_n$ if $\frac{a_n}{b_n} \rightarrow 0$ as $ n \rightarrow \infty$. 

 \begin{example}
 \begin{itemize}
 	\item $a_n \prec 1 \Leftrightarrow a_n \rightarrow 0$
 	\item $a_n \prec n \Leftrightarrow \frac{a_n}{n} \rightarrow 0$
 \end{itemize}
 	
 \end{example}

 % Also, if $a_n \prec b_n$ then $\b_n \succ a_n$.

 \begin{example}
 \begin{itemize}
 	\item $\begin{aligned}
 	 		a_n &\succ 1\\
 	 			&\Rightarrow 1 \prec a_n\\
 	 			&\Rightarrow \frac{1}{a_n} \rightarrow 0\\
 	 			&\Rightarrow a_n \rightarrow \infty
 	 	\end{aligned}$
 	 \item $\begin{aligned}
 	 	n^{\frac{1}{2}}
 	 \end{aligned}$
 \end{itemize}
 	
 \end{example}

 The symbol $\sim$ means both $\prec$ and $\succ$.
 \end{notation}

 


 \textbf{Monday September 19}


\textbf{Lemma 1.3}\\

Under some regularity conditions (identifiability, smoothness of log likelihood, support doesn't depend on parameters, ...) then 
	\begin{enumerate}
	 	\item $\Theta_{M_0} \subseteq \Theta_M$ 
	 	 $$2(\log L_M - \log L_{M_0}) \rightarrow^\mathscr{D} \chi^2_{(\#M - \#M_0)}$$
	 		Here, recall, 

	 			$$L_M = \sup_{\theta \in \Theta} f_\theta(x_1, \dots, x_n) $$
	 	\item If $\Theta_M \subseteq \Theta_{M_0}$ then,

	 		$$n^{-1}2(\log L_M - \log L_{M_0}) \rightarrow^P  2(\sup_{\theta \in \Theta} E\log f_\theta(x_1, \dots, x_n) - E \log f_{\theta_0}(x_1, \dots, x_n))$$

	 		Moreover, if $M \subset M_0 $ then

	 		$$\lim_{n\rightarrow \infty} (2(\sup_{\theta \in \Theta} E\log f_\theta(x_1, \dots, x_n) - E \log f_{\theta_0}(x_1, \dots, x_n))) < 0  $$
	 \end{enumerate} 

\begin{theorem}
	 	Let BIC(M) = $-2 \log L_M + (c n)(\#M)$ where $ 1 \prec c(n) \prec n$. This generalizes BIC so that $c(n)$ replaces $\log(n)$ but still converges slower than n (as does log).\\

	 	Let $\hat{M} = \arg \min_{M \in 2^{\{1, 2, \dots, p\}}} BIC(M)$ then

	 	$$P(\hat{M} = M_0) = 1 $$


	 \end{theorem}	 

	 \begin{proof}
	 	Consider the difference, 

	 $$	BIC(M)-BIC(M_0) = 2(\log L_{M_0} - \log L_M) + c(n)(\#M - \#M_0)$$

	 We want to show (with probability going to 1) that 
	 	$$BIC(M) - BIC(M_0) >0 \quad \forall M \neq M_0 $$

	 	\textit{Case 1} $M \supset M_0$\\
	 	Then $c(n)(\#M - \#M_0) \rightarrow \infty$\\

	 	Meanwhile, $2(\log L_{M_0} - \log L_M) = O_p (1)$.\\

	 	\begin{remark}
	 		Fact. If $U_n = O_p(1), \alpha_n \rightarrow \infty$ then

	 		$$P(U_n + \alpha_n >0) \rightarrow 1 $$
	 	\end{remark}

	 	So, 

	 		$$P(BIC(M) - BIC(M_0))\rightarrow 1$$

	 \textit{Case 2} $M \subseteq M_0$

	 $n^{-1} 2(\log L_{M_0} - \log L_M) \rightarrow c(n)>0$

	 \begin{remark}
	 	Fact. $n^{-1}U_n \rightarrow c >0, \alpha_n \prec n $ and $n^c \prec n$ then

	 	$$P(U_n + \alpha_n > 0) \rightarrow 1 $$
	 \end{remark}

	 So again, 
	 	$$P(BIC(M) - BIC(M_0))\rightarrow 1 $$

	 	Thus, $P(BIC(M)\text{ is uniquely minimized at }M_0) \rightarrow 1$.

	 \end{proof}

\section{Non iid Linear Regression}\index{Non iid Linear Regression}

Suppose 
$$ y = X\beta + \epsilon $$

where $\epsilon \sim N(0, \sigma^2 \Sigma)$ with arbetrary but known matrix $\Sigma > 0$.

Then MLE for $\hat{\beta}$ is

$$\hat{\beta} = (X^T \Sigma X)^{-1}X^T \Sigma^{-1} X $$

MLE for $\hat{\sigma}^2$ is

$$\hat{\sigma}^2 = ||Q_X(\Sigma^{-1})y ||^2/n $$ 


But remember $||Q_X(\Sigma^{-1})y ||^2_{\Sigma^{-1}} \sim \sigma^2 \chi^2_{n-p}$, so now we have

$$E\left(||Q_X(\Sigma^{-1})y ||^2_{\Sigma^{-1}} \right) = \sigma^2 (n-p)$$

so the unbiased estimator is, 

$$\tilde{\sigma}^2 = \frac{||Q_X(\Sigma^{-1})y ||^2_{\Sigma^{-1}}}{n - p} $$


\begin{theorem}
	Under $y = X\beta + \epsilon$  with $\epsilon$ as above, we have

	\begin{enumerate}
		\item $\hat{\beta}, \tilde{\sigma}^2$ are UMVUE
		\item $\hat{\beta} \sim N(\beta, \sigma^2(x^T\Sigma^{-1}X)^{-1})$
		\item $\hat{\sigma}^2 \sim \sigma^2(n-p)^{-1} \chi^2_{n-p}$
		\item $\tilde{\sigma}^2 \indep \hat{\beta}$
	\end{enumerate}
	
	All theories developed previously for $\epsilon \sim N(0, \sigma^2 I_n)$ can be generalized here in a straightforward manner.
\end{theorem}



%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapter{General Linear Hypothesis \& Simultaneous Confidence Intervals}

% \section{Overview}\index{Overview}

% \begin{itemize}
% 	\item General linear models
% 	\item Scheffe's simulteaneous confidence
% 	\item Singular decomposition
% 	\item Non Gaussian error
% \end{itemize}

 \section{General Linear Model}\index{General Linear Model}

\begin{definition}[General Linear Models]
	General Linear Models are the same as linear Gaussian Model, except it is stated in a coordinate-free or geometric way. 
\end{definition}

Let $\mathcal{S} \leq \mathbb{R}^N$.\\

A general linear model gives,

$$y \sim N(\mu, \sigma^2 I_N) $$

where $\mu \in \mathcal{S}$. \\

If we take X to be a basis matrix of $\mathcal{S}$, that is span(X) = $\mathcal{S}$, then we have 

$$y = \mu X + \epsilon = X\beta + \epsilon $$

the same as before. (because $\mu \in \mathcal{S}, \text{span}(x) = \mathcal{S} \Rightarrow \mu = X\beta for some \beta$)

The MLE can be derived in a similar way. 

\textbf{Wednesday September 21}\\

\textit{MLE for $\mu$}\\

Likelihood:

	$$\frac{1}{(2\pi)^{\frac{n}{2}}[det(\sigma^2 I_N)]^{\frac{N}{2}}} e^{2 \frac{1}{2\sigma^2}||y - \mu||^2} $$ 

	maximiaze this over $\mu \in \mathcal{S}, \sigma^2 > 0$.\\

	First we maximize over $\mu \in \mathcal{S}$ equivalent to minimizing $||y - \mu||^2$.\\

	\begin{align*}
		||y - \mu||^2 &= ||y - P_\mathcal{S}y + P_\mathcal{S}y - \mu||^2\\
			&= ||y - P_\mathcal{S}y||^2 - 2<y - P_\mathcal{S}y, P_\mathcal{S}y - \mu> + ||P_\mathcal{S}y - \mu ||^2\\
			&= \dots 2<y - P_\mathcal{S}y, P_\mathcal{S}y - \mu> = 0\\
			&= ||y - P_\mathcal{S}y||^2 + ||P_\mathcal{S}y - \mu ||^2\\
			&\Rightarrow \hat{mu} = P_\mathcal{S}y
	\end{align*}

By the argument exactly like the coordinate case, we can show that 

$$\hat{\sigma}^2_{MLE} = \frac{y^TQ_\mathcal{S}y}{N}$$

Because 

$$ \frac{Q_\mathcal{S}y}{\sigma^2} \sim N(0, Q_\mathcal{S}) $$

we know that 

$$\frac{y^TQ_\mathcal{S}y}{\sigma^2} \sim \chi^2_{N-p} $$

$$E(\frac{y^TQ_\mathcal{S}y}{\sigma^2}) = N-p $$

So, an unbiased estimator for $\sigma^2$ would be 

$$\hat{\sigma}^2 = \frac{y^TQ_\mathcal{S}y}{N-p} $$

and an unbiased estimator for $\mu$ is

$$ E(\hat{\mu}) = P_\mathcal{S}\mu = \mu$$

What is the compelte and sufficient statistic? We may use results from exponential family. 

\begin{align*}
	\exp(\theta_1 t_1 + \theta_t 2_2)\\
	\exp(-\frac{1}{2} ||y - \mu ||^2)= \exp(-\frac{1}{2\sigma^2}(||P_\mathcal{S}y ||^2 + || Q_{\mathcal{S}}y||^2) + \frac{1}{2\sigma^2} <P_\mathcal{S}y, \mu>) 
\end{align*}

So, complete and sufficient statistic wouldbe be

$$(||P_\mathcal{S}y ||^2 + || Q_{\mathcal{S}}y||^2,P_\mathcal{S}y )  \leftrightarrow (|| Q_{\mathcal{S}}y||^2,P_\mathcal{S}y) $$

By Lehmann-Scheffe,

\begin{theorem}
	Under $y \sim N(\mu, \sigma^2 I_n), \mu \in \mathcal{S}$ we have
	\begin{enumerate}
		\item $\hat{\mu} \indep \hat{\sigma}^2, \hat{\mu} \sim N(\mu, \sigma^2 P_\mathcal{S}), (N-p)\hat{\sigma}^2 \sim \sigma^2 \chi^2_{N-p} $
		\item $(\hat{\mu}, \tilde{\sigma}^2)$ is UMVUE 
	\end{enumerate}
\end{theorem}

\section{Hypothesis Testing}\index{Hypothesis Testing}

$y \sim N(\mu, \sigma^2 I_N), \mu \in \mathcal{S}$

Consider, 

$$\mathcal{S}^\prime \leq \mathcal{S} \leq \mathbb{R}^N$$


$$dim(\mathcal{S}^\prime) = k, dim(\mathcal{S}) = p, \quad k \leq p $$


We have 

$$ \mathbb{R}^n \ominus \mathcal{S}^\prime = (\mathbb{R}^N \ominus \mathcal{S}) \oplus (\mathcal{S \ominus \mathcal{S}^\prime}) $$

So,  

$$||P_{\mathbb{R}^N \ominus \mathcal{S}^\prime}y ||^2 = || P_{\mathcal{S} \ominus \mathcal{S}}y||^2 + ||P_{\mathbb{R}^N \ominus \mathcal{S}^\prime}y ||^2 $$

Want to get, 

$$H_0: \mu \in \mathcal{S}^\prime $$

So 

\begin{align*}
	\mu \in \mathcal{S}^\prime &\Leftrightarrow ||P_{\mathcal{S} \ominus \mathcal{S}}\mu || = 0\\
		&\Leftrightarrow ||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y || \text{ is small}\\
\end{align*}

Thus we can use,

$$\frac{||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y ||^2 / (p - k)}{||P_{\mathbb{R}^N \ominus \mathcal{S}}y ||^2 / (N-p)} \sim F_{p-k, N-p} $$

Both Lack of Fit and explicit intercept can be written in gerneral linear model.\\

In Lack of Fit, 

$$\mathcal{S} = \text{span}(1_{m_1} \oplus \dots \oplus  1_{m_n}) $$ 

$$\mathcal{S}^\prime = \text{span}((1_{m_1} \oplus \dots \oplus  1_{m_n}) x) $$

In Explicit Intercept, 

$$\mathcal{S} = \text{span}(1_{m_1} \vdots X) $$
$$\mathcal{S}^\prime = \text{span}((1_{n}) $$

\textbf{Alternative Distribution}\\

When $H_0$ is not true it means that $\mu \notin \mathcal{S}^\prime$. Then we know that $|| P_{\mathcal{S} \ominus \mathcal{S}^\prime}y||^2$ is not small. In fact, 

$E(P_{\mathcal{S} \ominus \mathcal{S}^\prime}y) = P_{\mathcal{S} \ominus \mathcal{S}^]prime}\mu \neq 0$\\

$Var(P_{\mathcal{S} \ominus \mathcal{S}^\prime}y) = \sigma^2 P_{\mathcal{S} \ominus \mathcal{S}^\prime}$

$$P_{\mathcal{S} \ominus \mathcal{S}^\prime}y \sim N(P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu, \sigma^2 P_{\mathcal{S} \ominus \mathcal{S}^\prime}) $$

$$||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y ||^2 \sim \chi^2_{p-k} (||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y ||^2)$$

\begin{definition}
	If 
	$$X \sim \chi^2_{r_1}(s) $$

	$$Y \sim \chi^2_{r_2} $$ 
	
	$$X \indep Y $$


	then, 

	$$ \frac{X/r_1}{Y_{r_2}} \sim F_{r_1, r_2} (s) $$
\end{definition}


We still have that

$$\mathcal{S} \ominus \mathcal{S}^\prime \perp \mathbb{R}^N \ominus \mathcal{S} $$

$$Cov( P_{\mathcal{S} \ominus \mathcal{S}^\prime}y, P_{\mathbb{R}^N \ominus \mathcal{S}}y) = 0 $$

$$P_{\mathcal{S} \ominus \mathcal{S}^\prime}y \indep P_{\mathbb{R}^N \ominus \mathcal{S}}y $$

These are still true even though $\mu \notin \mathcal{S}^\prime$. Why? Because $\mu \in \mathcal{S}$.

$$||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y||^2 \indep ||P_{\mathbb{R}^N \ominus \mathcal{S}}y ||^2  $$

so to compute power:

$$\frac{||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y ||^2 \ (p-k)}{|| P_{\mathbb{R}^n \ominus \mathcal{S}}y||^2 / (N-p)} \sim F_{p-k, N-p (||P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu ||^2)} $$


\textbf{Friday September 23}\\

\section{Scheffe's Simutaneous Confidence Intervals}\index{Scheffe's Simutaneous Confidence Intervals}

It's conceptually easy to construct individuals C.I.

$$P_\theta (\theta \in C(X)) = 1 - \alpha $$

We want to construct C.I. for several infinite sets of parameters. Then the width of the cofidence interval has to be adjusted (wider). 

\begin{enumerate}
	\item Boneroni adjustment
	\item Sheffe's approach
\end{enumerate}

\textbf{Simultaneous C.I.}\\

Say we have a set of parameters. 

		$$\{\theta_\lambda : \lambda \in \Lambda \} $$

Simultaneous C.I. for $\{\theta_\lambda : \lambda \in \Lambda \} $ is a family of subsets of $\Theta$ (the parameter space).

		$$\{C_\lambda(x) \subset \Theta : \lambda \in \Lambda \} $$ 

Where $C_\lambda(x)$ is a set in $\Theta$ depending only on x, that is it's a statistic.

		$$x \rightarrow 2^\Theta $$

This collection is called \textbf{Simultaneous Confidence Region} if 

		$$P(C_\lambda(x) \text{ covers } \theta_\lambda \forall \lambda \in \Lambda) = 1 - \alpha $$



In general linear model where,

		$$y = \mu + \epsilon, \quad \mu \in \mathcal{S} $$

		$$\mathcal{S}^\prime \leq \mathcal{S} $$

		$$\mathcal{S} \leq \mathbb{R}^N $$

		$$\epsilon \sim N(0, \sigma^2 I_N)$$

we are interested in constructing S.C.I. for

		$$\{C^T P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu: C \in \mathbb{R}^N \}$$

That is we want, 

		$$C_c(X): c \in \mathbb{R}^N$$

such that

		$$P(C^T P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu \in C_c(X) \forall c \in \mathbb{R}^N ) = 1 - \alpha$$

or equivalently, 

		$$P(d^T \mu \in C_d(X) \forall dc \subset \mathcal{S} \ominus \mathcal{S}^\prime ) = 1 - \alpha$$

Here, d has a special name.

\begin{definition}[Contrast]
	Suppose $\mathcal{S} \leq \mathcal{S} \leq \mathbb{R}^N$.  A \textbf{contrast} for hypothesis, 

		$$H_0: \mu \in \mathcal{S}^\prime$$
		$$H_1: \mu \in \mathcal{S} \ominus \mathcal{S}^\prime $$

	is $d^T\mu$ where $d \in \mathcal{S} \ominus \mathcal{S}^\prime $. 
\end{definition}

\textbf{Pivital Quantity}\\

\begin{definition}[Pivotal Quantity]
	$$X \sim P_\theta$$

	A \textbf{pivotal quantity} is a funciton $T(X, \theta)$ such that it's distribution under $P_\theta$ is independent of $\theta$. It's almost like an ancillary statistics, except it contains the parameter $\theta$. 
\end{definition}

\begin{theorem}
	Suppose $y \sim N(\mu, \sigma^2 I_N)$ and that $\mu \in \mathcal{S}$.\\

	 Let 

		$$\delta = P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu$$

	Let 
		$$F(\delta) = \frac{||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y - \delta ||^2}{(p-k) \tilde{\sigma}^2}$$

		where $p = \text{dim}(\mathcal{S})$ and $k = \text{dim}(\mathcal{S}^\prime)$

	Then,

		$$F(\delta) \sim F_{p-k, N-p} $$
\end{theorem}

This implies that $F(\delta)$ is a pivotal quantity because its distribution doesn't depend on $\delta$.

\begin{proof}
	We have
		$$P_{\mathcal{S} \ominus \mathcal{S}^\prime}y - \delta  = P_{\mathcal{S} \ominus \mathcal{S}^\prime}y - P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu$$

		$$P_{\mathcal{S} \ominus \mathcal{S}^\prime}(y - \mu) \sim N(0, P_{\mathcal{S} \ominus \mathcal{S}^\prime})$$

	So, 
		$$||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y - \delta ||^2 \sim \chi^2_{p-k}$$

	But we also know that 
		$$P_{\mathbb{R}^N \ominus \mathcal{S}} y \indep P_{\mathcal{S} \ominus \mathcal{S}^\prime}y  $$

	Recall, 

		$$||P_{\mathbb{R}^N \ominus \mathcal{S}}y||^2 \sim \sigma^2 \chi^2_{N-p} $$

	Take the ratio and use the definition of $\hat{\sigma}^2$ to complete the Theorem.
\end{proof}

\textbf{Equivalence Between Confidence Region and Hypothesis Test}\\

Consider the hypothesis test, 

		$$H_0: \{\theta\} $$
		$$H_1: \{\theta\}^C$$

at level $\alpha$. \\


A accpetance region is any subset $A_\theta \subseteq \mathcal{X}$ (the sample space) so that

		$$P_\theta(X \in A(\theta)) = 1 - \alpha $$

A acceptance region is a mapping from the parameter space to a subset of x. 

		$$\Theta \rightarrow 2^{\mathcal{X}}, \theta \mapsto A(\theta)$$

On the other hand, for each $x \in \mathcal{X}$ let

		$$C(x) = \{\theta: H_0 \text{ is accepted.} \} $$
		$$C(x) = \{\theta: x \in A(\theta) \} $$

By this definition, 

		$$P_\theta(\theta \in C(x)) = P_\theta \left(x \in A(\theta)\right) = 1-\alpha$$

As an illustration of this equivalence, let's construct a Confidene Region for $P_{\mathcal{S} \ominus \mathcal{S}^\prime} \mu$.

		$$H_0: P_{\mathcal{S} \ominus \mathcal{S}^\prime} \mu = \delta $$
		$$H_1: P_{\mathcal{S} \ominus \mathcal{S}^\prime} \mu \neq \delta $$

Suppose we use the acceptance rule. 

		$$F(\delta) < F_{p-k, N-p}(1-\alpha) $$

Then the $(1-\alpha)x 100\%$ Confidence Region for $\delta$ is

		$$\{\delta: F(\delta) < F_{p-k, N-p}(1-\alpha) \} $$

we can evaluate if $\theta$ in the set by computing this above criteria. 


\textbf{Monday September 26}\\

SCI for contrasts.\\

We are interested $C_d(x): d \in \mathbb{R}^N$ such that $P(d^T \mu \in C_d(X_, d \in \mathcal{S} \ominus \mathcal{S}^\prime)$.\\

It turns out we can only do this because we can use Cauchy-Schwarz Inequality for a uniform bound.\\

As before, $\hat{\delta} = P_{\mathcal{S} \ominus \mathcal{S}^\prime} y, \delta = P_{\mathcal{S} \ominus \mathcal{S}^\prime}\mu$. By CS$\neq$,

	$$|d^T(\hat{\delta} - \delta)^2 \leq ||d||^2 - || \hat{\delta} - \delta ||^2 $$

But we know (from last lecture) that

		$$||\hat{\delta} - \delta||^2 \sim \sigma^2 \chi^2_{p-q} $$


		$$\frac{||P_{\mathbb{R}^N \ominus \mathcal{S}}y||^2}{\sigma^2} \sim \chi^2_{N-p} $$

and also that they are independent. 

		$$\hat{\sigma}^2 = \frac{||P_{\mathbb{R}^N \ominus \mathcal{S}}y||^2}{N-p} $$

		$$\frac{||P_{\mathcal{S} \ominus \mathcal{S}^\prime}y - \delta||^2}{\sigma^2 (p-q)} \sim F_{p-q, N-p} $$

		$$P\left(||\hat{\delta} - \delta||^2 \leq \hat{\sigma}^2 (p-q) F_{p-q, N-p} (1-\alpha) \right) = 1 - \alpha $$

	Using CS$\neq$,

		$$P \left(\frac{d^T(\hat{\delta} - \delta)^2}{||d||^2} \leq \hat{\sigma}^2 (p-q) F_{p-q, N-p} (1-\alpha) \right) \geq 1-\alpha $$

		$$\Leftrightarrow P(d^t \delta \in d^T \hat{\sigma}^2 \pm || d|| \hat{\sigma} \sqrt{ (p-q) F_{p-q, N-p} (1-\alpha)}) \geq 1 - \alpha$$

In geometric terms, 

	$$d^t P_{\mathcal{S} \ominus \mathcal{S}^\prime}y \pm || d|| ||P_{\mathbb{R}^N \ominus \mathcal{S}}y || \sqrt{\frac{dim \mathcal{S} - dim \mathcal{S}^\prime}{N - \mathcal{S} }F_{dim \mathcal{S} - dim \mathcal{S}^\prime, N - \mathcal{S}} (1-\alpha)}   (***)$$

\section{Coordinate Version of SCI}\index{Coordinate Version of SCI}

Instead of $y \sim N(\mu, \sigma^2 I_n), \mu \in \mathcal{S}$, we can se that $y = X\beta + \epsilon, \epsilon \sim N(0, \sigma^2, I_n)$.\\

Typically we want to construct simultaneous SC for $\beta_1, \dots, \beta_p$, that is,

	$$P(\beta_1 \in C_1(x), \dots, \beta_p \in C_p(x)) \geq 1- \alpha $$

	

If we can construct SCI function forall $S^T\beta, S \in \mathbb{R}^p$ then we can solve the problem beacuse, 
	
		$$S = \begin{pmatrix}
			1\\
			0\\
			\vdots\\
			0
		\end{pmatrix} , \dots, \begin{pmatrix}
			0\\
			0\\
			\vdots\\
			1
		\end{pmatrix}$$

	More genrally, we may want to construct SCI for $\beta_1, \dots \beta_q$
 for some q less than p. In this case we need SCI in the form 

 	$$\left\{S^T (I_q \vdots 0) \beta: S\in \mathbb{R}^q
 	 \right\} (**) $$

 	 instead of $S^T \beta$.\\

 	 So we construct general SCI, 

 	 	$$S^T A^T \beta $$

 	 where $A \in \mathbb{R}^{pxq}$. This would accommodate both (*), (**). This can be cast into the gerneral SCI, then (***). Need $\mathcal{S}^\prime, \mathcal{S}, \mathbb{R}^N$.\\

 	 Consider $H_0: A^T \beta = 0 \Leftrightarrow A^T (X^T X)^{-1} X^T \mu = 0 \Leftrightarrow C^T \mu = 0 \Leftrightarrow \mu \in span(x) \ominus span(C)$.

 	 So 
 	 	$$\mathcal{S}^\prime = span(x) \ominus span(C) $$

 	 	$$\mathcal{S} = span(x)$$

 	 	$$\mathcal{S} \ominus \mathcal{S}^\prime = span(C) $$

 	 We are now testing $H_0: \mu \in \mathcal{S}^\prime$ against $H_1: \mu \in \mathcal{S}$.\\

 	 Compute specific experessions in (***) (general SCI form), 

 	 		$$P_{\mathcal{S} \ominus \mathcal{S}^\prime}y = P_{span(C)y} = P_C y  $$

 	 		$$= C(C^TC)^{-1}C^T y $$

 	 		$$= X(X^TX)^{-1}A [A^T (X^T X)^{-1} A ] A^T (X^T X)^{-1} X^T y $$

 	 		$$d \in \mathcal{S} \ominus \mathcal{S}^\prime = span(c) = Cs = X(X^TX)^{-1}As$$


 	 		$$d^T P_{\mathcal{S} \ominus \mathcal{S}^\prime} y = d^t P_C y = S^Ta^T(X^TX)^{-1}X^T  = S^T A^T (X^T X)^{-1} X^T y = S^T A^T \hat{\beta}$$ 

Recall that, 
 	 		$$dim(\mathcal{S}) = p, dim(\mathcal{S}^\prime) = p-q $$

 	so plug everything into (***) to get, $(1 - \alpha)$-level SCI (conservaative: Prob $\geq (1 - \alpha)$), 

 			$$S^T A^T \hat{B} \pm ||X(X^T X)^{-1} A s || \hat{\sigma} \sqrt{q F_{q, N-p} (1 - \alpha)} $$


 	To summarize, the whole procedure, suppose we wanted to construct SCI for $\beta_1, \dots, \beta_p$ or $\beta_1, \dots, \beta_q$ or $\beta_1 - \beta_2, \beta_2 - \beta_3 \dots, $.

 	Then, we let A be a matrix such that span(A) encloses (minimally) the above ranges of $\beta$s, so that  

 			$$\beta_j \text{ or } \beta_1 - \beta_2 = S^T A^T \beta $$

 	For example for $\beta_1, \dots, \beta_p$, 

 		$$A = I_p $$

 		$$A^T = (I_q \vdots 0)$$ 


 	For $\beta_1 - \beta_2, \beta_2 - \beta_3, \dots$, 

 		$$ A = \begin{pmatrix}
 			1 & 0 \\
 			-1 & 1 \\
 			0 & -1 \\
 			\vdots & 0 \\
 			0 & \vdots \\
 			0 & 0
 		\end{pmatrix} $$


 The idea of Scheffe SCI is to enlarge the set to linear space. That is, even though you only want 

 		$$e_1^T \beta, \dots, e_q^T \beta $$ 

 		$$e_i = \begin{pmatrix}
 			0\\
 			\vdots \\
 			0 \\
 			1 \\
 			0 \\
 			\vdots\\
 			0

 		\end{pmatrix}$$

 	you still contruct sCI for more parameters than you want. 

 		$$\{s^T (I_q \vdots 0) \beta: A \in \mathcal{R}^q \} $$


The disadvantage is that for smaller number of contrasts, this tends to be conservative. Here, you may use Bonferroni's Method. The advantage is that the width is fixed regardless of number of contrasts, as long as they are the same subspace. 


\textbf{Wednesday September 28}\\

Last time, we covered Scheffe's SCI; one feature is that SCI for infinitely many linear combination. If you just want to SCI for a few linear combinations then conservative apprach is needed. In this case, Bonferoni SCI is prefereed, but Bonferroni SCI gets wider and wider as the number of parameters inceases. So Scheffe's is preferred for large number of parameters. \\


\section{Bonferroni's SCI}\index{Bonferroni's SCI}

Suppose we want to SCI for $\theta_1, \dots, \theta_k$ (that is we want $C_1 (X), \dots, C_k(X)$) such that 

	$$P(\theta_1 \in C_1(X), \dots, \theta_k \in C_k(X) ) \geq 1 - \alpha $$

Let 

% GET FROM PHOTO

So if you let $P(A_i) = 1 - \frac{\alpha}{k}$ then 

	$$P ( \bigcap^n_{i=1} A_i) \geq 1 - k(1 - (1 - \frac{\alpha}{k})) = 1 - \alpha $$

So the $(1-\alpha)$- level SCI is simply $(1 - \frac{\alpha}{k})$-level ICI. (Recall, S - Simultanerous, I - Individual).\\

As $k \rightarrow 0$, $1 - \frac{\alpha}{k} \rightarrow 1$  (which is disadvantageous for large k). Specialize to linear regression, where we want Bonferroni SCI for $\alpha_1^T\beta, \dots, \alpha_q^T\beta$ where $\beta$ is as in, 
		
		$$y = X\beta - \epsilon $$

and 

	$$\alpha_1, \dots, \alpha_q \in \mathbb{R}^p$$

We know that 

	$$\hat{\beta} \sim N(\beta, \sigma^2 (X^TX)^{-1}) $$

and

	$$ (N-p) \hat{\sigma}^2 \sim \sigma^2 \chi^2_{N-p}$$

where note we are using the biased $\hat{\sigma}^2$. Finally we have

		$$\frac{(N-p)\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{N-p} \quad (*)$$

Ultimately, with the $\alpha$ we obtain, 

		$$\alpha_k^t \hat{\beta} \sim N(\alpha_k \beta, \sigma^2 \alpha_k^T(X^TX)^{-1}\alpha_k)  $$

		$$\frac{\alpha_k^T \hat{\beta} - \alpha_k^T \beta}{\sigma \sqrt{\alpha_k^T(X^TX)^{-1}\alpha_k}} \sim N(0,1) \quad (**)$$

Note that (*) and (**) are independent. \\

Studentize: 

% FINISH NOTES FROM PHOTO


%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{One-Way ANOVA}

\section{ANOVA Model and Test Statistic}\index{ANOVA Model and Test Statistic}

This is a special case of general linear model. 

	$$y_{ij} \sim N(\mu_i, \sigma^2) \quad j = 1, \dots, n_i,  $$

All $y_{ij}$ are independent. \\

In matrix form we get,

		$$Y = \begin{pmatrix}
			Y_{11} \\
			\vdots \\
			Y_{1 n_i}\\
			\vdots \\
			\vdots\\ 
			Y_{p1}\\
			\vdots\\
			Y_{p n_i}\\
		\end{pmatrix}  \sim N \left( \begin{pmatrix}
			\mu_1\\
			\vdots\\
			\mu_1\\
			\vdots\\
			\vdots\\
			\mu_p \\
			\vdots\\
			\mu_p\\
		\end{pmatrix}, \sigma^2 I_n \right) $$


		$$\mu = \begin{pmatrix}	
			1_{n_1} & \dots & 0 \\
			0 & \ddots &  0 \\
			0 & \dots & 1_{n_p}\\
		\end{pmatrix}	\begin{pmatrix}
			\\mu_1\\
			\vdots\\
			\mu_p\\
		\end{pmatrix} $$

	In this case, 

		$$Y \sim N(\mu, \sigma^2 I), \mu \in \mathcal{S}$$

		$$\mathcal{S} = \text{span}\begin{pmatrix}
			1_{n_1} & \dots & 0 \\
			0 & \ddots &  0 \\
			0 & \dots & 1_{n_p}\\
		\end{pmatrix} $$

		Because, $\mu$ is defined as above. 

So we have a special case of General linear model. \\

Once you know this form, we know everything: decomposition of Sum of Squares, F-Statistics, Scheffe's, Bonferroni's, etc. We just need to speicalize the formulae using a specific model. The same general principle applies to all the linear models yet to come. Here, we want to test

		$$H_0: \mu_1 = \dots = \mu_p $$
or
		$$H_0: \mu = \begin{pmatrix}
			\mu_1\\
			\mu_1
			\vdots\\
			\mu_1\\
		\end{pmatrix}  $$


		$$\mathcal{S}^\prime = \text{span}(1_N) $$


So our hypotheses are now 

		$$H_0: \mu \in \mathcal{S}^\prime$$
		$$H_1: \mu in \mathcal{S} $$


F-Statistic

		$$F = \frac{||P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y||^2 / \text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime)}{||P_{\mathbb{R}^N \ominus \mathcal{S}} Y||^2 / \text{dim}(\mathbb{R}^N \ominus \mathcal{S})} \sim F_{\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime), \text{dim}(\mathbb{R}^N \ominus \mathcal{S})} $$

Here we'd reject if

		$$F > F_{\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime), \text{dim}(\mathbb{R}^N \ominus \mathcal{S})} (1 - \alpha) $$


In one-way ANOVA we have some special names. 

		$$ ||P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y||^2 \leftarrow \text{SSH}$$

		$$ \text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime) = p - 1 $$

		$$||P_{\mathbb{R}^N \ominus \mathcal{S}} Y||^2 \leftarrow \text{SSE} $$

		$$ \text{dim}(\mathbb{R}^N \ominus \mathcal{S}) = N-P $$

		$$\frac{SSH}{p-1} = MSH, \frac{SSE}{N-p} = MSE $$

		$$ F = \frac{MSH}{MSE}$$

Due to the simple structure of $X = \begin{pmatrix}
	1_{n_1} & \dots & 0 \\
			0 & \ddots &  0 \\
			0 & \dots & 1_{n_p}\\
\end{pmatrix}$ we can see (in HW) that SSH and SSE have special forms. 


 $$SSH = \displaystyle \sum^P_{i =1} n_i (\bar{Y}_{i\cdot} - \bar{Y}_{\cdot\cdot})^2$$

 where $\bar{Y}_{i \cdot} = \frac{1}{n_i} \displaystyle \sum^{n_i}_{j=1} Y_{ij}$ and $\bar{Y}_{\cdot \cdot} ^2= \frac{1}{N} \displaystyle \sum^P_{i = 1} \sum^{n_i}_{j=1} Y_{ij}$. \\

 		$$SSE =  \displaystyle \sum^P_{i = 1} \sum^{n_i}_{j=1} (Y_{ij} - \bar{Y}_{\cdot \cdot})^2$$

 		$$SST = ||P_{\mathbb{R}^N \ominus \mathcal{S}^\prime} Y ||^2  $$

 	
\textbf{Friday September 30}\\

\section{Scheffe's SCI}\index{Scheffe's SCI}


In this case recall, 

		$$\mathcal{S} = \text{span}\{1_{n_1} \oplus \dots \oplus 1_{n_p} \}$$

		$$\mathcal{S}^\prime = \text{span}\{1_N \} $$

The general form from Scheffe's for GLH (General Linear Hypothesis).\\

		$$H_0: \mu \in \mathcal{S}^\prime $$
		$$H_1: \mu \in \mathcal{S} $$

		$$d^T \hat{\mu} \pm \hat{\sigma} ||d|| \sqrt{(p - 1) F_{p-1, N-p} (1 - \alpha)} $$

We want $d \in \mathcal{S} \ominus \mathcal{S}^\prime$. 

		$$X = \begin{pmatrix}
			1_{n_1} & & \\
			& \ddots & \\
			& & 1_{n_p}
		\end{pmatrix} = 1_{n_1} \oplus \dots \oplus 1_{n_p} $$

$d \in \mathcal{S} = \text{span}X$\\
$d = XC$\\
$d \perp \mathcal{S}^\prime$\\

This means that $d^T1_N = 0$ and $C^TX^T 1_N = 0 $.\\

		$$\left(C_1, \dots, C_p \right) \begin{pmatrix}
			1^T_{n_1} & & \\
			& \ddots & \\
			& & 1^T_{n_p}\\
		\end{pmatrix} \begin{pmatrix}
			1_{n_1}\\
			\vdots\\
			1_{n_p}
		\end{pmatrix} = C_1 n_1 + \dots + C_p n_p$$

So d is of the form $XC$ where $n_1C_1 + \dots + n_p C_p = 0$. 

		$$ \hat{\mu} = \begin{pmatrix}
			1_{n_1} \bar{Y}_{1\cdot}\\
			\vdots\\
			1_{np} \bar{Y}_{p \cdot}
		\end{pmatrix} = X \begin{pmatrix}
			\bar{Y}_{1\cdot}\\
			\vdots\\
			\bar{Y}_{p \cdot}\\
		\end{pmatrix} = P _{1_{n_1} \oplus \dots \oplus 1_{n_p} } Y = P_{\mathcal{S}} Y$$

		$$d^T \hat{\mu} = C^T (X^T X)  \begin{pmatrix}
			\bar{Y}_{1\cdot}\\
			\vdots\\
			\bar{Y}_{p \cdot}\\
		\end{pmatrix} = C^T \begin{pmatrix}
			n_1 & & \\
			& \ddots & \\
			& & n_p
		\end{pmatrix} \begin{pmatrix}
			\bar{Y}_{1\cdot}\\
			\vdots\\
			\bar{Y}_{p \cdot}\\
		\end{pmatrix} = c_1n_1 \bar{Y}_{1\cdots} + \dots + C_p n_p \bar{Y}_{p \cdots} $$


		$$||d|| = \sqrt{d^t d} = \sqrt{c^T X^T X C } = \sqrt{\sum^P_{i = 1} C_i^2 n_i} $$


We have $\hat{\sigma}^2$ as before. \\

		$$\frac{||P_{\mathbb{R}^N \ominus \mathcal{S}} Y ||^2}{N - p} = \frac{\displaystyle \sum^P_{i = 1} \sum^{n_i}_{j=1} (Y_{ij} - \bar{Y}_{\cdot \cdot})^2}{N- p} = MSE $$


A alternative parameterization $t_i = n_i c_i$.\\


In this from, let $\hat{v} = \begin{pmatrix}
	\bar{Y_{1 \cdot}} \\
	\vdots \\
	\bar{Y}_{p \cdot}
\end{pmatrix}$\\

The SCI is 
		$$t^T \hat{v}  \pm \hat{\sigma} \sqrt{\sigma(t_i^2/n_i) (p-1) F_{p-1, N-p} (1 - \alpha)} $$

Commonly need contrasts are $\mu_i - \mu_i^\prime, \mu_1 - 3 \mu_2 + 2\mu_3 $.\\

The usually mention hypothesis, 

		$$\mu_1 = \dots = \mu_p, \quad \mu_i = \mu_i^\prime \quad \forall i \neq i^\prime $$


Test equality of subsets of mean (as given before) just use 

		$$t = \begin{pmatrix}
			0\\
			\vdots\\
			0\\
			1\\
			0\\
			\vdots\\
			0\\
			-1\\
			0\\
			\vdots\\
			0
		\end{pmatrix} $$

Where 1 is the $i^{th}$ entry and -1 is the $i^{\prime th}$ entry.\\

We can do serveral of these.\\

		$$t^T \hat{v} = \bar{Y}_{i \cdot} - \bar{Y}_{i^\prime \cdot} $$

		$$\sum \frac{t_i^2}{n_i} = \frac{1}{n_i} - \frac{1}{n_{i^\prime}} $$

\section{Bonferonni SCI}\index{Bonferonni SCI}

Say we want to construct SCI. 

		$$\{t_1^T v, \dots, t^T_q v \} $$ 

As discussed before, SCI, 

		$$ t_k^T \hat{v} \pm t_{N-p} \left(1 - \frac{\alpha}{2q}\right) \hat{\sigma} \sqrt{t_k^T (X^TX)^{-1}t_k} $$

Here, 

		$$X^TX = \begin{pmatrix}
			n_1 & & \\
			 & \ddots & \\
			 & & n_p 
		\end{pmatrix} $$

and, 

		$$t^T_k(X^T X)^{-1} t_k = \displaystyle \sum^P_{i=1} \frac{1}{n_i}t^2_{k_i} $$








%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Mutiway ANOVA}

\section{Orthogonal Design}\index{Orthogonal Design}


Recall, the General Linear Model: $Y \sim N(\mu, \sigma^2 I), \mu \in \mathcal{S}, \mathcal{S} \leq \mathbb{R}^N$. \\


Orthogonal esigns mean that $\mathcal{S}$ can be decomposed in to $\mathcal{S} = \mathcal{S}_1 \oplus \dots \oplus \mathcal{S}_\nu$. Later on:\\
\\
$\mathcal{S}_1$ factor A\\
$\mathcal{S}_2$ factor B\\
$\mathcal{S}_3$ interacts\\

In this case, 

	$$\hat{\mu} = P_{\mathcal{S}} Y = P_{\mathcal{S}_1} Y + \dots + P_{\mathcal{S}_\nu} Y$$

	$$ \mu = P_{\mathcal{S}} \mu = P_{\mathcal{S}_1} \mu + \dots + P_{\mathcal{S}_\nu} \mu = v_1 \in \mathcal{S}_1 + \dots + v_\nu \in \mathcal{S}_\nu $$


Unique Decomposition is covered in Chapter 1. \\

Suppose we want to test that there is no interaction: 

		$$H_0: v_i = 0 $$
		$$H_1: v_i \neq 0 $$

This is equivalent to 

		$$H_0: \mu \in \oplus_{j \neq i} \mathcal{S}_j$$
		$$H_1: \mu \in \mathcal{S} $$

In this case,  $ \mathcal{S}^\prime = \oplus_{j \neq i} \mathcal{S}_i, \mathcal{S} = \oplus^\nu_{j = 1} \mathcal{S}_j$. \\

So $\mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_i.$ Thus the F-statistics for GLH is

		$$\frac{||P_{\mathcal{S}_i} Y ||^2/ d_i}{|| P_{\mathbb{R}^N \ominus \mathcal{S}} Y ||^2 / (N-p)}  \stackrel{H_0}{\sim} F_{\alpha_i, N-p}$$

where $d_i = \text{dim}(\mathcal{S}_i)$

If we don't have orthogonality, suppose we have GLM, 

		$$Y = X_1 \beta_1 + \dots + X_p \beta_p + \epsilon $$

Letting $\beta_i = 0$, simply means

		$$\mu \in \text{span}(X_1, \dots, X_{i-1}, X_{i + 1}, \dots, X_p ) (= \mathcal{S}^i) $$


Since $X_1, \dots, X_p$ are not ortogonal, this is not $\text{span}(X) \ominus \text{span}(X_i)$ so we have that $\mathcal{S} \ominus \mathcal{S}^\prime \neq \text{span}(X_i)$. \\

Moreover, in the orthogonal case, the point estimation of $\beta_i$ relies entirely on $(Y, X_i)$. \\

Screening (?) on Variable Selection\\

In the orthogonal case, they are the same. We must demonstrate this. \\

		$$\hat{\beta } = (X^T X)^{-1} X^T Y $$


If $X_1 \perp \dots \perp X_p$, 

		$$X^T X = \begin{pmatrix}
			X_1^T X_1 & & 0 \\
			 & \ddots &  \\
			 0 & & X_p^T X_p
		\end{pmatrix} $$

		$$\hat{\beta} = \begin{pmatrix}
			\frac{X_1^T Y}{X_1^T X_1}\\
			\vdots\\
			\frac{X_p^T Y}{X_p^T X_p}\\
		\end{pmatrix} $$

		$$\hat{\beta}_i = \frac{X_i^T Y}{X_i^T X_i} $$

So to get $\beta_i$ you simply regress Y on $X_i$ which doesn't involve the other column.\\

Another effect of orthogonlity is you can decompose sum of squares additively. 

		$$||P_\mathcal{S} Y ||^2  = ||P_{\mathcal{S}_1} Y ||^2  + \dots ||P_{\mathcal{S}_\nu} Y ||^2 $$

You can tabulate this nicely in ANOVA table. \\

If no orthogonality then you don't report $||P_{\mathcal{S}_i} Y ||^2 $ as the sum of squares associated with $\beta_i$. \\

The correct sum of squares, 

		$$||P_{\text{span}(X) \oplus \text{span}(X_{-i})} Y ||^2$$

where $X_{-1} = (X_1, \dots, X_{i - 1}, X_{i+1}, \dots, X_p)$. \\

So even though you can sitll tabulate these substitutes they don't sum up to $||P_\mathcal{S} Y ||^2 $. \\

This is less meaningful than ANOVA table. In generalized linear models we have to be content with this inperfect ANOVA. 



\textbf{Monday October 2}\\

\section{Two-Way ANOVA (without Interactions)}\index{Two-Way ANOVA}

\textbf{Model - special case of general linear model}\\

		$$Y_{ijk} \sim N(\mu_{ij}, \sigma^2 I_N) $$

		$$k= 1, \dots, n_{ij} $$

		$$j = 1, \dots, c $$

		$$i = 1, dots, r $$

Assume (for now), 

		$$\mu_{ij} = \gamma_i + \tau_j $$

\textbf{Orthoginal Design}\\

To ensure orthoginal design, $n_{ij} = p_iq_i$, where $p_i, q_i$ are positive intigers. We will show that this condition ensures orthoganality. \\


\begin{notation}[Dot Notation]
	$n_{i\cdot} $ indicates that the sec
\end{notation}

Apply this notation to both numbers and matrix/vector notation. 

	$$n_{ij} = p_i q_j $$

	$$n_{i\cdot} = p_i q_\cdot $$

	$$ n_{\cdot j} = p_\cdot q_j $$

	$$ n_{\cdot \cdot} = p_\cdot q_\cdot $$

	

Orthogonal design means

		$$\frac{n_{i\cdot}n_{\cdot j}}{n_{\cdot \cdot}} = n_{ij} $$

		% INSERT PHOTO 

\textbf{Matrix Notation}\\

		$$ \mu = \begin{pmatrix}
			\mu_{11}\\
			\vdots \\
			\mu_{11}\\
			\mu_{12}\\
			\vdots \\
			\mu_{12}\\
			\vdots
		\end{pmatrix} = \begin{pmatrix}
			1_{n_{11}} (\gamma_1 + \tau_1)\\
			\vdots\\
			1_{n_{ij}} (\gamma_i + \tau_j)\\
			\vdots\\
			1_{n_{rc}} (\gamma_r + \tau_c)\\
		\end{pmatrix} \in \mathbb{R}^{n_{\cdot \cdot}}$$

			$$\{1_{n_{ij}} (\gamma_i  + \tau_j), i = 1, \dots, r; j = 1, \dots, c \} $$

Note that the latter index varies first. \\

Now, we introduce a systematic $\delta$ notation that will also be useful later. 


\begin{notation}
	For 
			$$(i,j) = \{1, \dots, r \} x \{1, \dots, c \} $$

			$$(u,v) = \{1, \dots, r \} x \{1, \dots, c \} $$

			$$w = 1, \dots, n_{uv} $$


	We have

		    % $$\delta^{ij}_{uvw} =  \begin{array}
		    % 	1, \text{ if (u,v) = (i,j)} \\
		    % 	0, \text{ else}
		    % \end{array}$$

\end{notation}

That means there are $n_{uv}$ 1's in this vector and $n_{\cdot \cdot} - n_{uv}$ 0's. \\

Let $E_{ij} = \{\delta^{ij}_{uvw}: u = 1, \dots, r, v = 1, \dots, c, w = 1, \dots, n_{uv}  \}$ where the last index funs first. \\

Let 	
		\begin{align}
			E_{i\cdot} &= \sum^n_{j=1} E_{ij}\\
			E_{\cdot j} &= \sum^n_{i=1} E_{ij}\\
		\end{align}


			$$R = \left(E_{i \cdot}, \dots, E_{r \cdot} \right) $$

			$$C = \left(E_{ \cdot j}, \dots, E_{\cdot c} \right) $$

In this notation, $\mu = \{\mu_{ij}: k = 1, \dots, n_i, j = 1, \dots, c, j = 1, \dots, r \}$. 

		$$\mu = E_{1\cdot} \gamma_1 + \dots + E_{r \cdot}\gamma_r + E_{\cdot 1} \tau_1 + \dots + E_{ \cdot c}\tau_c = R\gamma + C\tau $$

In the above, both $\gamma$ and $\tau$ are column vectors. \\

\textbf{Overall Mean}\\

		$$\mu = P_{1_n} \mu + Q_{1_n}\mu $$

Where, 

		$$P_{1_n} = \frac{1_N 1_N^T}{1_N^T 1_N} $$

		$$Q_{1_n} = I_N - P_{1_n} $$

So we may write $\mu$ as, 

	\begin{align*}
		 \mu &= P_{1_n} \mu + Q_{1_n}(R\gamma + C\tau)\\
		 		&= P_{1_n}\mu + \alpha + \beta\\
	\end{align*}

It turns out that $\frac{n_{i\cdot} n_{\cdot j}}{n_{\cdot \cdot}}$ ensures that $\alpha^T\beta = 0$. We must show this. 


		\begin{align*}
			\alpha^T\beta &= (Q_{1_n}R\gamma)^T(Q_{1_n} C \tau)\\
				&= \gamma^T (R^T \frac{Q_{1_N}Q_{1_N}}{Q_{1_N}} C ) \tau\\
			R^T Q_{1_n} C &= R^T (I_N - \frac{1_N 1_N^T}{1_N}) C \\
				&= R^T C  - \frac{R^T 1_N 1_N^T C}{N}\\
			R^T C = \begin{pmatrix}
				E_{1 \cdot}^T\\
				\vdots\\
				E_{r \cdot}^T
			\end{pmatrix} (E_{1 \cdot}, \vdots,	E_{ \cdot c})	
		\end{align*}


Now look at, 

		\begin{align*}
			E_{i \cdot}^T E_{\cdot j} &= \left( \sum^c_{s=1} E_{is} \right)^T \left(\sum^r_{t=1} E_{tj} \right)\\
				&= \sum^c_{s=1} \sum^r_{t=1} E^T_{is} E_{tj}
		\end{align*}

		% MORE NOTES


Hence, 

		$$R^TC = \begin{pmatrix}
			n_{11}&  \dots &  n_{1c}\\
			\vdots & & \vdots\\
			n_{r1}  & \dots & n_{rc}
		\end{pmatrix} $$
 
 		$$R^T 1_N = \begin{pmatrix}
 			E_{1 \cdot}^T\\
 			\vdots\\
 			E_{r\cdot}^T
 		\end{pmatrix} 1_N = \begin{pmatrix}
 			n_{1 \cdot}\\
 			\vdots\\
 			n_{r\cdot}
 		\end{pmatrix}$$

 		$$1^T_N C = (n_{\cdot 1}, \dots, n_{\cdot c}) $$


 		$$ \frac{R^T 1_N 1^T_N C }{N } = \begin{pmatrix}
 			\frac{n{1 \cdot} n_{\cdot 1}}{n_{\cdot \cdot}} & \dots & \dots \\
 			 & \ddots & \\
 			 \vdots & \dots & \ddots
 		\end{pmatrix}$$

So by orthogonal density, \\
\\

\textbf{Wednesday October 5}\\

\textbf{Geometric Representation}\\

		$$\mathcal{S}_1 = \text{span}(1_N) $$
		$$\mathcal{S}_2 = \text{span}(R) \ominus \text{span}(1_N) $$
		$$\mathcal{S}_3 = \text{span}(C) \ominus \text{span}(1_N) $$

By construction, $\mathcal{S}_1 \perp \mathcal{S}_2, \mathcal{S}_1 \perp \mathcal{S}_3$.\\

Since, 	
		$$ \mathcal{S}_2 = Q_{1_N} \text{span}(R) = \text{span}(Q_{1_N} R) $$ 
		$$ \mathcal{S}_3 = \text{span}(_{1_N} C) $$

we know that $R^T Q_N C = 0$ by orthogonal design. \\

So $\mathcal{S}_2 \perp \mathcal{S}_3.$\\

So it is justified to write

		$$\mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3 $$

which means that $\mathcal{S}_1 + \mathcal{S}_2 + \mathcal{S}_3 $ and $\mathcal{S}_1 \perp \mathcal{S}_2 \perp \mathcal{S}_3 $


\section{Testing Hypotheses}\index{Testing Hypotheses}


		$$\mu_{ij} = \gamma_i + \tau_j = (\bar{\gamma} + \bar{\tau}) + (\gamma_i - \bar{\gamma}) + (\tau_j - \bar{\tau})$$

Where $(\bar{\gamma} + \bar{\tau}) \in \mathcal{S}_1, (\gamma_i - \bar{\gamma}) \in \mathcal{S}_2, (\tau_j - \bar{\tau}) \in \mathcal{S}_3 $. 

		$$\mu_{ij} = w + \alpha_i + \beta_j $$ 

$\sum \alpha_i = 0 \Leftrightarrow \mathcal{S}_2 \perp \mathcal{S}_1$\\
$\sum \beta_j = 0 \Leftrightarrow \mathcal{S}_3 \perp \mathcal{S}_1$\\


We can test these hypotheses (among many other hypothoses), \\

		\begin{enumerate}[label = \Roman*]
			\item $H_0: \alpha_1 = \dots = \alpha_r = 0$ (no row effect)
			\item $H_1: \beta_1 = \dots = \beta_c = 0$ (no column efect)
		\end{enumerate}



For Hypothesis I, 

		$$\mu \perp \mathcal{S} \Leftrightarrow \mu \in \mathcal{S}_1 \oplus \mathcal{S}_3 $$

		$$\mathcal{S}^\prime = \mathcal{S}_1 \oplus \mathcal{S}_3, \mathcal{S} = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3 $$

So specialized on GLM, \\

	F-Statistic: 

			$$\frac{||P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y || ^2 / \text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime)}{|| P_{\mathbb{R}^N \ominus \mathcal{S}} Y|| ^2 / \text{dim}(\mathbb{R}^N \ominus \mathcal{S})} \sim F_{\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime), \text{dim}(\mathbb{R}^N \ominus \mathcal{S})} $$

	Specialize to our own context: 

			$$P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y = \{\bar{Y}_{\cdot \cdot} - Y_{\cdot \cdot \cdot}; k= 1, \dots, n_{ij}; i = 1, \dots, r; j = 1, \dots, c \} $$

			$$||P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y||^2 = \sum^r_{i=1} n_{i \cdot} (\bar{Y}_{\cdot \cdot} - Y_{\cdot \cdot \cdot})^2 $$


			$$\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime) = \text{dim}(\mathcal{S}_2) = r - 1 $$

			$$P_{\mathbb{R}^N \ominus (\mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3)}  $$

Now let's take just the subscript and apply results from HW problem (note the $\mathcal{S}$ are not the exact same), 

			$$\mathbb{R} \ominus (\mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3) = (\mathbb{R}^N \ominus \mathcal{S}_1) \ominus (\mathcal{S}_2 \oplus \mathcal{S}_3) $$

So we may rewrite above and again apply result from homework to get, 

			$$P_{(\mathbb{R}^N \ominus \mathcal{S}_1) \ominus (\mathcal{S}_2 \oplus \mathcal{S}_3)} =  P_{(\mathbb{R}^N \ominus \mathcal{S}_1)}  - P_{(\mathcal{S}_2 \oplus \mathcal{S}_3)} = P_{(\mathbb{R}^N \ominus \mathcal{S}_1)}  - P_{(\mathcal{S}_2} - P_{\mathcal{S}_3)} $$

So, 

			$$||P_{\mathbb{R}^N \ominus \mathcal{S}} Y|| ^2 = ||P_{\mathbb{R}^N \ominus \mathcal{S}_1} Y|| ^2 - ||P_{\mathcal{S}_2} Y|| ^2 - ||P_{ \mathcal{S}_3} Y|| ^2 $$

This is left as HW. 


		$$||P_{\mathbb{R}^N \ominus \mathcal{S}} Y|| ^2 = \{Y_{ijk} - \bar{Y}_{i \cdot \cdot} - \bar{Y}_{ \cdot j \cdot} + \bar{Y}_{\cdot \cdot \cdot}, \dots\} $$

		$$||P_{\mathbb{R}^N \ominus \mathcal{S}_1} Y|| ^2 = \{ Y_{ijk} - \bar{Y}_{\cdot \cdot \cdot}, k = \dots \} $$

		$$||P_{\mathcal{S}_2} Y|| ^2 = \{\bar{Y}_{i \cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot}, \dots \} $$

		$$||P_{ \mathcal{S}_3} Y|| ^2 = \{ \bar{Y}_{ \cdot j \cdot} +  - \bar{Y}_{\cdot \cdot \cdot}, \dots\} $$


		% get from photo



So $F_I$ becomes the familiar form, 

		$$F_I = \frac{MSR}{MSE} $$

$MSR = SSR / (r = 1)$\\
$SSR = \displaystyle \sum^r_{i = 1} n_{i\cdot} (\bar{Y}_{j \cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot})$\\
$MSE = SSE / (N - r -c +1)$\\
$SSE = \displaystyle \sum^r_{i = 1} \sum^c_{j = 1} \sum^{n_{ij}}_{k = 1} (Y_{ijk} - \bar{Y}_{i \cdot \cdot} - \bar{Y}_{ \cdot j \cdot} + \bar{Y}_{\cdot \cdot \cdot} ) $\\


		$$F_I \sim F_{r - 1 , N - r - c + 1}$$

Similarly, for testing Hypothesis II, still using GHL, 

		$$\beta_1  = \dots = \beta_c = 0 $$

		$$\mu \perp \mathcal{S}_3$$

		$$\mu \in \mathcal{S}_1 \oplus \mathcal{S}_2 $$

		$$\mathcal{S}^\prime = \mathcal{S}_1 \oplus \mathcal{S}_2 $$

		$$\mathcal{S} = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3 $$

		$$\mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_3 $$



		$$||P_{\mathcal{S} \ominus \mathcal{S}^\prime} ||^2 = |P_{\mathcal{S}_3} ||^2 = \sum^{c}_{j = 1} n_{ij} (\bar{Y}_{ \cdot j \cdot} -  \bar{Y}_{\cdot \cdot \cdot} )^2 = SSC $$


		$$ \text{dim}(\mathcal{S}_3) = c - 1 $$

		$$F_{II} = \frac{MSC}{MSE} $$


$MSC = SSC / (c-1)$\\
SSC is as above. \\

		$$F_{II} \sim F_{c-1, N -r -c + 1} $$


		Now we may summarize everything into the ANOVA table. 

		% INSERT PHOTO


\textbf{Friday October 7}\\

\section{Scheffe's SCI}\index{Scheffe's SCI}

Recall, the general hypothesis,

		$$H_0: \mu \in \mathcal{S}^\prime, $$
		$$H_1: \mu \in \mathcal{S}$$

We want SCI for contrasts,

		$$\{c^T \mu: c \in \mathcal{S} \ominus \mathcal{S}^\prime \} = \{c^T \delta: c \in \mathcal{S} \ominus \mathcal{S}^\prime \} $$


where $\delta = P_{\mathcal{S} \ominus \mathcal{S}^\prime} \mu$. \\

This because
		
		$$c = P_{\mathcal{S} \ominus \mathcal{S}^\prime} c $$

so, 

		$$C^T \mu = C^T  P_{\mathcal{S} \ominus \mathcal{S}^\prime} \mu = C^T \delta$$


We have

		$$C^T P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y \pm ||C|| ||P_{\mathcal{S} \ominus \mathcal{S}^\prime}Y|| $$

		$$\sqrt{ \frac{\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime)}{\text{dim}(\mathbb{R}^N \ominus \mathcal{S})} F_{\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime), \text{dim}(\mathbb{R}^N \ominus \mathcal{S})} } $$


Here we have two $H_0 $ of interest. 

	\begin{enumerate}[label = \Roman*]
		\item $$\alpha_1 = \dots = \alpha _r = 0 \Leftrightarrow \mu \in \mathcal{S}_1 \oplus \mathcal{S}_3 = \mathcal{S}^\prime$$

		$\mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_2$\\

So 

		\begin{align*}
			\delta = P_{\mathcal{S}_2}\mu\\
				&= \alpha \\
				&= \{\alpha_i: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \}
		\end{align*}


Note that

		$$C \in \mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_2 $$

So C is of the form

		$$C = \{C: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \} $$

But we also know that $C \perp 1_N$. Therefore, 


		$$\displaystyle \sum^r_{i=1} \sum^c_{j=1} \sum^{n_{ij}}_{k=1}  C_i =\displaystyle \sum^r_{i=1} C_i \sum^c_{j=1} \sum^{n_{ij}}_{k=1}  1 =  \sum^r_{i=1} C_i * n_{i\cdot} = 0$$

		$$P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y =  P_{\mathcal{S}_2} Y = \{\bar{Y}_{i\cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot}: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r  \}$$ 


		$$C^T P_{\mathcal{S}_2}Y = \displaystyle \sum^r_{i=1} \sum^c_{j=1} \sum^{n_{ij}}_{k=1}  C_i (\bar{Y}_{i\cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot})  =\sum^r_{i=1} C_i * n_{i\cdot} (\bar{Y}_{i\cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot}) $$

So usually we use this alternative parameterization, 

		$$t_i = n_{i\cdot} c_i$$


So $\displaystyle \sum^r_{i=1} t_i = 0$.\\

		$$ C^T P_{\mathcal{S}_2}Y = \displaystyle \sum^r_{i=1} t_i (\bar{Y}_{i\cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot})$$


$\text{dim}(\mathcal{S} \ominus \mathcal{S}^\prime) = \text{dim}(\mathcal{S}_2) = r - 1$\\
$\text{dim}(\mathbb{R}^N \ominus \mathcal{S}) = N -r - c + 1$\\

		\begin{align*}
			||c||^2 &= \displaystyle \sum^r_{i=1} \sum^c_{j=1} \sum^{n_{ij}}_{k=1}  C_i^2\\
				&= \displaystyle \sum^r_{i=1} n_{i \cdot} C_i^2\\
				&= \displaystyle \sum^r_{i=1} \frac{t_i^2}{n_i}
		\end{align*}


		$$ P_{\mathbb{R}^N \ominus \mathcal{S}} = \displaystyle \sum^r_{i=1} \sum^c_{j=1} \sum^{n_{ij}}_{k=1} (Y_{ijk} - \bar{Y}_{i\cdot \cdot} - \bar{Y}_{\cdot i \cdot} + \bar{Y}_{\cdot \cdot \cdot})^2$$

So to summarize Scheffe's SCI for $H_0$ is 


		% FINISH FROM PHOTO

		\item $H_0: \beta_1 = \dots = \beta_c = 0$


	\end{enumerate}
 


\section{Non-additive 2-way ANOVA (with Interactions)}\index{Non-additive 2-way ANOVA (with Interactions)}

\textit{"The whole is greater than the sum of its parts."}\\

In this case, we have, 

		$$Y_{ijk} \sim N(\mu_{ij}, \sigma^2) $$

where $\mu_{ij}$ cannot be decomposed. \\

So 

		$$\mu_{ij} = \theta + \alpha_i + \beta_j + \gamma{ij} $$

$k = 1, \dots, n_{ij}$\\
$ j = 1, \dots, c$\\
$ i = 1, \dots, r $\\

$\mathcal{S}_1 = \text{span}(\theta: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r )$\\

$\mathcal{S}_2 = \text{span}(\alpha_i: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r )$\\

$\mathcal{S}_3 = \text{span}(\beta_j: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r )$\\

$\mathcal{S}_4 = \text{span}(\gamma_{ij}: k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r )\ominus (\mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3)$\\

with $n_{ij} = \phi_i \epsilon_j$ (orthogonal design)\\

		$$\mathcal{S}_1 \perp \mathcal{S}_2 \perp \mathcal{S}_3 \perp \mathcal{S}_4 $$

So that it is justified to write

		$$\mathcal{S}_1 + \dots \mathcal{S}_4 = \mathcal{S}_1 \oplus \dots \oplus \mathcal{S}_4$$

Using matrix notation as before, 


$\mathcal{S}_1 = \text{span}(1_N)$\\

$\mathcal{S}_2 = \text{span}\{E_{1\cdot} \dots E_{r \cdot}\} \ominus \mathcal{S}_1$\\

$\mathcal{S}_3 = \text{span}\{E_{1\cdot} \dots E_{ \cdot c}\} \ominus \mathcal{S}_1$\\

$\mathcal{S}_4 = \text{span}\{E_{ij}:j = 1, \dots, c; i = 1, \dots, r \} \ominus (\mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3)$\\


		$$E_{ij} = \{d^{ij}_{uvw}: w = 1, \dots, n_{uv}; v = 1, \dots, c; u = 1, \dots, r \} $$

$\mathcal{S} = \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3 \oplus \mathcal{S}_4 = \text{span}\{E_{ij}:j = 1, \dots, c; i = 1, \dots, r\} $ \\


The projections, $P_{\mathcal{S}_i} Y, i = 1, \dots, 4$ and $P_{\mathbb{R}^N \ominus \mathcal{S}}$ are derived similarly. \\





\textbf{Monday October 10}\\

$\mathbb{R}^N \ominus \mathcal{S}$ is the garbage, but it's very useful for testing. \\


Explicit expression of projections: (check youself in HW)\\

		$$P_{\mathcal{S}_1} Y = \{\bar{Y}_{\cdot \cdot \cdot}:k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \} $$

		$$P_{\mathcal{S}_2} Y = \{\bar{Y}_{i \cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot} :k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \} $$

		$$P_{\mathcal{S}_3} Y = \{\bar{Y}_{\cdot j \cdot} - \bar{Y}_{\cdot \cdot \cdot} :k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \}$$ 

		$$P_{\mathcal{S}_4} Y = \{\bar{Y}_{ij \cdot}  - \bar{Y}_{i \cdot \cdot} - \bar{Y}_{\cdot j \cdot} + \bar{Y}_{\cdot \cdot \cdot} :k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \}$$


		$$ P_{\mathbb{R}^N \ominus\mathcal{S}} Y = \{Y_{ij k} - \{\bar{Y}_{ij \cdot}:k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r \}$$


We may extend this to, 


		$$||P_{\mathcal{S}_1} Y||^2 = N  \bar{Y}_{\cdot \cdot \cdot}^2 = $$

		$$||P_{\mathcal{S}_2} Y||^2 = \sum^r_{i=1} n_{i\cdot}  (\{\bar{Y}_{i \cdot \cdot} - \bar{Y}_{\cdot \cdot \cdot})^2 = SSR$$

		$$||P_{\mathcal{S}_3} Y||^2 = \sum^c_{j=1} n_{\cdot j}  (\{\bar{Y}_{ \cdot j \cdot} - \bar{Y}_{\cdot \cdot \cdot})^2 = SSC$$

		$$||P_{\mathcal{S}_4} Y||^2 = \sum^r_{i=1}\sum^c_{j=1} n_{ij}  (\{\bar{Y}_{ij \cdot}  - \bar{Y}_{i \cdot \cdot} - \bar{Y}_{\cdot j \cdot} + \bar{Y}_{\cdot \cdot \cdot})^2 = SSRC$$

		$$||P_{\mathbb{R}^N\ominus \mathcal{S}} Y||^2 = \sum^r_{i=1} \sum^c_{j=1} \sum^{n_{ij}}_{k=1} n_{ij}   (Y_{ij k} - \{\bar{Y}_{ij \cdot})^2 = SSE$$


$df(R) = r- 1$\\
$df(C) = c - 1$\\
$df(RC) = rc - r - c + 1$\\


How do we get this last degrees of freedom? 

	$$\text{dim(span}(E_{ij})) - \text{dim}(\mathcal{S}_1) - \text{dim} (\mathcal{S}_2 ) -  \text{dim}(\mathcal{S}_3) $$


$df(E) = N - rc$\\


We test the following hypotheses. \\

R effect:

		$$H_R: \alpha_1 = \dots = \alpha_r = 0$$
		$$H_C: \beta_1 = \dots = \beta_c = 0$$
		$$H_{RC}: \gamma_{ij} = 0$$


SSH (where H is null hypothesis), 

		$$H = R, C, RC$$

We use

		$$\frac{MSH}{MSE} = \frac{SSH/df(SSH)}{SSE/df(SSE)} \sim F_{df(SSH), df(SSE)} $$

For example, if H = RC, 

		$$SSH = SSSRC $$
		$$df(SSH) = rc - r - c +1$$

		$$\frac{MSRD}{MSE} \sim F_{rc - r - c + 1, N - rc}$$




\section{Scheffe SCI for 2-Way ANOVA with Interactions}\index{Scheffe SCI for 2-Way ANOVA with Interactions}


Again this depends on which hypothesis (R, C, RC) you are interested in. \\

For $H_{RC}$

		$$c^T P_{\mathcal{S}_4} Y \pm ||C|| ||P_{\mathbb{R}^N \ominus \mathcal{S}} Y || - \sqrt{\frac{dim(\mathcal{S}_4)}{dim(\mathbb{R}^N \ominus \mathcal{S})} F_{dim(\mathcal{S}_4), dim(\mathbb{R}^N \ominus \mathcal{S})}(1 - \alpha)} $$


First, figure out specific form of C.\\

		$$C \in \mathcal{S} \ominus \mathcal{S}^\prime $$
		$$C \in \mathcal{S} \Rightarrow C = \{C_{ij}:k = 1, \dots, n_{ij}; j = 1, \dots, c; i = 1, \dots, r  \}$$


Also, 

		$$C^T 1_N = 0, C^T E_{i \cdot} = 0, C^T E_{\cdot j} = 0, $$

To show this, 

		\begin{align*}
			\displaystyle \sum_i \sum_j \sum_k C_{ij} 1_N &= \displaystyle \sum_i \sum_j C_{ij} \sum_k 1_N\\
				&= \displaystyle \sum_i \sum_j C_{ij} n_{ij}\\
				&= 0
		\end{align*}

An simililarly for the other two equations equal to zero. \\

So if we let $t_{ij} = C_{ij} n_{ij}$ we can see again that summing it over i,j or i, or j all give zero. \\
\\

\textbf{Scheffe's SCI for Interactions}\\

		$$\displaystyle \sum^r_{i=1} \sum^c_{j=1} (\{\bar{Y}_{ij \cdot}  - \bar{Y}_{i \cdot \cdot} - \bar{Y}_{\cdot j \cdot} + \bar{Y}_{\cdot \cdot \cdot})  \pm \sqrt{\sum^r_{i=1} \sum^c_{j=1} \frac{t_{ij}^2}{n_{ij}}} \sqrt{\sum^r_{i=1} \sum^c_{j=1} \sum^{n_{ij}}_{k=1} (Y_{ij k} - \bar{Y}_{ij \cdot})^2 } \sqrt{\frac{rc - r - c + 1}{N - rc} F_{rc - r - c + 1, N - rc}(1 - \alpha)}$$


\section{Latin Squares}\index{Latin Squares}


Suppose we have three main effects. In general, if you have three effects you need to make a cube for the design and observations.\\
\\

Sometimes experiments over the entire cube would require too much time/money/etc. Can we test three effects using two-way table? Intuitively you have to avoid entangling the third effect with the first two effects.   

Let A be a finite set, $A = \{a_1, \dots, a_m\}$.\\

A latin square is a mxm matrix, 

			
			$$L = \begin{pmatrix}
				b_{11} & \dots & b_{1m}\\
				\vdots & & \vdots \\
				b_{m_1} & \dots & b_{mm}
			\end{pmatrix} $$

such that

	\begin{enumerate}
		\item Each row is a permutation of $(1, \dots, m)$.
		\item Each column is a permuation of $(1, \dots, m)$.
	\end{enumerate}

\begin{example}
	m = 3\\

	$\begin{pmatrix}
			1 & 2 & 3\\
			2 & 3 & 1\\
			3 & 1 & 2
		\end{pmatrix}$

	(You may shift each row to the left each time (or right).)
\end{example}


Let $k(i, j)$ be the element at the (i, j)th entry. \\

So, mathematically, a latin square is a mapping 

		$$K: A x A \rightarrow A, (i, j) \mapsto k(i,j) $$

such that (1) and (2) are satisfied. The following is the property of Latin Square. 

\begin{theorem}
	Let $K: A x A \rightarrow A$ be a Latin Square. Let $\eta: A \rightarrow \mathbb{R},k \mapsto \eta(k) $ be any function. Then

			\begin{align*}
				\sum_i \sum_j \eta(k(i,j)) &= \sum_i \eta(k(i,j))\quad \forall j \in A\\
						&= \sum_j \eta(k(i,j)) \quad \forall i \in A
			\end{align*}  

	That is, all row and column totals are the same. 
\end{theorem}

\begin{proof}
	This is simple because for each i, 

			$$ k(i, 1), \dots, k(i,m)$$

	is a permutation of $1, \dots, m$. Therefore, 

			$$\eta(k(i,1)), \dots, \eta(k(i,m)) $$

	is also a permutation of $\eta(1), \dots, \eta(m)$.\\

	So they sum to the same values, regardless of i. 

	Thus $\sum_j \eta(k(i,j))$ = a constant not dependant on i.

	Similarly, for $\sum_i \eta(k(i,j))$.

\end{proof}


Linear Model with Latin Square Design\\

		$$ \frac{1}{ij} \sim N(\mu_{ij}, \sigma^2) \quad \text{(independent)}$$

where $\mu_{ij} = \delta_i + \epsilon_j + \eta_{k(i,j)} $ where the $\eta$ values is the Latin Effect.


\textbf{Orthogonoal Decomposition}\\


\begin{notation}
	Dot Notation. For a latin square, 

			$$\{a_{ij}: j = 1, \dots, m; i = 1, \dots, m \} $$
 
 Let, 
			$$a_{i \cdot} = \sum_j^m a_{ij}, a_{ \cdot j} = \sum_i^m a_{ij}$$

Let 
			$$ a_{\begin{matrix}
				k\\
				\cdot
			\end{matrix}} = \sum_{k(i,j) = k} a_ij$$

	be the sum over all cells whose latin letter is k. 
\end{notation}


\begin{notation}
			$$d^{ij}_{uv} = \left\{ \begin{array}{ll}
										1 &  (u,v) = (i, j)\\
										0 & else
									\end{array} \right.$$

			$$E_{ij} = \left\{\ d^{ij}_{uv}: v = 1, \dots, m; u = 1, \dots, m \right\} $$


								
\end{notation}

		$$E_{i\cdot} = \sum^m_{j=1} E{ij} $$
		$$E_{\cdot j} = \sum^m_{i=1} E{ij} $$

		$$E_{\begin{matrix}
				k\\
				\cdot
			\end{matrix}} = \sum_{k(i,j) = k} E{ij} $$


$\mathcal{S}_1 = \text{span}(1_N), N = m^2 $\\
$\mathcal{S}_2 = \text{span}\{E_{i \cdot}\} \ominus \mathcal{S}_1$\\
$\mathcal{S}_3 = \text{span}(E_{\cdot j})\ominus \mathcal{S}_1 $\\
$\mathcal{S}_4 = \text{span}(E_{\begin{matrix}
				k\\
				\cdot
			\end{matrix}})\ominus \mathcal{S}_1 $\\
$\mathcal{S}_5 = \mathbb{R}^N \ominus (\mathcal{S}_1 \oplus \dots \oplus \mathcal{S}_4$\\


By construction, 
		
		$$\mathcal{S}_2 \perp \mathcal{S}_1,  \mathcal{S}_3 \perp \mathcal{S}_1, \mathcal{S}_4 \perp \mathcal{S}_1$$

By Latin Square design we can show that $\mathcal{S}_2 \perp \mathcal{S}_3$ and $\mathcal{S}_2 \perp \mathcal{S}_3 \perp \mathcal{S}_4$. \\


Only need to check that $\mathcal{S}_2 \perp \mathcal{S}_4$. \\


Proof in PHOTO\\


\textbf{Point Estimation}\\

Let 

	$$\mathcal{S} = \oplus_{i=1}^4 \mathcal{S}_i$$

	$$\mathcal{S}_5 = \mathbb{R}^N \ominus \mathcal{S} $$

	$$\mathcal{S}_1 \perp \dots \perp \mathcal{S}_5  $$

	$$Y = P_{\mathcal{S}_1} Y + \dots + P_{\mathcal{S}_5} Y $$

	$$\mu = P_{\mathcal{S}_1} \mu + \dots + P_{\mathcal{S}_5 }\mu $$

As before, 

		\begin{align*}
			\mathcal{S}_5 = \mathbb{R}^N \ominus (\mathcal{S}_1 \oplus \dots \oplus \mathcal{S}_4)\\
				&= (\mathbb{R}^N \ominus \mathcal{S}_1) \ominus (\mathcal{S}_2 \oplus \dots \oplus \mathcal{S}_4
		\end{align*}  


For any vector, 

		$$a = \{a_{ij}: i = 1, \dots, m; j = 1, \dots, m \} $$

		$$P_{\mathcal{S}_1} a = \{\bar{a}_{\cdot \cdot}: i = 1, \dots, m; j = 1, \dots, m \} $$

		$$P_{\mathcal{S}_2} a = \{\bar{a}_{i \cdot} - \bar{a}_{\cdot \cdot}: i = 1, \dots, m; j = 1, \dots, m \} $$

		$$P_{\mathcal{S}_3} a = \{\bar{a}_{\cdot j} - \bar{a}_{\cdot \cdot}: i = 1, \dots, m; j = 1, \dots, m \} $$

		$$P_{\mathcal{S}_4} a = \{\bar{a}_{\cdot k} - \bar{a}_{\cdot \cdot}: i = 1, \dots, m; j = 1, \dots, m \} $$

		$$P_{\mathcal{S}_5} a = \{a_{ij} - \bar{a}_{\cdot \cdot} - (\bar{a}_{i \cdot} - \bar{a}_{\cdot \cdot}) - (\bar{a}_{\cdot j} - \bar{a}_{\cdot \cdot}) - (\bar{a}_{\cdot k} - \bar{a}_{\cdot \cdot}): i = 1, \dots, m; j = 1, \dots, m \} = \{a_{ij} - \bar{a}_{i \cdot} - \bar{a}_{\cdot j} - \bar{a}_{k\cdot} + 2 \bar{a}_{\cdot \cdot}:i = 1, \dots, m; j = 1, \dots, m \} $$





\textbf{Friday October 14}\\

Clarification of last lecture: \\


		$$P_{\mathcal{S}_4} a = \{ \bar{a}_{k\cdot} - a_{\cdot \cdot}:  i = 1, \dots, m; j = 1, \dots, m \} $$

By $\bar{a}_{k\cdot}$ we mean, 

		$$ \bar{a}_{k\cdot} = \frac{1}{m_\cdot} \sum_{\{(i,j): k(i,j) = k\}} a){ij} $$


		$$(\bar{a}_{k\cdot})_{k = k(i,j) = \bar{a}_{k(i,j) \cdot}} $$


Using the above results and projection, 

		$$\mu_{ij} = \theta + \alpha_i + \beta_j + \gamma_{k(i,j)} $$

where, \\

$\theta = \bar{\mu}_{\cdot \cdot}$\\
$\alpha_i = \bar{\mu}_{i \cdot} - \bar{\mu}_{\cdot \cdot}$\\
$\beta_j = \bar{\mu}_{ \cdot j} - \bar{\mu}_{\cdot \cdot}$\\
$\gamma_{k(i,j)} = \bar{\mu}_{k_\cdot(i,j)} - \bar{\mu}_{\cdot \cdot}$\\


We may estimate these by $P_{\item }$ for i = 1, 2, 3. 

So,\\ 

$\bar{\mu}_{\cdot \cdot} \leftarrow \bar{Y}_{\cdot \cdot}$\\
$\bar{\mu}_{i \cdot} \leftarrow \bar{Y}_{i \cdot}$\\
$\bar{\mu}_{\cdot j} \leftarrow \bar{Y}_{\cdot j}$\\
$\bar{\mu}_{k_\cdot(i,j)} \leftarrow \bar{Y}_{k\cdot(i,j)}$\\

\textbf{Decomposition of Sum of Squares}\\

		$$|| P_{\mathcal{S}_1} Y ||^2 = m^2 \bar{Y}_{\cdot \cdot} $$

		$$|| P_{\mathcal{S}_2} Y ||^2 = m \sum^m_{i=1} (\bar{Y}_{i \cdot} - \bar{Y}_{\cdot \cdot})^2 $$

		$$|| P_{\mathcal{S}_3} Y ||^2 = m \sum^m_{j=1} (\bar{Y}_{\cdot j} - \bar{Y}_{\cdot \cdot})^2 $$

		$$|| P_{\mathcal{S}_4} Y ||^2 = m \sum^m_{k=1} (\bar{Y}_{k_\cdot} - \bar{Y}_{\cdot \cdot})^2 $$


\textbf{Test Hypothesis}\\

For example, we would want to test $H_0$ that there is no Latin effect. 

		$$H_0: \mu \in \mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3 \leftarrow \mathcal{S}^\prime $$

		$$H_1: \mu in \mathcal{S} = \mathcal{S}_1 \oplus \dots \oplus \mathcal{S}_4 $$

So our F-statistic is:

		$$\frac{|| P_{\mathcal{S} \ominus \mathcal{S}^\prime} Y ||^2 / dim( \mathcal{S} \ominus \mathcal{S}^\prime)}{|| P_{\mathbb{R}^N \ominus \mathcal{S}} Y ||^2 / dim(\mathbb{R}^N \ominus \mathcal{S}} $$


Note that $\mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_4$ and its dimension is $m - 1$. \\

Also that, 

		$$ P_{\mathbb{R}^N \ominus \mathcal{S}} Y = \{\bar{Y}_{ij} - \bar{Y}_{i\cdot} - \bar{Y}_{\cdot j} - \bar{Y}_{k_\cdot} + 2 \bar{Y}_{\cdot \cdot}: 1 = 1, \dots, m; j= 1, \dots, m\}$$

and its dimension is equal to $(m-1)(m-2)$.\\

		$$|| P_{\mathbb{R}^N \ominus \mathcal{S}} Y ||^2 = \sum^m_{i=1} \sum^m_{j=1} ( \bar{Y}_{ij} - \bar{Y}_{i\cdot} - \bar{Y}_{\cdot j} - \bar{Y}_{k_\cdot} + 2 \bar{Y}_{\cdot \cdot} ) ^2$$


\textbf{Simultanesous Confidence Interval}\\

Again, we use Latin as example, 

		$$H_0: \mu \in \mathcal{S}^\prime $$
		$$H_1: \mu \in \mathcal{S}$$

$\mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_4$\\

Set of contrats: 
		
		$$\{C^T \mu : C \in \mathcal{S} \ominus \mathcal{S}^\prime = \mathcal{S}_4  \} $$

$C \in \mathcal{S}_4$\\


\begin{align}
	C^T 1_N &= \sum^m_{i=1} \sum^m_{j=1} C_{k(i,j)} 1\\
			&= m \sum^m_{k=1}  C_{k}\\
			&= 0
\end{align}



\textit{SCI}\\

		$$C^T P_{\mathcal{S}_4} Y \pm ||C|| ||P_{\mathcal{S}_5}  Y|| \sqrt{\frac{dim(\mathcal{S}_4)}{dim(\mathcal{S}_5)} F_{dim(\mathcal{S}_4), dim(\mathcal{S}_5) } (1 - \alpha)} $$

where,\\
$||C||^2 = \sum^m_{i=1} \sum^m_{j=1} C^2_{k(i,j)} = m \sum^m_{k=1}  C^2_{k}$\\


\section{Orthogonal Nested Model}\index{Orthogonal Nested Model}

This is somewhat like interaction with one main effect, but this is not interpreted as interaction. So the application background is not interaction but nested design. \\


Comparison between a crossed design and nested design. \\ 

% photo

Mathematically, 

		$$Y_{ijk} \sim N(\mu_{ij}, \sigma^2) $$

$i = 1, \dots, r$\\
$j = 1, \dots, c_i$\\
$k = 1, \dots, n_{ij}$\\

To decompose $\mu$, 


	$$\mathcal{S}_1 = span(1_N) $$

	$$\mathcal{S}_2 = \{\mu_{i}: i = 1, \dots, r; j = 1, \dots, c_i  \} \ominus \mathcal{S}_1 $$

	$$\mathcal{S}_3 = \{d_{ij}: k = 1, \dots, n_{ij}; i = 1, \dots, r; j = 1, \dots, c_i  \} \ominus (\mathcal{S}_1 \oplus \mathcal{S}_2) $$

	$$\mathcal{S}_4 = \mathbb{R}^n \ominus (\mathcal{S}_1 \oplus \mathcal{S}_2 \oplus \mathcal{S}_3)


Again, 

		$$E_{ij} = \{\delta^{ij}_{uvw}: u = 1, \dots, r, v = 1, \dots, c, w = 1, \dots, n_{ij}  \} $$	


		$$\mathcal{S}_2 = span(E_{i\cdot} : i = 1, \dots, r) \ominus \mathcal{S}_1 $$ 

		$$\mathcal{S}_3 = span(E_{i j}) \ominus \mathcal{S}_1 \ominus \mathcal{S}_2 $$ 

Explicitly, 

		$$ P _{\mathcal{S}_1} \mu = \{\bar{\mu}_{\cdot \cdot}: \dots \}$$

		$$ \bar{\mu}_{i \cdot} = \theta $$



% \section{Overview}\index{Overview}

% \begin{itemize}
% 	\item Orthogonal design
% 	\item Additive 2 way ANOVA
% 	\item simultaneous intervals
% 	\item nonadditive
% 	\item decomposition of sum of squares
% 	\item Latin square
% 	\item nested design
% \end{itemize}

% \section{Table}\index{Table}

% \begin{table}[h]
% \centering
% \begin{tabular}{l l l}
% \toprule
% \textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
% \midrule
% Treatment 1 & 0.0003262 & 0.562 \\
% Treatment 2 & 0.0015681 & 0.910 \\
% Treatment 3 & 0.0009271 & 0.296 \\
% \bottomrule
% \end{tabular}
% \caption{Table caption}
% \end{table}

% %------------------------------------------------

% \section{Figure}\index{Figure}

% \begin{figure}[h]
% \centering\includegraphics[scale=0.5]{placeholder}
% \caption{Figure caption}
% \end{figure}

%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Nonorthogonal Design}

\section{Overview}\index{Overview}

\begin{itemize}
	\item $\bar{X_{i\dot\dot}} - \bar{X_{\dot\dot i}}$
\end{itemize}


%----------------------------------------------------------------------------------------
%	CHAPTER 5
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Random Effects Model}

\section{Overview}\index{Overview}



%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

 \part{Part Two}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Basic Concepts}

\section{Overview}\index{Overview}


%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Estimation}

\section{Overview}\index{Overview}

%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Inference}

\section{Overview}\index{Overview}

\begin{itemize}
	\item deviance <-> sum of squares
\end{itemize}

%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Residuals}

\section{Overview}\index{Overview}

%----------------------------------------------------------------------------------------
%	CHAPTER 5
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Cetegorical Prediction}

\section{Overview}\index{Overview}

%----------------------------------------------------------------------------------------
%	CHAPTER 6
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Some Important GLM}

\section{Overview}\index{Overview}

%----------------------------------------------------------------------------------------
%	CHAPTER 7
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Multivariate GLM}

\section{Overview}\index{Overview}


%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

 \part{Part Three}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Principle Componant Analysis}

\section{Overview}\index{Overview}

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Canonical Correlation Analysis}

\section{Overview}\index{Overview}

%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Independent Componant Analysis}

\section{Overview}\index{Overview}


% %----------------------------------------------------------------------------------------
% %	BIBLIOGRAPHY
% %----------------------------------------------------------------------------------------

% \chapter*{Bibliography}
% \addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}}
% \section*{Books}
% \addcontentsline{toc}{section}{Books}
% \printbibliography[heading=bibempty,type=book]
% \section*{Articles}
% \addcontentsline{toc}{section}{Articles}
% \printbibliography[heading=bibempty,type=article]

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage
\phantomsection
\setlength{\columnsep}{0.75cm}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}}
\printindex

%----------------------------------------------------------------------------------------

\end{document}