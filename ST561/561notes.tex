
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.0 (9/2/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations


\usepackage{mathrsfs}
\usepackage{amsbsy}
\usepackage{graphicx}



\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
\coordinate [below=12cm] (midpoint) at (current page.north);
\node at (current page.north west)
{\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{background}}; % Background image
\draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering  Advanced Statistical Inference\\[15pt] % Book title
{\Large STAT 561 - Advanced Statistical Inference}\\[20pt] % Subtitle
{\huge Dr. Bing Li}}}; % Author name
\end{tikzpicture}};
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2013 John Smith\\ % Copyright notice

\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \textsc{book-website.com}\\ % URL

\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Part One}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Basic Ideas in Bayesian Anaylysis}


% %------------------------------------------------

\textbf{Mathematical Preparation}\\

\textbf{Monday January 9}\\

\begin{enumerate}
	\item Product $\sigma$-Field\\

			$(\Omega_1, \mathcal{F}_1, \mu_1), (\Omega_2, \mathcal{F}_2, \mu_2)$ are two measure spaces. The goal is to construct a $\sigma$-field on $\Omega_1x\Omega_2$. \\

			Let $\mathcal{A} = \{AxB: A \in \mathcal{F}_1, B \in \mathcal{F}_2\}$. \\

			The $\sigma$-field generated $\mathcal{A}$ is called the product $\sigma$-field, written as $\mathcal{F}_1 x \mathcal{F}_2$, that is $\sigma(\mathcal{A})$. This is NOT a cartesian product, which would be $\{(A, B): A\in \mathcal{F}_1, B \in \mathcal{F}_2\}$. 

	\item Proctuct Measure\\

			Let $E \in \mathcal{F}_1 x \mathcal{F}_2$. Let $E_2(\omega_1) = \{\omega_2:(\omega_1, \omega_2) \in E\}$  and similarly, $E_1(\omega_2) = \{\omega_1:(\omega_1, \omega_2) \in E\}$. \\

			It is true (in Billingsly) that 

			\begin{theorem}[Number Unknown]
				If $E \in \mathcal{F}_1 x \mathcal{F}_2$ then $E_1(\omega_2) \in \mathcal{F}_1$ for all $\omega_2 \in \Omega_2$. Similarly, $E_2(\omega_1) \in \mathcal{F}_2$ for all $\omega_1 \in \Omega_1$. \\


				If $f: \Omega_1 x \Omega_2 \rightarrow \mathbb{R}$ measurable $\mathcal{F}_1 x \mathcal{F}_2 \setminus \mathcal{R}$. Then for each $\omega_1 \in \Omega_1$, 

						$$ f(\omega_1, \cdot) \textcircled{m} \mathcal{F}_2 \setminus \mathcal{R} \text{ for each } \omega_2 \in \Omega_2. $$

						$$ f(\cdot, \omega_2) \textcircled{m} \mathcal{F}_1 \setminus \mathcal{R} $$

				Now, for each $E \in \mathcal{F}_1 x \mathcal{F}_2$ consider 

						$$f_{1, E}: \Omega_1 \rightarrow \mathcal{R}, \omega_1 \mapsto \mu_2(E_2,(\omega_2)) $$

				It can be shown that $f_{1, E}$ is uniformly measurable $\mathcal{F}_1 \setminus \mathcal{R}$ for all E. 


			\end{theorem}

	\begin{proof}
		Outline. 

			\begin{itemize}
				\item Show that if $\mathcal{L} = \{E: f_{1, E} \textcircled{m} \mathcal{F}_1 \setminus \mathcal{R}\}$ then $\mathcal{L}$ is a $\lambda$-system. 
				\item Let $\mathcal{P} = \{AxB: A \in \mathcal{F}_1, B \in \mathcal{F}_2\}$ then it is a $\pi$-system.
			

				Furthermore, if $ E = A x B$, 
				
						$$E_2(\omega_1) = \left\{ \begin{array}{ll}
							B & \omega_1 \in A\\
							\emptyset & \omega_1 \notin A
						\end{array} \right. $$	

				So, $\mu_2(E_2(\omega_1)) = \left\{ \begin{array}{ll}
							\mu_2(B) & \omega_1 \in A\\
							\emptyset & \omega_1 \notin A
						\end{array} \right. = I_A(\omega_1)\mu(B) = f_{1, E}$

				So, $f_{1, E} \textcircled{m} \mathcal{F}_1$. 

				Thus $\mathcal{P} \subseteq \mathcal{L}$. 	
				
				\item By $\pi-\lambda$ Theorem, $\mathcal{F}_1 x \mathcal{F}_2 \subseteq \mathcal{L}$. 

	\end{itemize}

		Similarly, $f_{2, E} \textcircled{m} \mathcal{F}_2 \setminus \mathcal{R}$. 

		We can now define two set functions, 

				$$\pi'(E) = \int f_{1, E} d\mu_1 $$
				$$\pi''(E) = \int f_{2, E} d\mu_2 $$

		Again using $\pi-\lambda$ Theorem, it can be shown that, $\pi', \pi''$ are both measure and if $\mu_1, \mu_2$ are $\sigma$-finite, then

				$$\pi' = \pi'' \text{ on } \mathcal{F}_1 x \mathcal{F}_2 $$

		Note that here, $\mathcal{P}$ equals $\mathcal{A}$ used at begining	of notes.


		We did not have a measure in $\mathcal{F}_1 x \mathcal{F}_2$. Now we have $\pi', \pi''$ both measures on $\mathcal{F}_1 x \mathcal{F}_2$, they are the same. We call this measure the product meaure, written as $\mu_1 x \mu_2$. 


		Note that $(\Omega_1 x \Omega_2, \mathcal{F}_1 x \mathcal{F}_2, \mu_1 x \mu_2)$ is called product measure space. 
	\end{proof}

	\item Tonelli's Theorem\\

			$(\Omega_1, \mathcal{F}_1, \mu_1), (\Omega_2, \mathcal{F}_2, \mu_2)$ are two $\sigma$-finite measure spaces. \\

			$(\Omega_1 x \Omega_2, \mathcal{F}_1 x \mathcal{F}_2, \mu_1 x \mu_2)$ is the product measure space.\\

			Suppose we have $f:\Omega_1 x \Omega_2 \rightarrow \mathbb{R} \textcircled{m} \mathcal{F}_1 x \mathcal{F}_2 \setminus \mathcal{R} $. Where $f \geq 0$ and 

					$$\int f d (\mu_1 x \mu_2) =\int \left[ \int(f(\cdot, \omega_2) d\mu_1) \right] d\mu_2$$

	\item Fubini's Theorem\\

	The conclusion of Tonelli's Theorem still holds if f is NOT nonnegative, but if f is integrable $\mu_2$. (integrable - integral of absolute value of function is finite) \\

\textbf{Wednesday January 11}\\	

	\item Conditional Probability\\

	This is a special application of Radon- Nikodgm Theorem. We know that 

			$$P(A|B) = \frac{P(A, B)}{P(B)} $$

	We may define $P(A|\mathcal{G})$ when $\mathcal{G}\subseteq \mathcal{F}$ as sub-$\sigma$-field. We defined this intuitively in elementary probability course (definition above), but we ar enot going to define it generally. \\

	Now let $A \in \mathcal{F}$ and $\mathcal{G} \subset \mathcal{F}$ be a $\sigma$-field. Consider the set function 

			$$\nu: \mathcal{G} \rightarrow \mathbb{R}, G \mapsto P(AG)$$ 

	It can be easily shown that $\nu$ is a measure on $\mathcal{G}$. Consider another set function, 

			$$\mu: \mathcal{G} \rightarrow \mathbb{R}, G \mapsto P(G) $$

	So $\mu$ is nothting but P restricted on $\mathcal{G}$.\\

	It's easy to show that $\nu << \mu$. 

			$$\mu(G) = 0 \Rightarrow P(G) = 0 \Rightarrow P(AG) = 0 \Rightarrow \nu(G) = 0$$

		By Radon-Nikodgm Theorem, there exists a $\delta$ such that

			$$\nu(G) = \int_G \delta d\mu \quad \forall G \in \mathcal{G} $$

		$\delta$ is called R-N Derivative, written as 

				$$\delta = \frac{d\nu}{d\mu} $$

		and is similar in  to $\frac{P(AG)}{P(G)}$, but it's more general. \\

		$\delta$ is called the conditional probability of A given $\mathcal{G}$. To distinguish it form P(A|B), where B is a set, we use $P(A||\mathcal{G})$, where $\mathcal{G}$ is a $\sigma$-field. By construction, \\

				\begin{enumerate}
					\item $\delta$ is measurable $\mathcal{G}$
					\item $\int_G \delta dp = P(AG) \quad \forall G\in\mathcal{G}$ \\
				\end{enumerate}


		Note that, by RNT, $\delta$ is unique with probability 1. Any $\delta'$ satisfying (a) and (b) has $\delta' = \delta a.e. P$. So, we say that $\delta$ is a version of conditional probability. \\

		So, $\delta$ is a version of $P(A||\mathcal{G})$ if and only if (a) and (b) are satisfied. We may define $P(A||\mathcal{G})$ either by RNT or (a) and (b). \\


		Properties of Conditional Probability\\

		It behaves like probability, but since it is a function, unique up to a.e. P, these properties have to be qualified by a.s. P.

		\begin{enumerate}	
			\item $P(\emptyset || \mathcal{G}) = 0, P(\Omega||\mathcal{G}) = 1$ a.s. P
			\item $0 \leq P(A|| \mathcal{G}) \leq 1$ a.s. P
			\item If $A_1, A_2, \dots$ are disjoint members of $\mathcal{F}$ then $P(\bigcup_n A_n || \mathcal{G}) = \sum_n P(A_n || \mathcal{G})$ a.s. P
		\end{enumerate}

		Let's consider the special case where $\mathcal{G}$ is a $\sigma$-field generated by some random element, T (i.e. $\mathcal{G} = \sigma(T)$). More specifically, for some measurable space $(\Omega_T, \mathcal{F}_T)$ where 


				$$T: \Omega \rightarrow \Omega_T \textcircled{m} \mathcal{F}\setminus\mathcal{F}_T \quad \mathcal{G} = T^{-1}(\mathcal{F}_T)$$

		Here, we write

		\begin{align*}
			P(A || \mathcal{G}) &= P(A || \sigma(T))\\
				&=P(A|| T^{-1}(\mathcal{F}_T))\\
				&= P(A || T)
		\end{align*}

		The following theorem makes checking that something is a conditional probability easier. In principle, we have to check $\int_G \delta dp = P(AG) \quad \forall G\in\mathcal{G}$. 

		\begin{theorem}[33.1 in Billingsly]
			Let $\mathcal{P}$ be a $pi$-system generating $\mathcal{G}$ and suppose that $\Omega$ is a countable union of sets in $\mathcal{P}$. An integrable function, $f$, is a version of $P(A || \mathcal{G})$ if 

				\begin{enumerate}
					\item f is measurable $\mathcal{G}$
					\item $\int_G f dp = P(AG) \quad \forall G \in \mathcal{P}$ 
				\end{enumerate}
		\end{theorem}

	\item Conditional Distribution\\

	Let there be probability space $(\Omega, \mathcal{F}, P)$, measurable space $(\Omega_X, \mathcal{F}_X)$, and a random element, $X:\Omega \rightarrow \Omega_X \textcircled{m} \mathcal{F}\setminus\mathcal{F}_X$. Also, let $\mathcal{G} \subseteq \mathcal{F}$ be a sub $\sigma$-field. \\

	We are going to define conditional distribution of X given G. Under very mild conditions there is a function

			$$f: \mathcal{F}_X x \Omega \rightarrow \mathbb{R} $$

	such that for each $A \in \mathcal{F}_X$, $f(A, \cdot)$ is a version of 

			$$P(X \in A || \mathcal{G}) = P(X^{-1}(A) || \mathcal{G}) $$

	and, for each $\omega \in \Omega$,  $f(\cdot, \omega)$ is a probability measure on $(\Omega_X, \mathcal{F}_X)$. \\

	The only condition for this existance is $(\Omega_X, \mathcal{F}_X)$ must be a Borel Space, that is $\mathcal{F}_X$ is Borel $\sigma$-field. This should always be the case for our purposes. 
	

	\item Conditional Expectation\\

	Let us have the same probability space, measurable space, random element, and sub $\sigma$-field as defined before, but here with $\bar{\mathbb{R}}$.  \\

	We want to define conditional expectaiton of X given $\mathcal{G}$. \\

	First, assume $X \geq 0$. Consider a set function, 

			$$\nu: \mathcal{G} \rightarrow \mathbb{R}, G \mapsto \int_G X dP $$ 

	It can be easily shown that $\nu$ is a measure. 

	Let $\mu$ again be $\mathcal{G} \rightarrow \mathbb{R}, G \mapsto  P(G)$. Then $\nu << \mu$. By RNT, $\delta = \frac{d\nu}{d\mu}$ is well defined. This is defined to be conditional expectation of X given $\mathcal{G}$, written as 

			$$E(X|| \mathcal{G}) $$

	Suppose $X \ngeq 0$, but integrable P. Recall that $X = X^+ - X^-$. Since $X^+, X^- \geq 0$, then both $E(X^+|| \mathcal{G}), E(X^-|| \mathcal{G})$ are defined by RNT. We define, 

			$$E(X|| \mathcal{G}) = E(X^+|| \mathcal{G}) - E(X^-|| \mathcal{G}) $$

				 

\textbf{Friday January 13}

	As in the case of $P(A || \mathcal{G})$, the equivalent conditions for $d: \Omega \rightarrow \mathbb{R} $ is a version of $E(X||\mathcal{G})$. 

			\begin{enumerate}
				\item $\delta$ measurable $\mathcal{G}$
				\item $\int_G \delta dP = \int X dP \quad \forall G \in \mathcal{G}$ 
			\end{enumerate}

			INSERT PHOTO FROM BOARD - "Mesh"


	The value of $\delta$ in each thick outlined cell is the average (with respect to P measure ) of $X(\omega)$ over the subcells (thin outlined) in thick cells. 

	We see from this definition that if  $ A \in \mathcal{F}$, $X = I_A$ then the second condition becomes 

			$$\int_G \delta dP = \int_G I_A dP = P(A \bigcap G) $$

	So, $E(I_A||\mathcal{G}) = P(A || \mathcal{G})$. 

	\textbf{Properties of Conditional Expectations}\\

	\begin{theorem}{34.2 in Billingsly}
		Suppose that $X, Y, X_n$ are integrable P. 

			\begin{enumerate}
				\item If X = a a.e. P, then $E(X || \mathcal{G})$ a.s. P
				\item $a, b \in \mathbb{R}$ then

						$$E(aX + bY || \mathcal{G}) = a(E(X ||\mathcal{G})) + b(E(X || \mathcal{G})) a.s. P $$
				\item If $X \leq Y$ a.s. P then 

						$$E(X ||\mathcal{G}) \leq E(Y ||\mathcal{G}) $$
				\item $|E(X||\mathcal{G})| \leq E(|X| ||\mathcal{G})$ a.s. P (in fact this is true for all convex functions).
				\item If $X_n \rightarrow X$ a.s. P, $|X_n| \leq Y,$ and Y integrable P, then

						$$E(X_n ||\mathcal{G}) \rightarrow E(X ||\mathcal{G}) a.s. P $$
			\end{enumerate}

	\end{theorem}

	\begin{proof}
		Found in Billingsly. 
	\end{proof}

	\begin{theorem}[34.4 in Billingsly]
		If $\mathcal{G}_1 \subseteq \mathcal{G}_2 \subset \mathcal{F}$ and X integrable P, then

				$$E(E(X ||\mathcal{G}_2)|| \mathcal{G}_1) = E(X ||\mathcal{G}_1) $$

		This is called the Law of Iterative Conditional Expectation. 
	\end{theorem}

	\begin{theorem}[34.3 in Billingsly]
		
		If $X$ measurable $\mathcal{G}$, $Y \textcircled{m} \mathcal{F}$, then

				$$E(X Y||\mathcal{G}) = XE(Y ||\mathcal{G}) a.s. P $$
	\end{theorem}

	Other Properties

		\begin{enumerate}
			\item X, Y are random elements such that XY integrable P.
			\item If $\mathcal{G} \subseteq \mathcal{F}$ is the sub $\sigma$-field, then

					$$E(X E(Y||\mathcal{G})) = E(E(X ||\mathcal{G})Y) = E(E(X ||\mathcal{G}) E(Y ||\mathcal{G})) $$ 

			Conditional expectation is a self-adjoint operation. 

			\begin{proof}

			"Wire Theorem"\\

				$$\begin{aligned}
								E(X E(Y ||\mathcal{G}))	& = E( E(X E(Y ||\mathcal{G}) ||\mathcal{G}))\\
									&= E( E(Y ||\mathcal{G})E(X ||\mathcal{G}))	\\
									&= E( E(E(X ||\mathcal{G}) Y ||\mathcal{G})) \\
									&= 	E(E(X ||\mathcal{G}) Y)	
								\end{aligned}$$
			\end{proof}
		\end{enumerate}

	\item Conditional Distribution of a Random Element Given Another Random Element\\

		Here we have the typical probability space, measurable spaces for X and Y. 

		Let there be a function, 

			$h: \mathcal{F}_X x \Omega_Y \rightarrow \mathbb{R}$

		This function is called the conditional distribution of X given Y if 

				$$\tilde{h}(A, \omega) = h(A, Y(\omega)) $$

		We say that $\tilde{h}: \mathcal{F}_X x \Omega \rightarrow \mathbb{R}$ is the condiitional distribution of X given $\mathcal{G} = Y^{-1}(\mathcal{F}_Y$. 

		That is, 

				\begin{enumerate}
				 	\item For each $A \in \mathcal{F}_X$

				 			$$\tilde{h}(A, Y(\cdot)) = P(X^{-1}(A) || Y^{-1}(\mathcal{F}_Y)) $$
		 			\item  For each $\omega \in \Omega$

		 					$$\tilde{h}(\cdot, Y(\omega)) = P_{X|Y}(A | y) $$
				 \end{enumerate} 


	\item Conditional Density of One Random Element Given Another Random Element\\

		Suppose probability space and $\sigma$-finite measure spaces for X and Y. 

		Here our relevant function is

				$$g: \Omega_X x \Omega_Y $$

		which is the conditional density of X given Y if for all $A \in \mathcal{F}_X$, 

				$$\int_A g(x, y) d \mu_X(x) = P_{X|Y}(A|y) $$

		In the following special case, g ahs an explicit formula. 


		$(\Omega, \mathcal{F}, P)$\\
		$(\Omega_X, \mathcal{F}_X, \mu_X)$\\
		$(\Omega_Y, \mathcal{F}_Y, \mu_Y)$\\
		$(\Omega_X x \Omega_Y, \mathcal{F}_X x \mathcal{F}_Y, \mu_X x \mu_Y)$\\
		$(X, Y): \Omega \rightarrow \Omega_X x \Omega_Y \textcircled{m} \mathcal{F}\setminus \mathcal{F}_X x \mathcal{F}_Y$\\

		Let $P_X = P X^{-1}, P_Y = P Y^{-1}, P_{XY} = P (XY)^{-1}$. \\
		Assume $P_X << \mu_X, P_Y << \mu_Y, P_{XY} << \mu_X x \mu_Y$. 



				$$f_X = \frac{dP_X}{d\mu_X} $$
				$$f_Y = \frac{dP_Y}{d\mu_Y} $$ 
				$$f_{XY} = \frac{dP_{XY}}{d(\mu_X x \mu_Y) } $$ 

		Let

				$$f_{X|Y} = \begin{array}{ll}
				\frac{f_{XY}}{f_Y} & if f_Y \neq 0\\
				0 & f_Y = 0
					
				\end{array} $$

				$$f_{Y|X} = \begin{array}{ll}
				\frac{f_{XY}}{f_X} & if f_X \neq 0\\
				0 & f_X = 0
					
				\end{array} $$

		Then it is easy to show that each is indeed the conditional density of their respective elements (first given second). 
 
\textbf{Wednesday January 18}\\

Claim: $g(x, y)$ is the conditional density. 

\begin{proof}
	Want to show that for all $A \in \mathcal{F}_X$, 

			$$ \int_A g(x, y) d\mu_x(x) = P_{X|Y} (A|y) $$

	Which means that

			$$\int_A g(x, y(\omega)) d\mu_x(x) = P_{X|Y} (X^{-1}(A)|\sigma(y)) $$

	This is true if for all $G' \in \sigma(y)$

			$$\int_{G'} \int_A g(x, y(\omega)) d\mu_X (x) dP(\omega) = P(X^{-1}(A) \bigcap G') $$


	But note that 

				\begin{align*}
					G' &\in \sigma(y)\\
					\Leftrightarrow G' &\in Y^{-1}(\mathcal{F}_Y)\\
					G' &= Y^{-1}(G)\text{ for some } G\in \mathcal{F}_Y
				\end{align*}



	So we want to check that  

			$$\int_{Y^{-1}(G)} \int_A g(x, y(\omega)) d\mu_X (x) dP(\omega) = P(X^{-1}(A) \bigcap Y^{-1}(G)) $$

			\begin{align*}
				\int_{Y^{-1}(G)} \int_A g(x, y(\omega)) d\mu_X (x) dP(\omega) &= \int_{G} \int_A g(x, y) d\mu_X (x) dP_Y(y) \\
					&=\int_{G} \int_A \frac{f_{XY}(x, y)}{f_Y(y)} d\mu_X (x) [f_Y(y)] d\mu_Y(y) \\
					&=\int_{G} \int_A f_{XY}(x, y) d\mu_X (x) d\mu_Y(y)\\
					&=\int_{GxA} f_{XY}(x, y) d(\mu_X x \mu_Y)(x,y)\\
					&= P_{XY}(GXA) \\
					&= P \circ (X, Y)^{-1}(Ax G)\\
					&= P(X \in A, Y \in G)\\
					&= P(\omega: \omega \in X^{-1}(A) \& \omega \in Y^{-1}(G))\\
					&= P(X^{-1}(A) \bigcap Y^{-1}(G)) 
			\end{align*}
\end{proof}

\end{enumerate}
	
\section{Frequentist \& Bayesian Settings}\index{Frequentist \& Bayesian Settings}

We have our probability space $(\Omega, \mathcal{F}, P)$. We also have some data, 

		$$(\Omega_X, \mathcal{F}_X, \mu_X)$$ 
		$$ X: \Omega \rightarrow \Omega_X \textcircled{m} \mathcal{F}/ \mathcal{F}_X$$

Here, usually $\Omega_X$ is a $\mathbb{R}^m$.\\

Typically we have 

	$$X = (X_1, \dots, X_n)$$

and possibly, 

	$$ X_i = \begin{pmatrix}
		X_{i1}\\
		\vdots\\
		X_{ip}
	\end{pmatrix}$$

We could say that these data are independent and identically distributed (iid) random vectors of dimension p. In this case m = np. \\

The goal of statical inference is to estimate.

		$$P_X = PX^{-1} = P_0$$

The ?? distribution of X. \\

There are two schools of thought

	\begin{enumerate}
		\item Frequentist Approach - assume a family of distributions, $\mathcal{P}$, where $\mathcal{P} << \mu_x$. Usually we assume that $\mathcal{P}$ is a parametric family, $\mathcal{P} = \{P_\theta: \theta \in \Omega_\theta \subseteq \mathbb{R}^p \}$. We assume that $P_0 \in \mathcal{P}$, that is there exisgts $\theta_0 \in \Omega_\theta$ such that $P_\theta = P_0$. The goal is to estimate $P_0$. 
		\item Bayesian Approach - here we assume the data is generated by the conditional distribtuion $P_{X|\theta}$. We observe X, then determine what is the best estimate of the random $\theta.$
\end{enumerate}


\section{Prior Posterior \& Likelihood}


Here let there be probability space $(\Omega, \mathcal{F}, P)$; $\sigma$-finite measurable spaces $(\Omega_X, \mathcal{F}_X, \mu_X)$, $(\Omega_\theta, \mathcal{F}_\theta, \mu_\theta)$. Together, 

		$$(\Omega_X x \Omega_\theta, \mathcal{F}_X x \mathcal{F}_\theta, \mu_X x \mu_\theta)$$ 


Also, a random element, 

		$$(X, \theta):\Omega \rightarrow \Omega_X x \Omega_\theta \textcircled{m} \mathcal{F}\setminus\mathcal{F}_X x \mathcal{F}_\theta$$


$P_X = P \circ X^{-1} \leftarrow$ marginal distribution of X\\
$P_\theta = P \circ \theta^{-1} \leftarrow$ prior distribution \\
$P_{X, \theta} = P \circ (X, \theta)^{-1} \leftarrow$ joint distribution of X and $\theta$\\
$P_{X|\theta}(A|\theta): \mathcal{F}_X x \Omega_\theta \rightarrow \mathbb{R}$. Likeliehood distribution\\
$P_{\theta|X}(G|x): \mathcal{F}_\theta x \Omega_X \rightarrow \mathbb{R}$. Posterior distribution\\


Note in the following the first inequalities are \textbf{assumed}.\\

$P_X << \mu_X \Rightarrow f_X = \frac{d P_X}{d\mu_X}$ Marginal Density\\
$P_\theta << \mu_\theta \Rightarrow \pi_\theta = \frac{d P_\theta}{d\mu_\theta}$ Prior Density\\
$P_(X, \theta) << \mu_X x \mu_\theta \Rightarrow f_{X, \theta} (x, \theta) = \frac{d P_{X, \theta}}{d(\mu_x x \mu_\theta)}$ Joint Density\\


FINISH FROM PHOTO


One way to estimate $\theta$ is by maximizing $\pi_{\theta|X}(\theta|x)$. Want to do so with value that is most likely to happen (given the data). 

		$$\pi_{\theta|X} (\theta|x) = P_{\theta|X}(\theta = \theta | x) $$

By construction, 

		\begin{align*}
			\pi_{\theta|X} &= \frac{f_X\theta}{f_X}\\
				&= \frac{f_{X|\theta}\pi_\theta}{\int_{\Omega_\theta} f_{X|\theta} \pi_\theta d\mu_theta}
		\end{align*}


\section{Conditional Independence and Frquentist/Bayesian Sufficiency}\index{Conditional Independence and Frquentist/Bayesian Sufficiency}


\textbf{Independence}\\

Two random elements are said to be independent if for all $A' \in \sigma(X), G' \in \sigma(\theta)$ we have

		$$P(A' \bigcap G') = P(A')P(G') $$

This statement can also be expressed in $(\Omega_X x \Omega_\theta, \mathcal{F}_X x \mathcal{F}_\theta, P_X x P_\theta)$ as follows. \\

Since $A' \in \sigma(X) = X^{-1} (\mathcal{F}_X), A' = X^{-1}(A)$ for some $A \in \mathcal{F}_X$. So $G' = \Theta^{-1}(G). G \in \mathcal{F}_\Theta$. 

		\begin{align*}
			P(A' \bigcap G') &= P(X^{-1}(A) \bigcap \Theta^{-1}(G) )\\
				&=P(\{ \omega: \omega \in  X^{-1}(A) \bigcap \Theta^{-1}(G)\} )\\
				&=P(\{ \omega: \omega \in  X^{-1}(A) \& \omega \in \Theta^{-1}(G)\} )\\
				&=P(\{ \omega: X(\omega) \in A, \Theta(\omega) \in G\} )\\
				&=P(\{ \omega:( X(\omega),  \Theta(\omega)) \in A x G\} )\\
				&=P(\{ \omega:( X, \Theta)(\omega) \in A x G\} )\\
				&=P(\{ \omega:\omega \in( X, \Theta)^{-1} A x G\} )\\
				&=[P \circ ( X, \Theta)^{-1} ](A x G)\\
				&=P_{X, \Theta}(A x G)
		\end{align*}

Also note that

		$$P(A') = P(X^{-1}(A)) = P_X(A) $$
		$$P(G') = P_\Theta (G) $$


So with independence, (and for $A \in \mathcal{F}_X, G \in \mathcal{F}_\Theta$)

		$$P_{X, \Theta}(A x G) = P_X (A) P_\Theta(G) $$

But we know that this implies that $P_{X, \Theta}$ is the product measure $P_X x P_\Theta$. 

\textbf{Conditional Independence}\\

Now, given sub $\sigma$-field $\mathcal{G} \in \mathcal{F}$ we want to define $X \& \Theta$ conditionally independent given $\mathcal{G}$. 

\begin{definition}
	We say that $X \& \Theta$ are conditionally independent given $\mathcal{G}$ (i.e. $X \indep \Theta | \mathcal{G}$) if for all $A' \in \sigma(X), G' \in \sigma(\Theta)$ we have 

			$$P[A' \cap G' || \mathcal{G}] = P[A' || \mathcal{G}]P[G' || \mathcal{G}] a.s. P $$

	Equivalently for all $A \in \mathcal{F}_X, G \in \mathcal{F}_\Theta$, 

			$$P[X^{-1}(A) \cap \Theta^{-1}(G) || \mathcal{G}] =P[X^{-1}(A) || \mathcal{G}]P[\Theta^{-1}(G) || \mathcal{G}]  $$

	Equivalently, 

			$$P_{X, \Theta | \mathcal{G}} (A x G | \mathcal{G}) = P_{X|\mathcal{G}}(A | \mathcal{G}) P_{\Theta|\mathcal{G}} (G | \mathcal{G}) $$
\end{definition}


\textbf{Equivalent Condition for Conditional Independence}\\

\begin{theorem}[1.1 in Notes]
	The following statements are equivalent. 

	\begin{enumerate}
		\item $X \indep \Theta | \mathcal{G}$
		\item $P(X^{-1}(A) || \Theta, \mathcal{G}) = P(X^{-1}(A)| \mathcal{G}) a.s. P \quad \forall A \in \sigma(X)$
		\item $P(\Theta^{-1}(G) || X, \mathcal{G}) = P(\Theta^{-1}(G)|| \mathcal{G}) a.s. P \quad \forall G \in \sigma(\Theta)$
	\end{enumerate}
\end{theorem}

\begin{proof}
	It suffies to proof that $1 \Leftrightarrow 2$.\\

	$1 \Rightarrow 2$. We know that for all $A \in \mathcal{F}_X, G \in \mathcal{F}_\Theta$ that

			$$ P[X^{-1}(A) \cap \Theta^{-1}(G) || \mathcal{G}] =P[X^{-1}(A) || \mathcal{G}]P[\Theta^{-1}(G) || \mathcal{G}]$$

	Want that for all $A \in \mathcal{F}_X$ that $P(X^{-1}(A) || \Theta, \mathcal{G}) = P(X^{-1}(A)| \mathcal{G})$. 

	\begin{align*}
		P(X^{-1}(A) || \Theta, \mathcal{G}) &\equiv P\left(X^{-1}(A)|| \sigma(\sigma(\Theta) \cup \mathcal{G})\right) \\
			&= P(\dots || \sigma(\Theta^{-1}(\mathcal{F}_\Theta) \cup \mathcal{G}))
	\end{align*}

	So it suffices to show that 

			$$P(X^{-1}(A) || \sigma(\Theta^{-1}(\mathcal{F}_\Theta) \cup \mathcal{G})) = P(X^{-1}(A) || \mathcal{G})$$

	From the definition given we want to show that the above statement is true. which is so that the for all $B \in \sigma(\Theta^{-1}(\mathcal{F}_\Theta) \cup \mathcal{G} )$, 

			$$\int_B P(X^{-1}(A) || \mathcal{G}) dP = P(X^{-1} (A) \cap B) $$

	But this is very hard because B is hard to characterize. But we have theorem that says you only have to check (*) for all B in a $\pi$-system generating $\sigma(\Theta^{-1}(\mathcal{F}_\Theta)\cup \mathcal{G})$.

			$$\mathcal{P} = \{\Theta^{-1} (G) \cap F: G \in \mathcal{F}_\Theta, F \in \mathcal{G} \} $$

	It is trivial to show that $\mathcal{P}$ is a $\pi$-system. 

	MORE IN PHOTO

	Meanwhile, 

			$$\mathcal{P} \subseteq \sigma(\Theta^{-1} (\mathcal{F}_\Theta) \cup \mathcal{G}) $$

	Therefore, 

			$$\sigma(\Theta^{-1}(\mathcal{F}_\Theta)\cup \mathcal{G}) = \sigma(\mathcal{P}) $$

	So, sufficent to check (*) $\forall B \in \mathcal{P}'$

			$$B \in \mathcal{P} \Rightarrow B = \Theta^{-1}(G) \cap F, G \in \mathcal{F}_\Theta, F \in \mathcal{G} $$

	So, we want

			$$\int_{\Theta^{-1}(G)\cap F} P(X^{-1}(A) || \mathcal{G}) dP = P(\Theta^{-1}(G) \cap F \cap X^{-1}(A)) $$


			\begin{align*}
				\int_{\Theta^{-1}(G)\cap F} P(X^{-1}(A) || \mathcal{G}) dP &= \int_{\Theta^{-1}(G) \cap F} E \left(I_{X^{-1}(A)} || \mathcal{G} \right) dP\\
						&= E\left(I_{\Theta^{-1}(G)} I_F E(I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( E(I_{\Theta^{-1}(G)} I_F || \mathcal{G}) E(I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( I_F E(I_{\Theta^{-1}(G)}  || \mathcal{G}) E(I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( I_F E(I_{\Theta^{-1}(G)} I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left(  E(I_F I_{\Theta^{-1}(G)} I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( I_F I_{\Theta^{-1}(G)} I_{X^{-1}(A)}\right)\\
						&= P(F \cap \Theta^{-1}(G) \cap X^{-1}(A))
			\end{align*}

\textbf{Monday January 23}\\

	$2 \Rightarrow 1$. We want to show that 

			$$P(X^{-1}(A)|| \mathcal{G}) P(\Theta^{-1}(G) || \mathcal{G}) $$

		is conditional probability of 

			$$P(X^{-1}(A) \cap \Theta^{-1}(G)|| \mathcal{G}) $$

		for all $F \in \mathcal{G}$. 
		
				\begin{align*}
						\int_F P(X^{-1}(A)|| \mathcal{G})P(\Theta^{-1}(G) || \mathcal{G}) dP &= E\left[I_F E\left(I_{X^{-1}(A)} || \mathcal{G} \right) E\left(I_{\Theta^{-1}(G) }|| \mathcal{G} \right) \right]\\
								&= E\left[ E\left(I_{X^{-1}(A)} || \mathcal{G} \right) E\left(I_F I_{\Theta^{-1}(G) }|| \mathcal{G} \right) \right]\\
								&= E\left[ E\left(I_{X^{-1}(A)} || \mathcal{G} \right) I_F I_{\Theta^{-1}(G)} \right]\\
								&= E\left[ E\left(I_{X^{-1}(A)} I_F I_{\Theta^{-1}(G)}|| \Theta, \mathcal{G} \right)  \right]\\
								&= E\left[I_{X^{-1}(A)} I_F I_{\Theta^{-1}(G)} \right]\\
								&= P(X^{-1}(A) \cap \Theta^{-1}(G) \cap F)
					\end{align*}	


\end{proof}

\section{Equivalence of Frequentist \& Bayesian Sufficiency}\index{Equivalence of Frequentist \& Bayesian Sufficiency}


Here we have, \\

$(\Omega_\Theta, \mathcal{F}_\Theta, \mu_\Theta), (\Omega_X, \mathcal{F}_X, \mu_X),(\Omega_T, \mathcal{F}_T)$\\

Where 

		$$T: \Omega_X \rightarrow \Omega_T \textcircled{m} \mathcal{F}_X / \mathcal{F}_T $$

is called a statistic. 

		$$T = T(X) \text{ or } T \circ X = T(X(\omega)) $$

In fewquentist setting, we say that T is \textbf{suffienct} if $P_{X|T, \Theta}$ does not depend on $\Theta$. It can be easily verified (see Homework) that $P_{X|T, \Theta}$ doesn't depend on $\Theta$ implies that

		$$P_{X|T, \Theta} = P_{X|T} \text{ a.s. P} $$

This is "nearly" fequentist. Above is exchangeable with "$X \indep \Theta| T$", but can't say this in frequentist setting. 


		$$P_{\Theta | T, X}  =  P_{\Theta|T} \Leftrightarrow P_{\Theta|X} = P_{\Theta|T}$$

That is to say that a statistic, T, is sufficient for $\Theta$ if and only iff the posterir distribution of $\Theta|X$ is the same as the posterior distribution of $\Theta|T$. This would be used in a Bayesian setting. 

\begin{definition}[Bayesian Sufficient]
	We say that $T \circ X$ is \textbf{Bayesian sufficient} if 

			$$P_{\Theta | X}  =  P_{\Theta|T} \text{ a.s. P } $$
\end{definition}


\textbf{Lemma 1.1} (HW 2) Suppose that $f(\theta)$ is a p.d.f such that

		$$f(\theta) \propto exp\{-a\theta^2 + b\theta \}, \quad a > 0 $$

		Then, 

		\begin{enumerate}
			\item $\theta \sim N( \frac{b}{2a}, \frac{1}{2a})$
			\item $\int exp\{-a\theta^2 + b\theta \} d\theta =\sqrt{\frac{\pi}{a}} exp\{\frac{b^2}{4a} \} $
		\end{enumerate}


\begin{example}
	Suppose that

			$$X | \Theta \sim N(\Theta, \sigma^2) $$
			$$\Theta \sim N(\mu, \tau^2)$$

	Find $\pi_{\Theta|X} (\theta|x), f_X(x)$. 

	\textbf{Solution:}\\

	\begin{align*}
		\pi(\theta|x) &\propto f(x|\theta) \pi(\theta)\\
				&= \frac{1}{\sqrt{2 \pi \sigma^2}} exp\{-\frac{1}{2} \frac{(x-\theta)^2}{\sigma^2}\} * \frac{1}{\sqrt{2 \pi \tau^2}} exp\{-\frac{1}{2} \frac{(\theta -\mu)^2}{\tau^2}\}\\
				&\propto  exp\{-\frac{1}{2} \frac{(x-\theta)^2}{\sigma^2}\} *  exp\{-\frac{1}{2} \frac{(\theta -\mu)^2}{\tau^2}\}\\
				&= exp\{-(\frac{1}{2\sigma^2} + \frac{1}{2 \tau^2})\theta^2 + (\frac{x}{\sigma^2} + \frac{\theta}{\tau^2})\theta\}
	\end{align*}

Using  Lemma 1.1, 

		$$\theta|X \sim N\left(\frac{\frac{x}{\sigma^2} + \frac{\mu}{\tau^2}}{1/2(2\sigma^{-2} + 1/2\tau^{-1} )}, \frac{1}{\frac{1}{\sigma^2} + \frac{1}{\tau^2}}\right) $$

How about $f_X(x)$?\\

		\begin{align*}
			f_X(x) &= \int f(x|\theta) \pi(\theta) d\theta\\
					&\vdots\\
					&= \frac{1}{{2 \pi \sigma \tau}} * exp\{-\frac{x^2}{2\sigma^2} -  \frac{\mu^2}{2\tau^2}\} \int exp\{- (\frac{1}{2\sigma^2} + \frac{1}{2 \tau^2})\theta^2 + (\frac{x}{\sigma^2} + \frac{\mu}{\tau^2})\theta \}\\
					&= \dots * \sqrt{\frac{\pi}{\frac{1}{2\sigma^2} + \frac{1}{2 \tau^2}}} exp\{\frac{(\frac{x}{\sigma^2} + \frac{\mu}{\tau^2})^2}{4 (\frac{1}{2\sigma^2} + \frac{1}{2\tau^2})}\}
		\end{align*}

		We want to identify this as a p.d.f of x, so we can treat anything that is not x as a constant. Using elementary algebra we get...

		\begin{align*}
			&\propto exp \{- (\frac{x^2}{2(\tau^2 + \sigma^2)} + \frac{x \mu}{(\sigma^2 + \tau^2)}) \}
		\end{align*}

		Applying Lemma 1.1 for x and simplifying, 

				$$X \sim N(\mu, \tau^2 + \sigma^2) $$


\end{example}

This can be extended to multivariate setting, 2-sample setting, ANOVA setting, regression setting, etc. It is essential to all aspects of linear models. 



\textbf{Wednesday January 25}\\

\begin{example}
	Suppose

			$$X_1, \dots, X_n | \theta \sim^{iid} N(\theta, \sigma^2) $$
			$$\theta \sim N(\mu, \tau^2) $$

	$\pi(\theta| X_1, \dots, X_n) =$?\\


	By Example 1.1, 

			$$\theta | \bar{X} \sim N\left(\frac{\frac{\bar{X}}{\sigma^2/n} + \frac{\mu}{\tau^2}}{\frac{1}{\sigma^{2}/n} + \frac{1}{\tau^{2}})}, \frac{1}{\frac{1}{\sigma^2/n} + \frac{1}{\tau^2}} \right) $$


	Note that the sample size will effect how much weight each of the pervious means is given. As $n \rightarrow \infty$, depending on which coeffecient goes to 1, 

			$$E(\theta | \underline{X}) \rightarrow \bar{X} \text{ or } \mu $$

	Similarly with variance, as $n \rightarrow \infty$, 

			$$Var(\theta| \underline{X}) \rightarrow 0 $$

	We can generalize/approximate this phenomenon as follows, 

			$$\theta|X_1, \dots, X_n \sim N(\hat{\theta_1}, I^{-1}(\hat{\theta})) $$

	where $\hat{\theta}$ is the MLE. \\

	In our special case, 

			$$\theta| X_1, \dots, X_n \sim N(\bar{X}, \frac{\sigma^2}{n}) $$


\end{example}


	\section{Conjugate Priors}\index{Conjugate Priors}

	\subsection{Introduction}\index{Introduction}

	In general, computing posterior distributions or posterior means is a hard problem involving high-dimentional numerical integration. This was a big hurdle for Bayesian methods before computers. Now we can do methods such as Monte Carlo inegration (MCMC).\\

	In special cases, such as with the Exponential Family and mixture of distributions, posterior can be expressed explicitly through the use of conjugate families. \\

	\begin{definition}[Conjugate Family]
			A family of distributions, $\mathcal{P}$, on ($\Omega_\theta, \mathcal{F}_\theta$) is a \textbf{conjugate family} if 

					$$P_\Theta \in \mathcal{P} \Rightarrow \mathcal{P}_{\Theta|X} \in \mathcal{P} $$

	
		\end{definition}

Not unique, for example if you let $\mathcal{P}$ be the colleciton of all distributions on ($\Omega_\theta, \mathcal{F}_\theta$), then then it is always conjugate. Usually there is a suitable conjugate family. 

\subsection{Exponential Family}

\begin{example}
	$X_1, \dots, X_n |\theta \sim Pois(\theta)$\\

			\begin{align*}
				f(x_1, \dots, x_n | \theta) &= \prod^n_{i=1} \frac{\theta^{x_i}}{x_i!} e^{-\theta} \\
						&= \frac{\theta^{\sum x_i} e^{-n\theta}}{\prod(x_i!)}\\
						&\propto \theta^{c_1} e^{-c_2 \theta}
			\end{align*}

	Note that we are treating $\theta$ as the variable of interest here, not x. Recall that if $\theta \sim Gamma(\alpha, \beta)$ then we have a distribution approaching the form above, that is, 

			$$\pi(\theta) \propto \theta^{\alpha - 1} e^{-\theta/\beta}$$

	If we use this form, then we may find $pi(\theta|\underline{X})$ using the following:

			$$pi(\theta|\underline{X}) \propto \theta^{c_1 + \alpha -1} e^{-(c_2 + 1/\beta)\theta} $$

	Which gives us that

			$$\theta| X_1, \dots, X_n \sim Gamma(c_1 + \alpha, (c_2 + 1/\beta)^{-1}) $$

	This is generally true for all exponential family distributions. 


\end{example}

We say that X has exponential family distrubiton if is has p.d.f in the form of 

		$$\frac{e^{\theta^T t(x)}}{\int e^{\theta^T t(x) d\mu(x)}} $$

where $\mu$ is a $\sigma$-finite measure on $(\Omega_X, \mathcal{F}_X)$. \\

Essentially, p.d.f. with $\mu(x) \propto e^{\theta^T t(x)}$.\\

More generally, suppose that $\phi: \Theta \mapsto \Phi$ bijection (one-to-one onto). Then X has Exponential Family distribution if and only if the p.d.f of X with $\mu$ is 

		$$ \frac{e^{\phi^T t(x)}}{\int e^{\phi^T t(x) d\mu(x)}}$$

In this case, $X \sim Exp(\phi, t, \mu)$, when $\phi$ is identity, this is called the canonical form of exponential family. \\

\begin{theorem}[1.3 from class]
	If 
		$$P_\Theta \sim Ep(\xi, \phi, \nu )$$
		$$P_{X|\Theta} \sim Ex(\phi, t, \mu)$$

	then 

			$$P_{\Theta|X} \in Ep(\xi_X, \phi, \nu_x) $$

	where

			$$\xi_X(\alpha) = \xi(\alpha) + t(x)$$

			$$d\nu_X(\theta) = \frac{d \nu(\theta)}{\int e^{\phi^T(\theta) t(x)} d\mu(x)} $$
\end{theorem}

\begin{proof}
	$P_\Theta \in Ep(\xi, \phi, nu) \Rightarrow \pi(\theta) = \frac{e^{\xi^T(\alpha) t(\theta)}}{\int e^{\xi^T(\alpha) t(\theta) d\nu(x)}}$\\

	\begin{align*}
		f(x|\theta) &= \frac{e^{\phi^T t(x)}}{\int e^{\phi^T t(x) d\mu(x)}}\\
		\pi(\theta|x) &= \text{PHOTO}\\
				&=  
	\end{align*}

GIANT fraciton of fractions = PHOTO

So, with respect to the new measure, 
	
	$$d\nu_X(\theta) = \frac{d\nu(\theta)}{\int e^{\phi^T(\theta) t(x)}d\mu(x)} $$

where the pdf is 

		$$\frac{}{} $$

So, $\theta|X \sim Ep(\xi(\alpha) + t(x), \phi(\theta), \frac{d\nu(\theta)}{\int e^{\phi^T (\theta) t(x) d\mu(x)}})$

\end{proof}

\textbf{Friday January 27}\\

\begin{example}[One Parameter Normal]
			$$X|\theta \sim N(\theta, \sigma^2) $$

	Want to assign conjugate prior for $\theta$. 

			$$f(x|\theta) \propto exp\{(\frac{-1}{2\sigma^2})\theta^2 + (\frac{\mu}{\sigma^2})\theta\} $$


	Recall Lemma 1.1. 

			$$ f(\theta) \propto exp\{(\frac{-1}{2\sigma^2})\theta^2 + (\frac{\mu}{\sigma^2})\theta\} $$

			$$\Rightarrow \theta \sim N(\mu, \sigma^2) $$

	So, consider the following family, 

			$$\mathcal{F} = \{f(\theta) \propto exp\left\{\frac{-1}{2\alpha_2^2}\theta^2 + \frac{\alpha_1}{\alpha_2^2}\theta\} \right\} $$

	Suppose that $\pi \in \mathcal{F}$. Then we have that

			\begin{align*}
				\pi(\theta|X) &\propto f(x|\theta) \pi(\theta)\\
					&\propto exp\{(\frac{-1}{2\sigma^2})\theta^2 + (\frac{\mu}{\sigma^2})\theta\} *  exp\left\{\frac{-1}{2\alpha_2^2}\theta^2 + \frac{\alpha_1}{\alpha_2^2}\theta\} \right\}\\
					&= exp\left\{(\frac{-1}{2\alpha_2^2} - \frac{1}{2\alpha_2})\theta^2 + (\frac{x}{\sigma^2} + \frac{\alpha_1}{\alpha_2^2})\theta\} \right\} \\
					&= - \frac{1}{2}(\frac{1}{\sigma^2} + \frac{1}{\alpha^2})
			\end{align*}

	So we have that 

		$$\theta|X \sim N(\frac{\frac{\alpha_1}{\alpha_2} + \frac{x}{\sigma_2}}{(\frac{1}{\sigma^2} + \frac{1}{\alpha^2})}, \frac{1}{\frac{1}{\sigma^2} + \frac{1}{\alpha^2}}) $$

\end{example}
	\subsection{Convex Hull of Conjugate Family}\index{Convex Hull of Conjugate Family}

	\begin{remark}

	Mathematically,  if $A \subseteq \mathbb{R}^k$,
		
		$$Conv(A) = \left\{\alpha_1 s_1 + \dots + \alpha_k s_k: \alpha_1 \geq 0, \dots, \alpha_k \geq 0; \alpha_1 + \dots + \alpha_k = 1; s_1 \in A, \dots, s_k \in A \right\} $$
	\end{remark}

\begin{remark}
					By the way, a convex combination of a set of probability measures, say $P_1, \dots, P_k$, is called the \textbf{mixture} of $P_1, \dots, P_k$. So $Conv(\mathcal{P})$ is simply the colleciton of all mixture distributions derived from $\mathcal{P}$. 
				\end{remark}

	\begin{theorem}
		If $\mathcal{P}$ is conjugate to $P_{X|\theta}$, then $Conv(\mathcal{P})$ is also conjugate. 


	\end{theorem}

	\begin{proof}
		Suppose that $P_\Theta \in Conv(\mathcal{P})$. Want to show that $P_{\Theta|X} \in Conv(\mathcal{P})$. 

		Sicne $P_\Theta \in Conv(\mathcal{P})$ there exists

				$$\alpha_1, \dots, \alpha_k \in \mathbb{R}, \sum^k \alpha_i = 1, \alpha_i \geq 0 $$

				$$P_\Theta^{(1)} \dots  P_\Theta^{(k)} \in \mathcal{P}$$

		such that

				$$P_\Theta = \sum^k \alpha_i P_\Theta^{(i)} $$

				\begin{align*}
					f_X(x) &= \int f(x|\theta) dP_\Theta\\
							&= \int f(x|\theta) d(\sum^k \alpha_i P_\Theta^{(i)})\\
							&= \sum^k \alpha_i \int f(x|\theta) dP_\Theta^{(i)}\\
							&=\sum^k \alpha_i \int m_i(x) dP_\Theta^{(i)}\\
						dP_\Theta(\cdot|x)	&= \frac{f(x|\theta)}{f_X(x)} dP_\Theta\\
						&=\frac{f(x|\theta)}{\sum^k \alpha_i m_i(x)} \sum^k \alpha_k d P_\Theta^{(i)}\\
						&=\sum^k \frac{\alpha_i m_i(x)}{\sum^k \alpha_j m_j(x)} \frac{f(x|\theta)dP_\Theta^{(i)}}{m_i(x)}
				\end{align*}

				Note that the first term is great than zero and sums to 1. It's our new $\alpha^*$. In the second term note that $P_\Theta^{(i)} \in \mathcal{P}$ and $P_{\Theta|X}^{(i)}(\cdot|x) \in \mathcal{P}$. 

				Thus this is a convex combination of members of $\mathcal{P}$ in $Conv(\mathcal{P})$.  

				
	\end{proof}




This is a nice way to construct conjugate families of mixtures of exponential family. 

\section{Two-Parameter,  Normal Family}

\begin{definition}[Inverse $\chi^2$ Disribution]
	A random variable T is said to have an inverse $\chi^2$ distribution of $\chi^{-2}_{(k)}$ if and only if, 

		$$\frac{1}{T} \sim \chi^2_{(k)} $$

	If, for some $\tau < 0$, $\frac{T}{\tau} \sim \chi^{-2}_{(k)}$ then we write that 

			$$T \sim \tau \chi^{-2}_{(k)} $$

	
\end{definition}

By Jacobian theorem, 

			$$T = g(X) $$

			$$f_T(t) = f_X(g^{-1}(t)) |det(\frac{\partial g^{-1}(t)}{\partial t})| $$

Above will be shown in HW. 

\begin{theorem}
	If $T \sim \chi^{-2}_{(\nu)}$ then, 

			$$f(t) = \frac{1}{\Gamma(\frac{\nu}{2}) 2^{\frac{\nu}{2}}} t^{-\frac{\nu}{2}-1} e^{\frac{-1}{t}}, t > 0 $$
\end{theorem}

\begin{proof}
	Shown in HW 2. 
\end{proof}

Now suppose that $X_1, \dots, X_n | \phi, \lambda \sim^{iid} N(\lambda, \phi)$ where both $\lambda$ and $\phi$ are random. How do we assign conjugate prior of ($\lambda, \phi$)?\\

From classical statistics (Stat 514) we know that the sufficience statistic for $\lambda, \phi$  is $(\sum X_i, \sum X_i^2)$ or $(T = \bar{X}, S = \sum^n_{i=1} (X_i -\bar{X})^2$. 


So $\pi(\lambda, \phi|X_1, \dots, X_n) = \pi(\lambda,\phi | T, S)$. Moreover, form Normal theory, 

		$$T \indep X | \lambda, \phi $$
		$$T | \lambda, \phi \sim N(\lambda, \phi/n) $$
		$$S | \lambda, \phi \sim \phi \chi^2_{(n-1)} $$


\textbf{Monday January 30}\\


So, we need only to look at [GET NOTES FROM BUDDY]


	\begin{align*}
		f(T, S |\lambda, \phi) &= f(S|\lambda, \phi) f(T|\lambda,\phi)\\
			&=f(S|\phi) f(T|\lambda, \phi)\\
			&=[S|\phi] [T|\lambda, \phi]\\
			&= \left[\phi^{-(\frac{n-1}{2} - 1)} exp\left\{(\frac{-s}{2\phi})(\frac{1}{\phi})\right\}\right] * \left[\frac{1}{\sqrt{2\pi} (\phi/n)^{1/2}} exp\left\{\frac{-1}{2(\phi/n)}\lambda^2 + \frac{t}{\phi/n}t + (\frac{-1}{2(\phi/n)})t^2\right \} \right]
	\end{align*}

	So we have $N(t, \frac{\phi}{n}) s\chi^{-2}_{(n-3)})$. 

	More generally, the distribution, 

			$$N(a, \frac{\phi}{m}) * \tau \chi^{-2}_{(k)} $$

	is referred to (by Bing Li) as NICH (Normal-Inverse CHi-square). Here, 

			$$NICH(a, m, \tau, k)$$


	We may say that $(\lambda, \phi) \mapsto [S|\phi][T|\lambda, \phi]) \sim NICH(t,n,s, n-3)$. The likelihood is NICH. 


\textbf{Lemma 1.2} 

		$$NICH(a_1, m_1, \tau_1, k_1) * NICH(a_2, m_2, \tau_2, k_2) = NICH(a_3, m_3, \tau_3, k_3)$$


		Where we have that\\
\\
		$a_3 = \frac{m_1a_1 + m_2a_2}{m_1 + m_2}$\\
		$m_3 = m_1 + m_2$\\
		$\tau_3 = \tau_1 + \tau_2 + m_1a_1^2 + m_2a_2^2 - m_3a_3^2$\\
		$k_3 = k_1 + k_2 -3$\\


Before proof, let's rewrite likelihood as

		$$NICH(a, m, \tau, k) \propto  \phi^{-1/2}exp\{\frac{-1}{2(\phi/m)}\lambda^2 + \frac{a}{\phi/m}\lambda\} \phi^{-(k/2) - 1} exp\{-\frac{\tau + ma^2}{2\phi}\}$$

\begin{proof}
	By definition 

			\begin{align*}
				NICH_1 * NICH_2 &\propto \phi^{-1/2}exp\{\frac{-1}{2(\phi/m_1)}\lambda^2 + \frac{a_1}{\phi/m_1}\lambda\} \phi^{-(k_1/2) - 1} exp\{-\frac{\tau_1 + m_1a_1^2}{2\phi}\} * \phi^{-1/2}exp\{\frac{-1}{2(\phi/m_2)}\lambda^2 + \frac{a_2}{\phi/m_2}\lambda\} \phi^{-(k_2/2) - 1} exp\{-\frac{\tau_2 + m_2a_2^2}{2\phi}\}\\
					&= \phi^{-1/2}exp\{(\frac{-1}{2(\phi/m_1)} - \frac{1}{2(\phi/m_2)})\lambda^2 + (\frac{a_1}{\phi/m_1} + \frac{a_2}{\phi / m_2})\lambda\} \phi^{-(k_1/2) - 1} \phi^{-(k_2/2) - 1} exp\{-\frac{\tau_1 + m_1a_1^2}{2\phi} - \frac{\tau_2 + m_2a_2^2}{2\phi}\}\\
					&= \phi^{-1/2}exp\{(\frac{-1}{2(\phi/m_3)} )\lambda^2 + (\frac{a_3}{\phi/m_3})\lambda\} \phi^{-(k_3/2) - 1}  exp\{-\frac{\tau_3 + m_3a_3^2}{2\phi} \}
			\end{align*}



\end{proof}


Recall we have that $(\lambda, \phi) \mapsto [S|\phi][T|\lambda, \phi]) \sim NICH(t,n,s, n-3)$. By Lemma 1.2, we assign prior

		$$[\lambda| \phi][\phi] \sim NICH(a,m,\tau, k) $$

This gives us that 

		$$\pi(\lambda, \phi | T, S) \sim NICH(a^*, m^*, \tau^*, k^*) $$


where we have that \\
\\

$a^* = \frac{m a + n T}{m + n}$\\
$m^* = m + n$\\
$\tau^* = \tau + S + m a^2 + nt^2 - (m+n)(\frac{ma + nt}{m+n})^2$\\
$k^* = k + n -3 + 3$\\


So, $[\lambda | \phi, X] \sim N(a^*, \frac{\phi}{m^*})$, and $[\phi|X] \sim \tau^* \chi^{-2}_{k^*}$. 


Note that we can make inference about $\lambda, \phi$ using 

		$$\frac{\lambda - a^*}{\sqrt{\phi/m^*}} | \phi, X \sim N(0, 1) $$

since the RHS doesn't depend on $\phi$ we have that it is independence form $\phi|X$. 

		$$\frac{\lambda - a^*}{\sqrt{\phi/m^*}} | X \sim N(0, 1) $$

\textbf{Wednesday February 1}\\

We want to make inference about 

		$$\frac{\frac{\lambda - a^*}{\sqrt{\phi/m^*}}}{\sqrt{\frac{\tau^*}{\phi}/m^*}} | X \sim t_{m^*} $$

We may reorganize this to be 


		$$\frac{m^*(\lambda - a^*)}{\sqrt{\tau^*/m^*}}\sim t_{m^*} $$


Similarly, for making inference about $\phi$, 

		$$\phi | X \sim \tau^* \chi^{-2}_{m^*} $$

		$$\frac{\tau^*}{\phi}|X \sim \chi^2_{m^*} $$


\section{Noninformative Prior}\index{Noninformative Prior}

\subsection{General Concept}

Even when you don't have a prior distribution it's still beneficial to use Bayesian setting. For example, in dealing with nuisance parameters we deal with high dimension prior. \\

So here, try to use Bayesian to solve "no prior information" proglem. We want to use a flat prior, e.g. the Lebesgue measure. But, in what is this flat? Supppose we impose the Lebesgue measure on $\theta$. Then the prior for monotone transformation of $\theta$, say $\theta^3$, is not Lebesgue anymore. 


\begin{definition}[Improper Prior]
	An infinite, but $\sigma$-finite measure on $\Omega_\Theta$ is called an \textbf{improper prior}.
\end{definition}

\begin{example}
			$$ X|\theta \sim N(\theta, \phi) $$

	Note that here $\phi$ is known. 

	$$\pi(\theta) \equiv 1 $$
	$$f(X|\theta) = \sqrt{2\pi}^{-1} e^{-1/2 * (x - \theta)^2} $$

	$$\pi(\theta|X) = \sqrt{2\pi}^{-1} e^{-1/2 * (x - \theta)^2} * 1  = N(X, 1)$$

	$$f_X(x) = \frac{f(x|\theta) \pi(\theta)}{\pi(\theta|X)} = \frac{f(x|\theta)}{\pi(\theta|x)} = 1$$

	The marginal is Lebesgue improper! But, posterior is important
\end{example}


\begin{example}
	$$X_1, \dots, X_n|\lambda, \phi \sim N(\lambda, \phi) $$

	Here both parameters are random. 

	We have sufficient statistics, $(S = \bar{X}, T= \sum(X_i - \bar{X})^2)$. \\

	\begin{align*}
		[(\lambda, \phi) \mapsto f(t, s| \lambda, \phi)] &\sim NICH(t, n, s, n-3)\\
			&\propto \phi^{-1/2} exp\left\{\frac{-1}{2(\phi/n)}\lambda^2 + \frac{t}{\phi/n}\lambda\right\} \phi^{-((n-3)/2) - 1} exp\{-\frac{s+ nt^2}{2\phi}\}
	\end{align*}\\

	$$[\lambda|\phi] = 1 $$
	$$[\phi] = \frac{1}{\phi} $$

	When we multiply the p.d.f of our NICH by $\frac{1}{\phi}$ we get NICH(t, n, s, n-1) following the same argument in Seciton 1.5 (?) (proper case).


	We can show that 

			$$\frac{\sqrt{n}(\lambda - t(x))}{\sqrt{s(x)/(n-1)}}|X \sim t_{(n-1)} $$

			$$\frac{s(x)}{\phi}|X \sim \chi^2_{(n-1)} $$

	Exactly the same as frequentist sample distribution, except what random has changed. 

\end{example}

\subsection{Invariant Prior} % (fold)
\label{sub:invariant_prior}

What is flat? What is a natural generalization of Lebesgue Measure? \\

Lebesgue measure is invariant under translation. 

		$$\theta \mapsto \theta + c $$

If $\lambda$ is Lebesgue and T is translation, 

		$$\lambda \cdot T^{-1} = \lambda $$

This is natural generalization of flatness. Change T to be some other transformation that some how resembles translation = group of transformation. \\

\begin{remark}
Review definition of group of transformations. 

$\Omega$ (set)\\

$\mathcal{G}$ is a set of bijections (one-to-one on to) on $\Omega$ such that 

\begin{enumerate}
	\item for all $g_1, g_2 \in \mathcal{G}, g_1 \circ g_2 \in \mathcal{G}$
	\item for all $e \in \mathcal{G}$ such that $e \circ g = g \circ e = g$ for all $g \in \mathcal{G}$
	\item $g \in \mathcal{G} \Rightarrow g^{-1} \in \mathcal{G}$
\end{enumerate}	
\end{remark}

With our own problem, 

$$\Omega_\Theta \subseteq \mathbb{R}^P $$

Consider a parametric group, 

		$$\{g_t : t \in \Omega_\theta \} $$

Consider two types of transformations, 

		$$\theta \mapsto g_t(\theta) = L_t$$
		$$\theta \mapsto g_\theta(t) = R_t$$

We may refer to these transformations as Left and Right transformations, respectively. 

Two types of invariant priors or two generalizations of Lebesgue measure, 2-generalization of flatness. 

\begin{definition}
	A measure, $\Pi$, on $\Omega_\Theta$ is the left Haar measure. 

			$$\Pi = \Pi \circ L^{-1}_t \forall t \in \Omega_\Theta $$

	A measure, $\Pi$, is the right Harr measure if 

			$$ \Pi = \Pi \circ R^{-1}_t \forall t \in \Omega_\Theta$$
\end{definition}

\begin{example}
	$$ \Omega_\Theta = \mathbb{R}$$

Translation group;
	$$\mathcal{G} = \{(\theta \mapsto \theta + c = g_c(\theta)): c \in \mathbb{R} \} $$


Suppose that the improper prior density of $\theta$ is $\pi(\theta)$. 

		$$L_t(\theta)= g_c(\theta) = \theta + c = \eta $$

then the density of $\eta$ is 

		$$\phi(g_c^{-1}(\eta) |\frac{\partial g_c^{-1}(\eta)}{\partial\eta}| $$

We want to find

		$$\pi(\cdot)  = \pi(\cdot - c) $$
		$$ \pi(\theta)  = \pi(\theta - c) \quad \forall \theta \in \mathbb{R}$$

If we take $\theta = 0$, 

	$$\pi(0) = \pi(-c) $$


If we take $-c = \theta$, 

		$$\pi(\theta) = \pi(0) \propto \text{ Lebesgue} $$

	Left Haat measure IS the Lebesgue measure. 

Right transformation, 
		
		\begin{align*}
			R_c(\theta) &= g_\theta(c)\\
				&=c + \theta\\
				&= \theta + c\\
				&=g_c(\theta)\\
				&= L_c(\theta)
		\end{align*}

	The right Haar measure is also proportional to the Lebesgue. 
\end{example}

\textbf{Friday February 3}\\

\begin{example}
	Haan measures for this group:

			$$\Omega_\Theta = (0, \infty) $$

			$$\mathcal{G} = \{(\theta \mapsto a\theta): a > 0 \} $$

	We can show that a goup used for distributions like N(0, $\theta$)

	Left tansformation:\\

			$$L_a = a\theta = \eta $$

	if density for $\theta$ is $\pi(\theta)$. 

			\begin{align*}
				\pi_\eta(\eta) &= \pi(\frac{\eta}{a}) |\frac{d \eta/a}{d \eta}|\\
						&=\pi\left(\frac{\eta}{a}\right) \frac{1}{a}\\
				\pi_\eta(1) &= \pi\left(\frac{1}{a}\right)\frac{1}{a}\\
				\pi(\frac{1}{a}) &\propto a\\
				\pi(\theta = \frac{1}{a}) &\propto \frac{1}{\theta}\\
			\end{align*}

	Right transformation:\\

			$$R_a(\theta) = g_\theta(a) = \theta a = L_a(\theta)$$

	Right Haar density is also 

			$$\pi(\theta) = \frac{1}{\theta} $$
\end{example}


\begin{example}
	$N(\mu, \sigma^2), \Omega_\Theta = \{(\mu, \sigma): \mu \in \mathbb{R}, \sigma > 0\}$

	Consider group, 

			$$\mathcal{G} = \{[(\mu, \sigma) \mapsto (c\mu+ b, c\sigma) = (g_{b,c}(\mu, \sigma))]: b \in \mathbb{R}, c > 0 \} $$

	Left transformation:\\

			$$L_{b, c} (\mu, \sigma) = g_{b, c}(\mu, \sigma) = (c\mu + b, c\sigma) = (\tilde{\mu} , \tilde{\sigma}) $$

			\begin{align*}
				\pi_\eta(\tilde{\mu} , \tilde{\sigma}) &= \pi(\frac{\tilde{\mu}- b}{c}, \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
					\frac{\partial \mu}{\partial \tilde{\mu}} & \frac{\partial \mu}{\partial \tilde{\sigma}}\\
					\frac{\partial \sigma}{\partial \tilde{\mu}} & \frac{\partial \sigma}{\partial \tilde{\sigma}}\\
					\end{matrix}\right|\\
				&=\pi(\frac{\tilde{\mu}- b}{c}, \frac{\tilde{\sigma}}{c}) \left|\	\begin{matrix}
					\frac{1}{c} & 0\\
					0 & \frac{1}{c}\\
					\end{matrix}\right|\\
				&=\pi(\frac{\tilde{\mu}- b}{c}, \frac{\tilde{\sigma}}{c}) \frac{1}{		c^2}\\
				\pi_\eta(0,1) &= \pi(\frac{- b}{c}, \frac{1}{c}) \frac{1}{c^2}
			\end{align*}

		So if we consider $(\frac{- b}{c}, \frac{1}{c})  = (\mu, \sigma)$ we get that

				$$\pi(\mu, \sigma) \propto \frac{1}{\sigma^2} $$

		which is the Left Haan measure. 


		Right transformation:\\

				$$ R_{b, c} (\mu, \sigma) = g_{\mu, \sigma}(b, c) = (\mu + \sigma b, c\sigma) = (\tilde{\mu}, \tilde{\sigma}) \neq L_{b,c} $$


				\begin{align*}
					\sigma b + \mu &= \tilde{\mu}\\
					\sigma c &= \tilde{\sigma}\\
					\sigma &= \frac{\tilde{\sigma}}{c}\\
					\mu &= \tilde{\mu} - \sigma b = \tilde{\mu} - \tilde{\sigma}\frac{b}{c}
				\end{align*}
				\begin{align*}
					\pi_\eta(\tilde{\mu}, \tilde{\sigma}) &= \pi(\tilde{\mu} - \	tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
						\frac{\partial \mu}{\partial \tilde{\mu}} & \frac{\partial \mu}{\partial \tilde{\sigma}}\\
						\frac{\partial \sigma}{\partial \tilde{\mu}} & \frac{\partial \sigma}{\partial \tilde{\sigma}}\\
						\end{matrix}\right|\\
					&= \pi(\tilde{\mu} - \tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
						1 & \frac{-b}{c}\\
						0 & \frac{1}{c}\\
						\end{matrix}\right|\\
					&= \pi(\tilde{\mu} - \tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
						1 & \frac{-b}{c}\\
						0 & \frac{1}{c}\\
						\end{matrix}\right|\\
					&= \pi(\tilde{\mu} - \tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \frac{1}{c}
				\end{align*}

				If we replace $(\tilde{\mu}, \tilde{\sigma}) = (0, 1)$ we see that 

						$$ \pi(\mu, \sigma) \propto \frac{1}{\sigma} $$

				Thus the Right and Left Harr are not the same. 
\end{example}

	

% subsection invariant_prior (end)
% %------------------------------------------------

\subsection{Jeffreys' Prior} % (fold)
\label{sub:Jeffrey}

Haar requires natural group of transformations which is not always available in particular appliations. An easily available prior, Jeffrey's prior is constructed using the folloiwng principle. \\

If we assign $\theta$ a measure, $\Pi$ and $\eta = T(\theta)$ is one-to-one transformation, then the prior assigned to $\eta$ should satisfy $\Pi \circ T^{-1}$

Jefferys' Prior\\

Let $f(X|\theta)$ be the likelihood. Let $I(\theta)$ be the Fisher Information

		$$I(\theta) = - E\left[\frac{\partial^2}{\partial\theta \partial \theta^T} \log f(X|\theta) |\theta \right] $$ 

The Jeffreys' Prior is defined as 

		$$\Pi(\theta) \propto \sqrt{det I(\theta)} $$

\begin{theorem}[1.16]
	Let $\pi_\Theta$ be the Jeffreys' prior density of $\theta$. $\Pi_\Phi(\phi)$ is the Jeffreys' prior density for $\phi$ where $\phi = h(\theta)$. Here, $h$ is one-to-one. Then we have that

			$$\pi_\Phi(\phi) = \Phi_\Theta(h^{-1}(\phi)) \left| det\left(\frac{\partial h^{-1} (\phi)}{\partial \phi}\right) \right| $$

	In terms of measure, $\Pi_\Phi = \Pi_\Theta \circ h^{-1}$.
\end{theorem}

\begin{proof}
	Let $f_{X|\Phi}(x|\phi)$ represent the likelihood of $\phi$. Because $\Phi$ and $\Theta$ are one-to-one, 

			$$f_{X|\Phi}(x|\phi) = f_{X|\theta}(x| \theta) = f_{X|\Theta}(X |h^{-1}(\phi)) $$

	Conditional distributions only depends on $\sigma$-field and two one-to-one variables generage same $\sigma$-field. \\

	GET MORE FROM PHOTOS
\end{proof}

\textbf{Monday February 6}\\

\begin{example}
	$$X_1, \dots, X_n \sim N(\theta, \sigma^2)$$

	where both $\theta \& \sigma$ are unknown. \\

	Find Jeffreys' prior for $\theta, \sigma$. \\

			$$f(x_i|\theta, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{-\frac{(x_i - \theta)^2}{2 \sigma^2} \} $$

			$$f(x|\theta, \sigma) = \frac{1}{(\sqrt{2 \pi \sigma^2})^n} \exp\{-\frac{\sum_i(x_i - \theta)^2}{2 \sigma^2} \} $$

			$$\log(f(x_i|\theta, \sigma) )= -n \sqrt{2 \pi \sigma^2} -\frac{(x_i - \theta)^2}{2 \sigma^2}  $$

			$$\frac{\partial^2}{\partial \theta^2}\log(f(x_i|\theta, \sigma) )= -\frac{n}{\theta} $$

			$$\frac{\partial^2}{\partial \theta \partial \sigma^2}\log(f(x_i|\theta, \sigma) )= -(\sigma^2)^{-2} \sum(x_i - \theta) $$

			$$\frac{\partial^2}{\partial \sigma^2}\log(f(x_i|\theta, \sigma) )= \frac{n}{2} (\sigma^2)^{-2} - (\sigma^2)^{-3} \sum(x_i - \theta)^2 $$

			Want to take negative expecation of each of theses. 

			$$I(\theta, \sigma^2) = \begin{pmatrix}
				\frac{n}{\sigma^2} & 0 \\
				0 & \frac{n}{2} (\sigma^2)^{-2}\\
			\end{pmatrix} $$

			Want to find the determinent of I. Here it's proportional to $\sigma^{-6}$. 

					$$\sqrt{det(I)} = \sigma^{-3} = \phi^{\frac{3}{2}} $$

			Which gives us our Jeffrey's prior for $(\theta, \sigma^2).$


\end{example}

% subsection jeffrey (end)

\section{Statistical Decision Theory}\index{Decision Theory}

	Rock, Paper, Scissor. \\

	Player One Action Space: $\Omega_{A, 1} = \{R, P, S\}$\\

	Player Two Action Space: $\Omega_{A, 2} = \{R, P, S\}$\\

	Loss Function - $L: \Omega_{A, 1} x \Omega_{A, 2} \rightarrow \mathbb{R}$\\

	Without loss of generality, we may assume the lose to be player one's loss. \\

	Winner gains one dollar, loser loses a dollar, and in the case of a tie, no money is exchanged. 


					\begin{table}[]
				\centering
				\caption{Paper Rock Scissor Loss Table}
				\label{PRS_Loss}
				\begin{tabular}{lllll}
				  & R  & P  & S  &  \\
				R & 0  & -1 & 1  &  \\
				P & 1  & 0  & -1 &  \\
				S & -1 & 1  & 0  & 
				\end{tabular}
				\end{table}


So we have $(\Omega_{A,1}, \Omega_{A, 2}, L)$. In statistics, we may refer to player one as the "statistician" and player two as "nature".\\

Player one's action is called the action space, $\Omega_A$. Player two's action space is called the parameter space, $\Omega_\Theta$.\\


But where does our data come from in this scenario? We are dealing with a statistical decision problem. 

\textbf{Statistical Decision Problem}\\

The statisticain (player one) wants to know nature's action, $\Theta$, but he doesn't. He observes a random variable, whose distribution depends on $\theta$. This is defused information about $\theta$. The random element, X, takes value on 

	$$(\Omega_X, \mathcal{F}_X)  $$

	where $X \sim P_\theta$, $\theta \in \Omega_\Theta$. \\


So we have

			$$ (\Omega_X, \mathcal{F}_X, \{P_\theta: \theta \in \Omega_\Theta\}) $$

on top of $(\Omega_A, \Omega_\Theta, L$. 

Decision rule:

		$$d: \Omega_X \rightarrow \Omega_A $$

and the collection of all decision rule is $\mathcal{D}$. This incurs a loss $L(\theta, d(x))$. \\

Before we take an action, our anticipation of loss is 

		$$E(L(\theta, d(x))|\theta) $$

which we may refer to as \textbf{risk}. \\

Game: $(\Omega_A, \Omega_\Theta, L)$\\
Data: $(\Omega_X, \mathcal{F}_X)$\\
Model: $\{P_\theta: \theta \in \Omega_\Theta\}$\\
Decision Rule: $\mathcal{D}$\\
Risk: $R$

\textbf{Bayesian Statistical Decision Problem}\\

Here, the game, data, model, and decision rule remain the same. The difference is in how we calculate the risk and the use of a prior.

Game: $(\Omega_A, \Omega_\Theta, L)$\\
Data: $(\Omega_X, \mathcal{F}_X)$\\
Model: $\{P_\theta: \theta \in \Omega_\Theta\}$\\
Decision Rule: $\mathcal{D}$\\
Prior: $P_\Theta$\\
Risk: $r$\\

		$$r(d) = E_{\Theta, X}[L(\Theta, d(x))] = \int_{\Omega_\Theta x \Omega_X} L(\theta, d(x)) dP_{\Theta, X} $$  



\textbf{Wednesday February 8}\\


Bayes Rule: Optimal estimation, optimal test. This is (mathematically) easier than frequentist optimal procedures becuase we have a metric in $\Theta$. Don't have to make uniform statements about $\theta$, such as UMVUE, UMP, UMPU, $\dots$. \\


\begin{definition}[Bayes' Rule]
	The Bayes rule is 

			$$d_B = \arg\min\{r(d): d \in \mathcal{D} \} $$

If $P_\Theta$ is improper then $d_B$ is called \textbf{generalized Bayes' Rule}. 

		$$r(d) = \int_{\Omega_\Theta x \Omega_X} L(\theta, d(x))  f_{X, \Theta}(x, \theta) d(\mu_x x \mu_\Theta)(x, \theta)$$


Note that usually $L(\theta, d(x)) \geq 0$ so we have that

		$$f_{x, \Theta} = \left\{\begin{array}{ll}
			f(x|\theta) \pi(\theta)\\
			\pi(\theta|x) f(x)
		\end{array} \right. $$

First way:

		\begin{align*}
			r(d) &= \int_{\Omega_\Theta } \left[\int_{\Omega_X} L(\theta, d(x))  f_{X|\Theta}(x| \theta) d(\mu_x )(x)\right] \pi(\theta) d\mu_\Theta\\
				&= \int_{\Omega_\Theta } R(\theta, d) \pi(\theta) d\mu_\Theta\\
		\end{align*} 

Other way would swap $\Theta, X$. (??)

		\begin{align*}
			r(d) &= \int_{\Omega_X } \left[\int_{\Omega_Theta} L(\theta, d(x))  \pi_{\Theta|X}(x| \theta) d(\mu_\Theta )(\theta)\right]  d\mu_X\\
				&= \int_{\Omega_X } \rho(x, d(x)) d\mu_X(x)\\
		\end{align*} 

Where we have the posterior expected loss, 

		$$\rho(x, a) = E(L(\theta, a)|X) $$

\end{definition}

INSERT PHOTO

How to calculate Bayes's rule, not by definition. \\

\begin{definition}
	

		$$\arg\min(r(d): d \in \mathcal{D}) $$

is the minimum over a set of functions,  $\mathcal{D}$. A member of $\mathcal{D}$ is 

		$$d: \Omega_X \rightarrow \Omega_A $$

But this can be converted into minimization over numbers or vectors by the following theorem. 
\end{definition}

\begin{theorem}[1.7 in Notes]
	Suppose $L(\theta ,a) \geq C < - \infty$ for all $\theta \in \Omega_\Theta, a \in \Omega_A$. Then the decision rule,  

			$$d_B: \Omega_X \rightarrow \Omega_A, x \mapsto \arg\min \{\rho(x, a): a \in \Omega_A \} $$

	is the Bayes' Rule. 
\end{theorem}

$\Omega_A$ is usually $\mathbb{R}, \mathbb{R}^P$ or subsets thereof. So we are not minimizing over functional spaces. 

\begin{proof}
	By Tonelli's theorem, (because we can assume WLoG that $L(\theta, a) \geq 0, \quad \forall \theta, a$) 

			 $$r(d) = \int_{\Omega_X} \rho (x, d(x)) f_X(x) d \mu_X(x)$$

So, for any x, 

		$$\rho (x, d_B(x)) \leq \rho (x, d(x)) $$

Thus, 

		$$  \int_{\Omega_X} \rho (x, d_B(x)) f_X(x) d \mu_X(x) \leq \int_{\Omega_X} \rho (x, d(x)) f_X(x) d \mu_X(x)$$

And we have that $r(d_B) \leq r(d) \forall d \in \mathcal{D}$. 
\end{proof}

In frequentist theory, optimality have to be stated uniformly. \\

"Commonly used optimal criteria" in frequentist decision theory 

		\begin{enumerate}
			\item admissibility
			\item minimax 
		\end{enumerate}

\begin{definition}
	A decision rule is \textbf{inadmissible} if there exists $d' \in \mathcal{D}$ such that 

		\begin{enumerate}
			\item $R(\theta, d') \leq R(\theta, d) \forall \theta \in \Omega_\Theta$
			\item  $R(\theta, d') < R(\theta, d) \text{ for some } \theta \in \Omega_\Theta$
		\end{enumerate}

A decision rule that is not inadmissible is admissible. 
\end{definition}

\begin{definition}
	A decision rule $d\in \mathcal{D}$ is a \textbf{minimax rule} if for all $d' \in \mathcal{D}$, 

			$$\sup_{\theta \in \Omega_\Theta} R(\theta, d) \leq \sup_{\theta \in \Omega_\Theta} R(\theta, d')$$
\end{definition}

The relation between Bayes' rule and admissible rule, generally "all Bayes rule are adimissible". 

\begin{theorem}[1.8 in Notes]

Suppose 

	\begin{enumerate}
		\item for each $d \in \mathcal{D}$, $R(\theta, d)$ is integrable with respect to $P_\Theta$ 
		\item for any $d_1, d_2 \in \mathcal{D}$, 

				$$R(\theta, d_1) - R(\theta, d_2) < 0 $$

		for some $\theta \in \Omega_\Theta$ which implies that

				$$P(R(\theta, d_1) - R(\theta, d_2) < 0 ) > 0$$
	\end{enumerate}

Then any Bayes or generalized Bayes rule is admissiable. 


\end{theorem}

\begin{proof}
	Suppose $d_1 \in \mathcal{D}$ is Bayes and inadmissible. Then there exists $d_2 \in \mathcal{D}$ such that

		\begin{enumerate}
			\item $R(\theta, d_2) \leq R(\theta, d_1) \forall \theta \in \Omega_\Theta$
			\item  $R(\theta, d_2) < R(\theta, d_1) \text{ for some } \theta \in \Omega_\Theta$
		\end{enumerate}

But this gives us that 

		$$R(\theta, d_2) - R(\theta, d_1) \geq 0  \forall \theta$$

		and
			$$P(R(\theta, d_2) - R(\theta, d_1) < 0 ) > 0$$
Recall from STAT 517 that if $f \geq 0, \mu(f> 0) > 0$ then 
		
				$$\int f d\mu > 0 $$

	or that  if $f \leq 0, \mu(f< 0) > 0$ then

			$$\int f d\mu < 0  $$

	This implies that 

		\begin{align*}
			\Rightarrow \int R(\theta, d_2) - R(\theta, d_1) d P_\Theta < 0\\
			\Rightarrow \int R(\theta, d_2)d P_\Theta < \int R(\theta, d_1) d P_\Theta\\
			\Rightarrow r(d_2) < r(d_1)\\
			\Rightarrow d_1 \text{ not Bayes'}
			\end{align*}

\end{proof}











% %----------------------------------------------------------------------------------------
% %	CHAPTER 4
% %----------------------------------------------------------------------------------------

% \chapterimage{chapter_head_1.pdf} % Chapter heading image

% \chapter{Random Variable}


% %----------------------------------------------------------------------------------------
% %	CHAPTER 5
% %----------------------------------------------------------------------------------------

% \chapterimage{chapter_head_1.pdf} % Chapter heading image

% \chapter{Convergence in Probability/Limit Theorem}


% %----------------------------------------------------------------------------------------
% %	CHAPTER 6
% %----------------------------------------------------------------------------------------

% \chapterimage{chapter_head_1.pdf} % Chapter heading image

% \chapter{Radon-Nikodym Derivative Theorem}

% %----------------------------------------------------------------------------------------
% %	CHAPTER 7
% %----------------------------------------------------------------------------------------

% \chapterimage{chapter_head_1.pdf} % Chapter heading image

% \chapter{Special Topics}


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% \chapter*{Bibliography}
% \addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}}
% \section*{Books}
% \addcontentsline{toc}{section}{Books}
% \printbibliography[heading=bibempty,type=book]
% \section*{Articles}
% \addcontentsline{toc}{section}{Articles}
% \printbibliography[heading=bibempty,type=article]

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage
\phantomsection
\setlength{\columnsep}{0.75cm}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}}
% \printindex

%----------------------------------------------------------------------------------------

\end{document}