
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.0 (9/2/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations


\usepackage{mathrsfs}
\usepackage{amsbsy}
\usepackage{graphicx}




\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\E}{\mathrm{E}}

\newcommand{\Var}{\mathrm{Var}}

\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\MSE}{\mathrm{MSE}}

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
\coordinate [below=12cm] (midpoint) at (current page.north);
\node at (current page.north west)
{\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{background}}; % Background image
\draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering  Advanced Statistical Inference\\[15pt] % Book title
{\Large STAT 561 - Advanced Statistical Inference}\\[20pt] % Subtitle
{\huge Dr. Bing Li}}}; % Author name
\end{tikzpicture}};
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2013 John Smith\\ % Copyright notice

\noindent \textsc{Published by Publisher}\\ % Publisher

\noindent \textsc{book-website.com}\\ % URL

\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Part One}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Basic Ideas in Bayesian Anaylysis}


% %------------------------------------------------

\textbf{Mathematical Preparation}\\

\textbf{Monday January 9}\\

\begin{enumerate}
	\item Product $\sigma$-Field\\

			$(\Omega_1, \mathcal{F}_1, \mu_1), (\Omega_2, \mathcal{F}_2, \mu_2)$ are two measure spaces. The goal is to construct a $\sigma$-field on $\Omega_1x\Omega_2$. \\

			Let $\mathcal{A} = \{AxB: A \in \mathcal{F}_1, B \in \mathcal{F}_2\}$. \\

			The $\sigma$-field generated $\mathcal{A}$ is called the product $\sigma$-field, written as $\mathcal{F}_1 x \mathcal{F}_2$, that is $\sigma(\mathcal{A})$. This is NOT a cartesian product, which would be $\{(A, B): A\in \mathcal{F}_1, B \in \mathcal{F}_2\}$. 

	\item Proctuct Measure\\

			Let $E \in \mathcal{F}_1 x \mathcal{F}_2$. Let $E_2(\omega_1) = \{\omega_2:(\omega_1, \omega_2) \in E\}$  and similarly, $E_1(\omega_2) = \{\omega_1:(\omega_1, \omega_2) \in E\}$. \\

			It is true (in Billingsly) that 

			\begin{theorem}[Number Unknown]
				If $E \in \mathcal{F}_1 x \mathcal{F}_2$ then $E_1(\omega_2) \in \mathcal{F}_1$ for all $\omega_2 \in \Omega_2$. Similarly, $E_2(\omega_1) \in \mathcal{F}_2$ for all $\omega_1 \in \Omega_1$. \\


				If $f: \Omega_1 x \Omega_2 \rightarrow \mathbb{R}$ measurable $\mathcal{F}_1 x \mathcal{F}_2 \setminus \mathcal{R}$. Then for each $\omega_1 \in \Omega_1$, 

						$$ f(\omega_1, \cdot) \textcircled{m} \mathcal{F}_2 \setminus \mathcal{R} \text{ for each } \omega_2 \in \Omega_2. $$

						$$ f(\cdot, \omega_2) \textcircled{m} \mathcal{F}_1 \setminus \mathcal{R} $$

				Now, for each $E \in \mathcal{F}_1 x \mathcal{F}_2$ consider 

						$$f_{1, E}: \Omega_1 \rightarrow \mathcal{R}, \omega_1 \mapsto \mu_2(E_2,(\omega_2)) $$

				It can be shown that $f_{1, E}$ is uniformly measurable $\mathcal{F}_1 \setminus \mathcal{R}$ for all E. 


			\end{theorem}

	\begin{proof}
		Outline. 

			\begin{itemize}
				\item Show that if $\mathcal{L} = \{E: f_{1, E} \textcircled{m} \mathcal{F}_1 \setminus \mathcal{R}\}$ then $\mathcal{L}$ is a $\lambda$-system. 
				\item Let $\mathcal{P} = \{AxB: A \in \mathcal{F}_1, B \in \mathcal{F}_2\}$ then it is a $\pi$-system.
			

				Furthermore, if $ E = A x B$, 
				
						$$E_2(\omega_1) = \left\{ \begin{array}{ll}
							B & \omega_1 \in A\\
							\emptyset & \omega_1 \notin A
						\end{array} \right. $$	

				So, $\mu_2(E_2(\omega_1)) = \left\{ \begin{array}{ll}
							\mu_2(B) & \omega_1 \in A\\
							\emptyset & \omega_1 \notin A
						\end{array} \right. = I_A(\omega_1)\mu(B) = f_{1, E}$

				So, $f_{1, E} \textcircled{m} \mathcal{F}_1$. 

				Thus $\mathcal{P} \subseteq \mathcal{L}$. 	
				
				\item By $\pi-\lambda$ Theorem, $\mathcal{F}_1 x \mathcal{F}_2 \subseteq \mathcal{L}$. 

	\end{itemize}

		Similarly, $f_{2, E} \textcircled{m} \mathcal{F}_2 \setminus \mathcal{R}$. 

		We can now define two set functions, 

				$$\pi'(E) = \int f_{1, E} d\mu_1 $$
				$$\pi''(E) = \int f_{2, E} d\mu_2 $$

		Again using $\pi-\lambda$ Theorem, it can be shown that, $\pi', \pi''$ are both measure and if $\mu_1, \mu_2$ are $\sigma$-finite, then

				$$\pi' = \pi'' \text{ on } \mathcal{F}_1 x \mathcal{F}_2 $$

		Note that here, $\mathcal{P}$ equals $\mathcal{A}$ used at begining	of notes.


		We did not have a measure in $\mathcal{F}_1 x \mathcal{F}_2$. Now we have $\pi', \pi''$ both measures on $\mathcal{F}_1 x \mathcal{F}_2$, they are the same. We call this measure the product meaure, written as $\mu_1 x \mu_2$. 


		Note that $(\Omega_1 x \Omega_2, \mathcal{F}_1 x \mathcal{F}_2, \mu_1 x \mu_2)$ is called product measure space. 
	\end{proof}

	\item Tonelli's Theorem\\

			$(\Omega_1, \mathcal{F}_1, \mu_1), (\Omega_2, \mathcal{F}_2, \mu_2)$ are two $\sigma$-finite measure spaces. \\

			$(\Omega_1 x \Omega_2, \mathcal{F}_1 x \mathcal{F}_2, \mu_1 x \mu_2)$ is the product measure space.\\

			Suppose we have $f:\Omega_1 x \Omega_2 \rightarrow \mathbb{R} \textcircled{m} \mathcal{F}_1 x \mathcal{F}_2 \setminus \mathcal{R} $. Where $f \geq 0$ and 

					$$\int f d (\mu_1 x \mu_2) =\int \left[ \int(f(\cdot, \omega_2) d\mu_1) \right] d\mu_2$$

	\item Fubini's Theorem\\

	The conclusion of Tonelli's Theorem still holds if f is NOT nonnegative, but if f is integrable $\mu_2$. (integrable - integral of absolute value of function is finite) \\

\textbf{Wednesday January 11}\\	

	\item Conditional Probability\\

	This is a special application of Radon- Nikodgm Theorem. We know that 

			$$P(A|B) = \frac{P(A, B)}{P(B)} $$

	We may define $P(A|\mathcal{G})$ when $\mathcal{G}\subseteq \mathcal{F}$ as sub-$\sigma$-field. We defined this intuitively in elementary probability course (definition above), but we ar enot going to define it generally. \\

	Now let $A \in \mathcal{F}$ and $\mathcal{G} \subset \mathcal{F}$ be a $\sigma$-field. Consider the set function 

			$$\nu: \mathcal{G} \rightarrow \mathbb{R}, G \mapsto P(AG)$$ 

	It can be easily shown that $\nu$ is a measure on $\mathcal{G}$. Consider another set function, 

			$$\mu: \mathcal{G} \rightarrow \mathbb{R}, G \mapsto P(G) $$

	So $\mu$ is nothting but P restricted on $\mathcal{G}$.\\

	It's easy to show that $\nu << \mu$. 

			$$\mu(G) = 0 \Rightarrow P(G) = 0 \Rightarrow P(AG) = 0 \Rightarrow \nu(G) = 0$$

		By Radon-Nikodgm Theorem, there exists a $\delta$ such that

			$$\nu(G) = \int_G \delta d\mu \quad \forall G \in \mathcal{G} $$

		$\delta$ is called R-N Derivative, written as 

				$$\delta = \frac{d\nu}{d\mu} $$

		and is similar in  to $\frac{P(AG)}{P(G)}$, but it's more general. \\

		$\delta$ is called the conditional probability of A given $\mathcal{G}$. To distinguish it form P(A|B), where B is a set, we use $P(A||\mathcal{G})$, where $\mathcal{G}$ is a $\sigma$-field. By construction, \\

				\begin{enumerate}
					\item $\delta$ is measurable $\mathcal{G}$
					\item $\int_G \delta dp = P(AG) \quad \forall G\in\mathcal{G}$ \\
				\end{enumerate}


		Note that, by RNT, $\delta$ is unique with probability 1. Any $\delta'$ satisfying (a) and (b) has $\delta' = \delta a.e. P$. So, we say that $\delta$ is a version of conditional probability. \\

		So, $\delta$ is a version of $P(A||\mathcal{G})$ if and only if (a) and (b) are satisfied. We may define $P(A||\mathcal{G})$ either by RNT or (a) and (b). \\


		Properties of Conditional Probability\\

		It behaves like probability, but since it is a function, unique up to a.e. P, these properties have to be qualified by a.s. P.

		\begin{enumerate}	
			\item $P(\emptyset || \mathcal{G}) = 0, P(\Omega||\mathcal{G}) = 1$ a.s. P
			\item $0 \leq P(A|| \mathcal{G}) \leq 1$ a.s. P
			\item If $A_1, A_2, \dots$ are disjoint members of $\mathcal{F}$ then $P(\bigcup_n A_n || \mathcal{G}) = \sum_n P(A_n || \mathcal{G})$ a.s. P
		\end{enumerate}

		Let's consider the special case where $\mathcal{G}$ is a $\sigma$-field generated by some random element, T (i.e. $\mathcal{G} = \sigma(T)$). More specifically, for some measurable space $(\Omega_T, \mathcal{F}_T)$ where 


				$$T: \Omega \rightarrow \Omega_T \textcircled{m} \mathcal{F}\setminus\mathcal{F}_T \quad \mathcal{G} = T^{-1}(\mathcal{F}_T)$$

		Here, we write

		\begin{align*}
			P(A || \mathcal{G}) &= P(A || \sigma(T))\\
				&=P(A|| T^{-1}(\mathcal{F}_T))\\
				&= P(A || T)
		\end{align*}

		The following theorem makes checking that something is a conditional probability easier. In principle, we have to check $\int_G \delta dp = P(AG) \quad \forall G\in\mathcal{G}$. 

		\begin{theorem}[33.1 in Billingsly]
			Let $\mathcal{P}$ be a $pi$-system generating $\mathcal{G}$ and suppose that $\Omega$ is a countable union of sets in $\mathcal{P}$. An integrable function, $f$, is a version of $P(A || \mathcal{G})$ if 

				\begin{enumerate}
					\item f is measurable $\mathcal{G}$
					\item $\int_G f dp = P(AG) \quad \forall G \in \mathcal{P}$ 
				\end{enumerate}
		\end{theorem}

	\item Conditional Distribution\\

	Let there be probability space $(\Omega, \mathcal{F}, P)$, measurable space $(\Omega_X, \mathcal{F}_X)$, and a random element, $X:\Omega \rightarrow \Omega_X \textcircled{m} \mathcal{F}\setminus\mathcal{F}_X$. Also, let $\mathcal{G} \subseteq \mathcal{F}$ be a sub $\sigma$-field. \\

	We are going to define conditional distribution of X given G. Under very mild conditions there is a function

			$$f: \mathcal{F}_X x \Omega \rightarrow \mathbb{R} $$

	such that for each $A \in \mathcal{F}_X$, $f(A, \cdot)$ is a version of 

			$$P(X \in A || \mathcal{G}) = P(X^{-1}(A) || \mathcal{G}) $$

	and, for each $\omega \in \Omega$,  $f(\cdot, \omega)$ is a probability measure on $(\Omega_X, \mathcal{F}_X)$. \\

	The only condition for this existance is $(\Omega_X, \mathcal{F}_X)$ must be a Borel Space, that is $\mathcal{F}_X$ is Borel $\sigma$-field. This should always be the case for our purposes. 
	

	\item Conditional Expectation\\

	Let us have the same probability space, measurable space, random element, and sub $\sigma$-field as defined before, but here with $\bar{\mathbb{R}}$.  \\

	We want to define conditional expectaiton of X given $\mathcal{G}$. \\

	First, assume $X \geq 0$. Consider a set function, 

			$$\nu: \mathcal{G} \rightarrow \mathbb{R}, G \mapsto \int_G X dP $$ 

	It can be easily shown that $\nu$ is a measure. 

	Let $\mu$ again be $\mathcal{G} \rightarrow \mathbb{R}, G \mapsto  P(G)$. Then $\nu << \mu$. By RNT, $\delta = \frac{d\nu}{d\mu}$ is well defined. This is defined to be conditional expectation of X given $\mathcal{G}$, written as 

			$$E(X|| \mathcal{G}) $$

	Suppose $X \ngeq 0$, but integrable P. Recall that $X = X^+ - X^-$. Since $X^+, X^- \geq 0$, then both $E(X^+|| \mathcal{G}), E(X^-|| \mathcal{G})$ are defined by RNT. We define, 

			$$E(X|| \mathcal{G}) = E(X^+|| \mathcal{G}) - E(X^-|| \mathcal{G}) $$

				 

\textbf{Friday January 13}

	As in the case of $P(A || \mathcal{G})$, the equivalent conditions for $d: \Omega \rightarrow \mathbb{R} $ is a version of $E(X||\mathcal{G})$. 

			\begin{enumerate}
				\item $\delta$ measurable $\mathcal{G}$
				\item $\int_G \delta dP = \int X dP \quad \forall G \in \mathcal{G}$ 
			\end{enumerate}

			INSERT PHOTO FROM BOARD - "Mesh"


	The value of $\delta$ in each thick outlined cell is the average (with respect to P measure ) of $X(\omega)$ over the subcells (thin outlined) in thick cells. 

	We see from this definition that if  $ A \in \mathcal{F}$, $X = I_A$ then the second condition becomes 

			$$\int_G \delta dP = \int_G I_A dP = P(A \bigcap G) $$

	So, $E(I_A||\mathcal{G}) = P(A || \mathcal{G})$. 

	\textbf{Properties of Conditional Expectations}\\

	\begin{theorem}{34.2 in Billingsly}
		Suppose that $X, Y, X_n$ are integrable P.\\

			\begin{enumerate}
				\item If X = a a.e. P, then $E(X || \mathcal{G})$ a.s. P\\

				\item If $a, b \in \mathbb{R}$ then

						$$E(aX + bY || \mathcal{G}) = a(E(X ||\mathcal{G})) + b(E(X || \mathcal{G})) a.s. P $$
				\item If $X \leq Y$ a.s. P then 

						$$E(X ||\mathcal{G}) \leq E(Y ||\mathcal{G}) $$
				\item $|E(X||\mathcal{G})| \leq E(|X| ||\mathcal{G})$ a.s. P (in fact this is true for all convex functions).
				\item If $X_n \rightarrow X$ a.s. P, $|X_n| \leq Y,$ and Y integrable P, then

						$$E(X_n ||\mathcal{G}) \rightarrow E(X ||\mathcal{G}) a.s. P $$
			\end{enumerate}

	\end{theorem}

	\begin{proof}
		Found in Billingsly. 
	\end{proof}

	\begin{theorem}[34.4 in Billingsly]
		If $\mathcal{G}_1 \subseteq \mathcal{G}_2 \subset \mathcal{F}$ and X integrable P, then

				$$E(E(X ||\mathcal{G}_2)|| \mathcal{G}_1) = E(X ||\mathcal{G}_1) $$

		This is called the Law of Iterative Conditional Expectation. 
	\end{theorem}

	\begin{theorem}[34.3 in Billingsly]
		
		If $X$ measurable $\mathcal{G}$, $Y \textcircled{m} \mathcal{F}$, then

				$$E(X Y||\mathcal{G}) = XE(Y ||\mathcal{G}) a.s. P $$
	\end{theorem}

	Other Properties

		\begin{enumerate}
			\item X, Y are random elements such that XY integrable P.
			\item If $\mathcal{G} \subseteq \mathcal{F}$ is the sub $\sigma$-field, then

					$$E(X E(Y||\mathcal{G})) = E(E(X ||\mathcal{G})Y) = E(E(X ||\mathcal{G}) E(Y ||\mathcal{G})) $$ 

			Conditional expectation is a self-adjoint operation. 

			\begin{proof}

			"Wire Theorem"\\

				$$\begin{aligned}
								E(X E(Y ||\mathcal{G}))	& = E( E(X E(Y ||\mathcal{G}) ||\mathcal{G}))\\
									&= E( E(Y ||\mathcal{G})E(X ||\mathcal{G}))	\\
									&= E( E(E(X ||\mathcal{G}) Y ||\mathcal{G})) \\
									&= 	E(E(X ||\mathcal{G}) Y)	
								\end{aligned}$$
			\end{proof}
		\end{enumerate}

	\item Conditional Distribution of a Random Element Given Another Random Element\\

		Here we have the typical probability space, measurable spaces for X and Y. 

		Let there be a function, 

			$h: \mathcal{F}_X x \Omega_Y \rightarrow \mathbb{R}$

		This function is called the conditional distribution of X given Y if 

				$$\tilde{h}(A, \omega) = h(A, Y(\omega)) $$

		We say that $\tilde{h}: \mathcal{F}_X x \Omega \rightarrow \mathbb{R}$ is the condiitional distribution of X given $\mathcal{G} = Y^{-1}(\mathcal{F}_Y$. 

		That is, 

				\begin{enumerate}
				 	\item For each $A \in \mathcal{F}_X$

				 			$$\tilde{h}(A, Y(\cdot)) = P(X^{-1}(A) || Y^{-1}(\mathcal{F}_Y)) $$
		 			\item  For each $\omega \in \Omega$

		 					$$\tilde{h}(\cdot, Y(\omega)) = P_{X|Y}(A | y) $$
				 \end{enumerate} 


	\item Conditional Density of One Random Element Given Another Random Element\\

		Suppose probability space and $\sigma$-finite measure spaces for X and Y. 

		Here our relevant function is

				$$g: \Omega_X x \Omega_Y $$

		which is the conditional density of X given Y if for all $A \in \mathcal{F}_X$, 

				$$\int_A g(x, y) d \mu_X(x) = P_{X|Y}(A|y) $$

		In the following special case, g ahs an explicit formula. 


		$(\Omega, \mathcal{F}, P)$\\
		$(\Omega_X, \mathcal{F}_X, \mu_X)$\\
		$(\Omega_Y, \mathcal{F}_Y, \mu_Y)$\\
		$(\Omega_X x \Omega_Y, \mathcal{F}_X x \mathcal{F}_Y, \mu_X x \mu_Y)$\\
		$(X, Y): \Omega \rightarrow \Omega_X x \Omega_Y \textcircled{m} \mathcal{F}\setminus \mathcal{F}_X x \mathcal{F}_Y$\\

		Let $P_X = P X^{-1}, P_Y = P Y^{-1}, P_{XY} = P (XY)^{-1}$. \\
		Assume $P_X << \mu_X, P_Y << \mu_Y, P_{XY} << \mu_X x \mu_Y$. 



				$$f_X = \frac{dP_X}{d\mu_X} $$
				$$f_Y = \frac{dP_Y}{d\mu_Y} $$ 
				$$f_{XY} = \frac{dP_{XY}}{d(\mu_X x \mu_Y) } $$ 

		Let

				$$f_{X|Y} = \begin{array}{ll}
				\frac{f_{XY}}{f_Y} & if f_Y \neq 0\\
				0 & f_Y = 0
					
				\end{array} $$

				$$f_{Y|X} = \begin{array}{ll}
				\frac{f_{XY}}{f_X} & if f_X \neq 0\\
				0 & f_X = 0
					
				\end{array} $$

		Then it is easy to show that each is indeed the conditional density of their respective elements (first given second). 
 
\textbf{Wednesday January 18}\\

Claim: $g(x, y)$ is the conditional density. 

\begin{proof}
	Want to show that for all $A \in \mathcal{F}_X$, 

			$$ \int_A g(x, y) d\mu_x(x) = P_{X|Y} (A|y) $$

	Which means that

			$$\int_A g(x, y(\omega)) d\mu_x(x) = P_{X|Y} (X^{-1}(A)|\sigma(y)) $$

	This is true if for all $G' \in \sigma(y)$

			$$\int_{G'} \int_A g(x, y(\omega)) d\mu_X (x) dP(\omega) = P(X^{-1}(A) \bigcap G') $$


	But note that 

				\begin{align*}
					G' &\in \sigma(y)\\
					\Leftrightarrow G' &\in Y^{-1}(\mathcal{F}_Y)\\
					G' &= Y^{-1}(G)\text{ for some } G\in \mathcal{F}_Y
				\end{align*}



	So we want to check that  

			$$\int_{Y^{-1}(G)} \int_A g(x, y(\omega)) d\mu_X (x) dP(\omega) = P(X^{-1}(A) \bigcap Y^{-1}(G)) $$

			\begin{align*}
				\int_{Y^{-1}(G)} \int_A g(x, y(\omega)) d\mu_X (x) dP(\omega) &= \int_{G} \int_A g(x, y) d\mu_X (x) dP_Y(y) \\
					&=\int_{G} \int_A \frac{f_{XY}(x, y)}{f_Y(y)} d\mu_X (x) [f_Y(y)] d\mu_Y(y) \\
					&=\int_{G} \int_A f_{XY}(x, y) d\mu_X (x) d\mu_Y(y)\\
					&=\int_{GxA} f_{XY}(x, y) d(\mu_X x \mu_Y)(x,y)\\
					&= P_{XY}(GXA) \\
					&= P \circ (X, Y)^{-1}(Ax G)\\
					&= P(X \in A, Y \in G)\\
					&= P(\omega: \omega \in X^{-1}(A) \& \omega \in Y^{-1}(G))\\
					&= P(X^{-1}(A) \bigcap Y^{-1}(G)) 
			\end{align*}
\end{proof}

\end{enumerate}
	
\section{Frequentist \& Bayesian Settings}\index{Frequentist \& Bayesian Settings}

We have our probability space $(\Omega, \mathcal{F}, P)$. We also have some data, 

		$$(\Omega_X, \mathcal{F}_X, \mu_X)$$ 
		$$ X: \Omega \rightarrow \Omega_X \textcircled{m} \mathcal{F}/ \mathcal{F}_X$$

Here, usually $\Omega_X$ is a $\mathbb{R}^m$.\\

Typically we have 

	$$X = (X_1, \dots, X_n)$$

and possibly, 

	$$ X_i = \begin{pmatrix}
		X_{i1}\\
		\vdots\\
		X_{ip}
	\end{pmatrix}$$

We could say that these data are independent and identically distributed (iid) random vectors of dimension p. In this case m = np. \\

The goal of statical inference is to estimate.

		$$P_X = PX^{-1} = P_0$$

The ?? distribution of X. \\

There are two schools of thought

	\begin{enumerate}
		\item Frequentist Approach - assume a family of distributions, $\mathcal{P}$, where $\mathcal{P} << \mu_x$. Usually we assume that $\mathcal{P}$ is a parametric family, $\mathcal{P} = \{P_\theta: \theta \in \Omega_\theta \subseteq \mathbb{R}^p \}$. We assume that $P_0 \in \mathcal{P}$, that is there exisgts $\theta_0 \in \Omega_\theta$ such that $P_\theta = P_0$. The goal is to estimate $P_0$. 
		\item Bayesian Approach - here we assume the data is generated by the conditional distribtuion $P_{X|\theta}$. We observe X, then determine what is the best estimate of the random $\theta.$
\end{enumerate}


\section{Prior Posterior \& Likelihood}


Here let there be probability space $(\Omega, \mathcal{F}, P)$; $\sigma$-finite measurable spaces $(\Omega_X, \mathcal{F}_X, \mu_X)$, $(\Omega_\theta, \mathcal{F}_\theta, \mu_\theta)$. Together, 

		$$(\Omega_X x \Omega_\theta, \mathcal{F}_X x \mathcal{F}_\theta, \mu_X x \mu_\theta)$$ 


Also, a random element, 

		$$(X, \theta):\Omega \rightarrow \Omega_X x \Omega_\theta \textcircled{m} \mathcal{F}\setminus\mathcal{F}_X x \mathcal{F}_\theta$$


$P_X = P \circ X^{-1} \leftarrow$ marginal distribution of X\\
$P_\theta = P \circ \theta^{-1} \leftarrow$ prior distribution \\
$P_{X, \theta} = P \circ (X, \theta)^{-1} \leftarrow$ joint distribution of X and $\theta$\\
$P_{X|\theta}(A|\theta): \mathcal{F}_X x \Omega_\theta \rightarrow \mathbb{R}$. Likeliehood distribution\\
$P_{\theta|X}(G|x): \mathcal{F}_\theta x \Omega_X \rightarrow \mathbb{R}$. Posterior distribution\\


Note in the following the first inequalities are \textbf{assumed}.\\

$P_X << \mu_X \Rightarrow f_X = \frac{d P_X}{d\mu_X}$ Marginal Density\\
$P_\theta << \mu_\theta \Rightarrow \pi_\theta = \frac{d P_\theta}{d\mu_\theta}$ Prior Density\\
$P_(X, \theta) << \mu_X x \mu_\theta \Rightarrow f_{X, \theta} (x, \theta) = \frac{d P_{X, \theta}}{d(\mu_x x \mu_\theta)}$ Joint Density\\


FINISH FROM PHOTO


One way to estimate $\theta$ is by maximizing $\pi_{\theta|X}(\theta|x)$. Want to do so with value that is most likely to happen (given the data). 

		$$\pi_{\theta|X} (\theta|x) = P_{\theta|X}(\theta = \theta | x) $$

By construction, 

		\begin{align*}
			\pi_{\theta|X} &= \frac{f_X\theta}{f_X}\\
				&= \frac{f_{X|\theta}\pi_\theta}{\int_{\Omega_\theta} f_{X|\theta} \pi_\theta d\mu_theta}
		\end{align*}


\section{Conditional Independence and Frquentist/Bayesian Sufficiency}\index{Conditional Independence and Frquentist/Bayesian Sufficiency}


\textbf{Independence}\\

Two random elements are said to be independent if for all $A' \in \sigma(X), G' \in \sigma(\theta)$ we have

		$$P(A' \bigcap G') = P(A')P(G') $$

This statement can also be expressed in $(\Omega_X x \Omega_\theta, \mathcal{F}_X x \mathcal{F}_\theta, P_X x P_\theta)$ as follows. \\

Since $A' \in \sigma(X) = X^{-1} (\mathcal{F}_X), A' = X^{-1}(A)$ for some $A \in \mathcal{F}_X$. So $G' = \Theta^{-1}(G). G \in \mathcal{F}_\Theta$. 

		\begin{align*}
			P(A' \bigcap G') &= P(X^{-1}(A) \bigcap \Theta^{-1}(G) )\\
				&=P(\{ \omega: \omega \in  X^{-1}(A) \bigcap \Theta^{-1}(G)\} )\\
				&=P(\{ \omega: \omega \in  X^{-1}(A) \& \omega \in \Theta^{-1}(G)\} )\\
				&=P(\{ \omega: X(\omega) \in A, \Theta(\omega) \in G\} )\\
				&=P(\{ \omega:( X(\omega),  \Theta(\omega)) \in A x G\} )\\
				&=P(\{ \omega:( X, \Theta)(\omega) \in A x G\} )\\
				&=P(\{ \omega:\omega \in( X, \Theta)^{-1} A x G\} )\\
				&=[P \circ ( X, \Theta)^{-1} ](A x G)\\
				&=P_{X, \Theta}(A x G)
		\end{align*}

Also note that

		$$P(A') = P(X^{-1}(A)) = P_X(A) $$
		$$P(G') = P_\Theta (G) $$


So with independence, (and for $A \in \mathcal{F}_X, G \in \mathcal{F}_\Theta$)

		$$P_{X, \Theta}(A x G) = P_X (A) P_\Theta(G) $$

But we know that this implies that $P_{X, \Theta}$ is the product measure $P_X x P_\Theta$. 

\textbf{Conditional Independence}\\

Now, given sub $\sigma$-field $\mathcal{G} \in \mathcal{F}$ we want to define $X \& \Theta$ conditionally independent given $\mathcal{G}$. 

\begin{definition}
	We say that $X \& \Theta$ are conditionally independent given $\mathcal{G}$ (i.e. $X \indep \Theta | \mathcal{G}$) if for all $A' \in \sigma(X), G' \in \sigma(\Theta)$ we have 

			$$P[A' \cap G' || \mathcal{G}] = P[A' || \mathcal{G}]P[G' || \mathcal{G}] a.s. P $$

	Equivalently for all $A \in \mathcal{F}_X, G \in \mathcal{F}_\Theta$, 

			$$P[X^{-1}(A) \cap \Theta^{-1}(G) || \mathcal{G}] =P[X^{-1}(A) || \mathcal{G}]P[\Theta^{-1}(G) || \mathcal{G}]  $$

	Equivalently, 

			$$P_{X, \Theta | \mathcal{G}} (A x G | \mathcal{G}) = P_{X|\mathcal{G}}(A | \mathcal{G}) P_{\Theta|\mathcal{G}} (G | \mathcal{G}) $$
\end{definition}


\textbf{Equivalent Condition for Conditional Independence}\\

\begin{theorem}[1.1 in Notes]
	The following statements are equivalent. 

	\begin{enumerate}
		\item $X \indep \Theta | \mathcal{G}$
		\item $P(X^{-1}(A) || \Theta, \mathcal{G}) = P(X^{-1}(A)| \mathcal{G}) a.s. P \quad \forall A \in \sigma(X)$
		\item $P(\Theta^{-1}(G) || X, \mathcal{G}) = P(\Theta^{-1}(G)|| \mathcal{G}) a.s. P \quad \forall G \in \sigma(\Theta)$
	\end{enumerate}
\end{theorem}

\begin{proof}
	It suffies to proof that $1 \Leftrightarrow 2$.\\

	$1 \Rightarrow 2$. We know that for all $A \in \mathcal{F}_X, G \in \mathcal{F}_\Theta$ that

			$$ P[X^{-1}(A) \cap \Theta^{-1}(G) || \mathcal{G}] =P[X^{-1}(A) || \mathcal{G}]P[\Theta^{-1}(G) || \mathcal{G}]$$

	Want that for all $A \in \mathcal{F}_X$ that $P(X^{-1}(A) || \Theta, \mathcal{G}) = P(X^{-1}(A)| \mathcal{G})$. 

	\begin{align*}
		P(X^{-1}(A) || \Theta, \mathcal{G}) &\equiv P\left(X^{-1}(A)|| \sigma(\sigma(\Theta) \cup \mathcal{G})\right) \\
			&= P(\dots || \sigma(\Theta^{-1}(\mathcal{F}_\Theta) \cup \mathcal{G}))
	\end{align*}

	So it suffices to show that 

			$$P(X^{-1}(A) || \sigma(\Theta^{-1}(\mathcal{F}_\Theta) \cup \mathcal{G})) = P(X^{-1}(A) || \mathcal{G})$$

	From the definition given we want to show that the above statement is true. which is so that the for all $B \in \sigma(\Theta^{-1}(\mathcal{F}_\Theta) \cup \mathcal{G} )$, 

			$$\int_B P(X^{-1}(A) || \mathcal{G}) dP = P(X^{-1} (A) \cap B) $$

	But this is very hard because B is hard to characterize. But we have theorem that says you only have to check (*) for all B in a $\pi$-system generating $\sigma(\Theta^{-1}(\mathcal{F}_\Theta)\cup \mathcal{G})$.

			$$\mathcal{P} = \{\Theta^{-1} (G) \cap F: G \in \mathcal{F}_\Theta, F \in \mathcal{G} \} $$

	It is trivial to show that $\mathcal{P}$ is a $\pi$-system. 

	MORE IN PHOTO

	Meanwhile, 

			$$\mathcal{P} \subseteq \sigma(\Theta^{-1} (\mathcal{F}_\Theta) \cup \mathcal{G}) $$

	Therefore, 

			$$\sigma(\Theta^{-1}(\mathcal{F}_\Theta)\cup \mathcal{G}) = \sigma(\mathcal{P}) $$

	So, sufficent to check (*) $\forall B \in \mathcal{P}'$

			$$B \in \mathcal{P} \Rightarrow B = \Theta^{-1}(G) \cap F, G \in \mathcal{F}_\Theta, F \in \mathcal{G} $$

	So, we want

			$$\int_{\Theta^{-1}(G)\cap F} P(X^{-1}(A) || \mathcal{G}) dP = P(\Theta^{-1}(G) \cap F \cap X^{-1}(A)) $$


			\begin{align*}
				\int_{\Theta^{-1}(G)\cap F} P(X^{-1}(A) || \mathcal{G}) dP &= \int_{\Theta^{-1}(G) \cap F} E \left(I_{X^{-1}(A)} || \mathcal{G} \right) dP\\
						&= E\left(I_{\Theta^{-1}(G)} I_F E(I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( E(I_{\Theta^{-1}(G)} I_F || \mathcal{G}) E(I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( I_F E(I_{\Theta^{-1}(G)}  || \mathcal{G}) E(I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( I_F E(I_{\Theta^{-1}(G)} I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left(  E(I_F I_{\Theta^{-1}(G)} I_{X^{-1}(A)} || \mathcal{G})\right)\\
						&= E\left( I_F I_{\Theta^{-1}(G)} I_{X^{-1}(A)}\right)\\
						&= P(F \cap \Theta^{-1}(G) \cap X^{-1}(A))
			\end{align*}

\textbf{Monday January 23}\\

	$2 \Rightarrow 1$. We want to show that 

			$$P(X^{-1}(A)|| \mathcal{G}) P(\Theta^{-1}(G) || \mathcal{G}) $$

		is conditional probability of 

			$$P(X^{-1}(A) \cap \Theta^{-1}(G)|| \mathcal{G}) $$

		for all $F \in \mathcal{G}$. 
		
				\begin{align*}
						\int_F P(X^{-1}(A)|| \mathcal{G})P(\Theta^{-1}(G) || \mathcal{G}) dP &= E\left[I_F E\left(I_{X^{-1}(A)} || \mathcal{G} \right) E\left(I_{\Theta^{-1}(G) }|| \mathcal{G} \right) \right]\\
								&= E\left[ E\left(I_{X^{-1}(A)} || \mathcal{G} \right) E\left(I_F I_{\Theta^{-1}(G) }|| \mathcal{G} \right) \right]\\
								&= E\left[ E\left(I_{X^{-1}(A)} || \mathcal{G} \right) I_F I_{\Theta^{-1}(G)} \right]\\
								&= E\left[ E\left(I_{X^{-1}(A)} I_F I_{\Theta^{-1}(G)}|| \Theta, \mathcal{G} \right)  \right]\\
								&= E\left[I_{X^{-1}(A)} I_F I_{\Theta^{-1}(G)} \right]\\
								&= P(X^{-1}(A) \cap \Theta^{-1}(G) \cap F)
					\end{align*}	


\end{proof}

\section{Equivalence of Frequentist \& Bayesian Sufficiency}\index{Equivalence of Frequentist \& Bayesian Sufficiency}


Here we have, \\

$(\Omega_\Theta, \mathcal{F}_\Theta, \mu_\Theta), (\Omega_X, \mathcal{F}_X, \mu_X),(\Omega_T, \mathcal{F}_T)$\\

Where 

		$$T: \Omega_X \rightarrow \Omega_T \textcircled{m} \mathcal{F}_X / \mathcal{F}_T $$

is called a statistic. 

		$$T = T(X) \text{ or } T \circ X = T(X(\omega)) $$

In fewquentist setting, we say that T is \textbf{suffienct} if $P_{X|T, \Theta}$ does not depend on $\Theta$. It can be easily verified (see Homework) that $P_{X|T, \Theta}$ doesn't depend on $\Theta$ implies that

		$$P_{X|T, \Theta} = P_{X|T} \text{ a.s. P} $$

This is "nearly" fequentist. Above is exchangeable with "$X \indep \Theta| T$", but can't say this in frequentist setting. 


		$$P_{\Theta | T, X}  =  P_{\Theta|T} \Leftrightarrow P_{\Theta|X} = P_{\Theta|T}$$

That is to say that a statistic, T, is sufficient for $\Theta$ if and only iff the posterir distribution of $\Theta|X$ is the same as the posterior distribution of $\Theta|T$. This would be used in a Bayesian setting. 

\begin{definition}[Bayesian Sufficient]
	We say that $T \circ X$ is \textbf{Bayesian sufficient} if 

			$$P_{\Theta | X}  =  P_{\Theta|T} \text{ a.s. P } $$
\end{definition}


\textbf{Lemma 1.1} (HW 2) Suppose that $f(\theta)$ is a p.d.f such that

		$$f(\theta) \propto exp\{-a\theta^2 + b\theta \}, \quad a > 0 $$

		Then, 

		\begin{enumerate}
			\item $\theta \sim N( \frac{b}{2a}, \frac{1}{2a})$
			\item $\int exp\{-a\theta^2 + b\theta \} d\theta =\sqrt{\frac{\pi}{a}} exp\{\frac{b^2}{4a} \} $
		\end{enumerate}


\begin{example}
	Suppose that

			$$X | \Theta \sim N(\Theta, \sigma^2) $$
			$$\Theta \sim N(\mu, \tau^2)$$

	Find $\pi_{\Theta|X} (\theta|x), f_X(x)$. 

	\textbf{Solution:}\\

	\begin{align*}
		\pi(\theta|x) &\propto f(x|\theta) \pi(\theta)\\
				&= \frac{1}{\sqrt{2 \pi \sigma^2}} exp\{-\frac{1}{2} \frac{(x-\theta)^2}{\sigma^2}\} * \frac{1}{\sqrt{2 \pi \tau^2}} exp\{-\frac{1}{2} \frac{(\theta -\mu)^2}{\tau^2}\}\\
				&\propto  exp\{-\frac{1}{2} \frac{(x-\theta)^2}{\sigma^2}\} *  exp\{-\frac{1}{2} \frac{(\theta -\mu)^2}{\tau^2}\}\\
				&= exp\{-(\frac{1}{2\sigma^2} + \frac{1}{2 \tau^2})\theta^2 + (\frac{x}{\sigma^2} + \frac{\theta}{\tau^2})\theta\}
	\end{align*}

Using  Lemma 1.1, 

		$$\theta|X \sim N\left(\frac{\frac{x}{\sigma^2} + \frac{\mu}{\tau^2}}{1/2(2\sigma^{-2} + 1/2\tau^{-1} )}, \frac{1}{\frac{1}{\sigma^2} + \frac{1}{\tau^2}}\right) $$

How about $f_X(x)$?\\

		\begin{align*}
			f_X(x) &= \int f(x|\theta) \pi(\theta) d\theta\\
					&\vdots\\
					&= \frac{1}{{2 \pi \sigma \tau}} * exp\{-\frac{x^2}{2\sigma^2} -  \frac{\mu^2}{2\tau^2}\} \int exp\{- (\frac{1}{2\sigma^2} + \frac{1}{2 \tau^2})\theta^2 + (\frac{x}{\sigma^2} + \frac{\mu}{\tau^2})\theta \}\\
					&= \dots * \sqrt{\frac{\pi}{\frac{1}{2\sigma^2} + \frac{1}{2 \tau^2}}} exp\{\frac{(\frac{x}{\sigma^2} + \frac{\mu}{\tau^2})^2}{4 (\frac{1}{2\sigma^2} + \frac{1}{2\tau^2})}\}
		\end{align*}

		We want to identify this as a p.d.f of x, so we can treat anything that is not x as a constant. Using elementary algebra we get...

		\begin{align*}
			&\propto exp \{- (\frac{x^2}{2(\tau^2 + \sigma^2)} + \frac{x \mu}{(\sigma^2 + \tau^2)}) \}
		\end{align*}

		Applying Lemma 1.1 for x and simplifying, 

				$$X \sim N(\mu, \tau^2 + \sigma^2) $$


\end{example}

This can be extended to multivariate setting, 2-sample setting, ANOVA setting, regression setting, etc. It is essential to all aspects of linear models. 



\textbf{Wednesday January 25}\\

\begin{example}
	Suppose

			$$X_1, \dots, X_n | \theta \stackrel{iid}{\sim} N(\theta, \sigma^2) $$
			$$\theta \sim N(\mu, \tau^2) $$

	$\pi(\theta| X_1, \dots, X_n) =$?\\


	By Example 1.1, 

			$$\theta | \bar{X} \sim N\left(\frac{\frac{\bar{X}}{\sigma^2/n} + \frac{\mu}{\tau^2}}{\frac{1}{\sigma^{2}/n} + \frac{1}{\tau^{2}})}, \frac{1}{\frac{1}{\sigma^2/n} + \frac{1}{\tau^2}} \right) $$


	Note that the sample size will effect how much weight each of the pervious means is given. As $n \rightarrow \infty$, depending on which coeffecient goes to 1, 

			$$E(\theta | \underline{X}) \rightarrow \bar{X} \text{ or } \mu $$

	Similarly with variance, as $n \rightarrow \infty$, 

			$$Var(\theta| \underline{X}) \rightarrow 0 $$

	We can generalize/approximate this phenomenon as follows, 

			$$\theta|X_1, \dots, X_n \sim N(\hat{\theta_1}, I^{-1}(\hat{\theta})) $$

	where $\hat{\theta}$ is the MLE. \\

	In our special case, 

			$$\theta| X_1, \dots, X_n \sim N(\bar{X}, \frac{\sigma^2}{n}) $$


\end{example}


	\section{Conjugate Priors}\index{Conjugate Priors}

	\subsection{Introduction}\index{Introduction}

	In general, computing posterior distributions or posterior means is a hard problem involving high-dimentional numerical integration. This was a big hurdle for Bayesian methods before computers. Now we can do methods such as Monte Carlo inegration (MCMC).\\

	In special cases, such as with the Exponential Family and mixture of distributions, posterior can be expressed explicitly through the use of conjugate families. \\

	\begin{definition}[Conjugate Family]
			A family of distributions, $\mathcal{P}$, on ($\Omega_\theta, \mathcal{F}_\theta$) is a \textbf{conjugate family} if 

					$$P_\Theta \in \mathcal{P} \Rightarrow \mathcal{P}_{\Theta|X} \in \mathcal{P} $$

	
		\end{definition}

Not unique, for example if you let $\mathcal{P}$ be the colleciton of all distributions on ($\Omega_\theta, \mathcal{F}_\theta$), then then it is always conjugate. Usually there is a suitable conjugate family. 

\subsection{Exponential Family}

\begin{example}
	$X_1, \dots, X_n |\theta \sim Pois(\theta)$\\

			\begin{align*}
				f(x_1, \dots, x_n | \theta) &= \prod^n_{i=1} \frac{\theta^{x_i}}{x_i!} e^{-\theta} \\
						&= \frac{\theta^{\sum x_i} e^{-n\theta}}{\prod(x_i!)}\\
						&\propto \theta^{c_1} e^{-c_2 \theta}
			\end{align*}

	Note that we are treating $\theta$ as the variable of interest here, not x. Recall that if $\theta \sim Gamma(\alpha, \beta)$ then we have a distribution approaching the form above, that is, 

			$$\pi(\theta) \propto \theta^{\alpha - 1} e^{-\theta/\beta}$$

	If we use this form, then we may find $pi(\theta|\underline{X})$ using the following:

			$$pi(\theta|\underline{X}) \propto \theta^{c_1 + \alpha -1} e^{-(c_2 + 1/\beta)\theta} $$

	Which gives us that

			$$\theta| X_1, \dots, X_n \sim Gamma(c_1 + \alpha, (c_2 + 1/\beta)^{-1}) $$

	This is generally true for all exponential family distributions. 


\end{example}

We say that X has exponential family distrubiton if is has p.d.f in the form of 

		$$\frac{e^{\theta^T t(x)}}{\int e^{\theta^T t(x) d\mu(x)}} $$

where $\mu$ is a $\sigma$-finite measure on $(\Omega_X, \mathcal{F}_X)$. \\

Essentially, p.d.f. with $\mu(x) \propto e^{\theta^T t(x)}$.\\

More generally, suppose that $\phi: \Theta \mapsto \Phi$ bijection (one-to-one onto). Then X has Exponential Family distribution if and only if the p.d.f of X with $\mu$ is 

		$$ \frac{e^{\phi^T t(x)}}{\int e^{\phi^T t(x) d\mu(x)}}$$

In this case, $X \sim Exp(\phi, t, \mu)$, when $\phi$ is identity, this is called the canonical form of exponential family. \\

\begin{theorem}[1.3 from class]
	If 
		$$P_\Theta \sim Ep(\xi, \phi, \nu )$$
		$$P_{X|\Theta} \sim Ex(\phi, t, \mu)$$

	then 

			$$P_{\Theta|X} \in Ep(\xi_X, \phi, \nu_x) $$

	where

			$$\xi_X(\alpha) = \xi(\alpha) + t(x)$$

			$$d\nu_X(\theta) = \frac{d \nu(\theta)}{\int e^{\phi^T(\theta) t(x)} d\mu(x)} $$
\end{theorem}

\begin{proof}
	$P_\Theta \in Ep(\xi, \phi, nu) \Rightarrow \pi(\theta) = \frac{e^{\xi^T(\alpha) t(\theta)}}{\int e^{\xi^T(\alpha) t(\theta) d\nu(x)}}$\\

	\begin{align*}
		f(x|\theta) &= \frac{e^{\phi^T t(x)}}{\int e^{\phi^T t(x) d\mu(x)}}\\
		\pi(\theta|x) &= \text{PHOTO}\\
				&=  
	\end{align*}

GIANT fraciton of fractions = PHOTO

So, with respect to the new measure, 
	
	$$d\nu_X(\theta) = \frac{d\nu(\theta)}{\int e^{\phi^T(\theta) t(x)}d\mu(x)} $$

where the pdf is 

		$$\frac{}{} $$

So, $\theta|X \sim Ep(\xi(\alpha) + t(x), \phi(\theta), \frac{d\nu(\theta)}{\int e^{\phi^T (\theta) t(x) d\mu(x)}})$

\end{proof}

\textbf{Friday January 27}\\

\begin{example}[One Parameter Normal]
			$$X|\theta \sim N(\theta, \sigma^2) $$

	Want to assign conjugate prior for $\theta$. 

			$$f(x|\theta) \propto exp\{(\frac{-1}{2\sigma^2})\theta^2 + (\frac{\mu}{\sigma^2})\theta\} $$


	Recall Lemma 1.1. 

			$$ f(\theta) \propto exp\{(\frac{-1}{2\sigma^2})\theta^2 + (\frac{\mu}{\sigma^2})\theta\} $$

			$$\Rightarrow \theta \sim N(\mu, \sigma^2) $$

	So, consider the following family, 

			$$\mathcal{F} = \{f(\theta) \propto exp\left\{\frac{-1}{2\alpha_2^2}\theta^2 + \frac{\alpha_1}{\alpha_2^2}\theta\} \right\} $$

	Suppose that $\pi \in \mathcal{F}$. Then we have that

			\begin{align*}
				\pi(\theta|X) &\propto f(x|\theta) \pi(\theta)\\
					&\propto exp\{(\frac{-1}{2\sigma^2})\theta^2 + (\frac{\mu}{\sigma^2})\theta\} *  exp\left\{\frac{-1}{2\alpha_2^2}\theta^2 + \frac{\alpha_1}{\alpha_2^2}\theta\} \right\}\\
					&= exp\left\{(\frac{-1}{2\alpha_2^2} - \frac{1}{2\alpha_2})\theta^2 + (\frac{x}{\sigma^2} + \frac{\alpha_1}{\alpha_2^2})\theta\} \right\} \\
					&= - \frac{1}{2}(\frac{1}{\sigma^2} + \frac{1}{\alpha^2})
			\end{align*}

	So we have that 

		$$\theta|X \sim N(\frac{\frac{\alpha_1}{\alpha_2} + \frac{x}{\sigma_2}}{(\frac{1}{\sigma^2} + \frac{1}{\alpha^2})}, \frac{1}{\frac{1}{\sigma^2} + \frac{1}{\alpha^2}}) $$

\end{example}
	\subsection{Convex Hull of Conjugate Family}\index{Convex Hull of Conjugate Family}

	\begin{remark}

	Mathematically,  if $A \subseteq \mathbb{R}^k$,
		
		$$Conv(A) = \left\{\alpha_1 s_1 + \dots + \alpha_k s_k: \alpha_1 \geq 0, \dots, \alpha_k \geq 0; \alpha_1 + \dots + \alpha_k = 1; s_1 \in A, \dots, s_k \in A \right\} $$
	\end{remark}

\begin{remark}
					By the way, a convex combination of a set of probability measures, say $P_1, \dots, P_k$, is called the \textbf{mixture} of $P_1, \dots, P_k$. So $Conv(\mathcal{P})$ is simply the colleciton of all mixture distributions derived from $\mathcal{P}$. 
				\end{remark}

	\begin{theorem}
		If $\mathcal{P}$ is conjugate to $P_{X|\theta}$, then $Conv(\mathcal{P})$ is also conjugate. 


	\end{theorem}

	\begin{proof}
		Suppose that $P_\Theta \in Conv(\mathcal{P})$. Want to show that $P_{\Theta|X} \in Conv(\mathcal{P})$. 

		Sicne $P_\Theta \in Conv(\mathcal{P})$ there exists

				$$\alpha_1, \dots, \alpha_k \in \mathbb{R}, \sum^k \alpha_i = 1, \alpha_i \geq 0 $$

				$$P_\Theta^{(1)} \dots  P_\Theta^{(k)} \in \mathcal{P}$$

		such that

				$$P_\Theta = \sum^k \alpha_i P_\Theta^{(i)} $$

				\begin{align*}
					f_X(x) &= \int f(x|\theta) dP_\Theta\\
							&= \int f(x|\theta) d(\sum^k \alpha_i P_\Theta^{(i)})\\
							&= \sum^k \alpha_i \int f(x|\theta) dP_\Theta^{(i)}\\
							&=\sum^k \alpha_i \int m_i(x) dP_\Theta^{(i)}\\
						dP_\Theta(\cdot|x)	&= \frac{f(x|\theta)}{f_X(x)} dP_\Theta\\
						&=\frac{f(x|\theta)}{\sum^k \alpha_i m_i(x)} \sum^k \alpha_k d P_\Theta^{(i)}\\
						&=\sum^k \frac{\alpha_i m_i(x)}{\sum^k \alpha_j m_j(x)} \frac{f(x|\theta)dP_\Theta^{(i)}}{m_i(x)}
				\end{align*}

				Note that the first term is great than zero and sums to 1. It's our new $\alpha^*$. In the second term note that $P_\Theta^{(i)} \in \mathcal{P}$ and $P_{\Theta|X}^{(i)}(\cdot|x) \in \mathcal{P}$. 

				Thus this is a convex combination of members of $\mathcal{P}$ in $Conv(\mathcal{P})$.  

				
	\end{proof}




This is a nice way to construct conjugate families of mixtures of exponential family. 

\section{Two-Parameter,  Normal Family}

\begin{definition}[Inverse $\chi^2$ Disribution]
	A random variable T is said to have an inverse $\chi^2$ distribution of $\chi^{-2}_{(k)}$ if and only if, 

		$$\frac{1}{T} \sim \chi^2_{(k)} $$

	If, for some $\tau < 0$, $\frac{T}{\tau} \sim \chi^{-2}_{(k)}$ then we write that 

			$$T \sim \tau \chi^{-2}_{(k)} $$

	
\end{definition}

By Jacobian theorem, 

			$$T = g(X) $$

			$$f_T(t) = f_X(g^{-1}(t)) |det(\frac{\partial g^{-1}(t)}{\partial t})| $$

Above will be shown in HW. 

\begin{theorem}
	If $T \sim \chi^{-2}_{(\nu)}$ then, 

			$$f(t) = \frac{1}{\Gamma(\frac{\nu}{2}) 2^{\frac{\nu}{2}}} t^{-\frac{\nu}{2}-1} e^{\frac{-1}{t}}, t > 0 $$
\end{theorem}

\begin{proof}
	Shown in HW 2. 
\end{proof}

Now suppose that $X_1, \dots, X_n | \phi, \lambda \stackrel{iid}{\sim} N(\lambda, \phi)$ where both $\lambda$ and $\phi$ are random. How do we assign conjugate prior of ($\lambda, \phi$)?\\

From classical statistics (Stat 514) we know that the sufficience statistic for $\lambda, \phi$  is $(\sum X_i, \sum X_i^2)$ or $(T = \bar{X}, S = \sum^n_{i=1} (X_i -\bar{X})^2$. 


So $\pi(\lambda, \phi|X_1, \dots, X_n) = \pi(\lambda,\phi | T, S)$. Moreover, form Normal theory, 

		$$T \indep X | \lambda, \phi $$
		$$T | \lambda, \phi \sim N(\lambda, \phi/n) $$
		$$S | \lambda, \phi \sim \phi \chi^2_{(n-1)} $$


\textbf{Monday January 30}\\


So, we need only to look at [GET NOTES FROM BUDDY]


	\begin{align*}
		f(T, S |\lambda, \phi) &= f(S|\lambda, \phi) f(T|\lambda,\phi)\\
			&=f(S|\phi) f(T|\lambda, \phi)\\
			&=[S|\phi] [T|\lambda, \phi]\\
			&= \left[\phi^{-(\frac{n-1}{2} - 1)} exp\left\{(\frac{-s}{2\phi})(\frac{1}{\phi})\right\}\right] * \left[\frac{1}{\sqrt{2\pi} (\phi/n)^{1/2}} exp\left\{\frac{-1}{2(\phi/n)}\lambda^2 + \frac{t}{\phi/n}t + (\frac{-1}{2(\phi/n)})t^2\right \} \right]
	\end{align*}

	So we have $N(t, \frac{\phi}{n}) s\chi^{-2}_{(n-3)})$. 

	More generally, the distribution, 

			$$N(a, \frac{\phi}{m}) * \tau \chi^{-2}_{(k)} $$

	is referred to (by Bing Li) as NICH (Normal-Inverse CHi-square). Here, 

			$$NICH(a, m, \tau, k)$$


	We may say that $(\lambda, \phi) \mapsto [S|\phi][T|\lambda, \phi]) \sim NICH(t,n,s, n-3)$. The likelihood is NICH. 


\textbf{Lemma 1.2} 

		$$NICH(a_1, m_1, \tau_1, k_1) * NICH(a_2, m_2, \tau_2, k_2) = NICH(a_3, m_3, \tau_3, k_3)$$


		Where we have that\\
\\
		$a_3 = \frac{m_1a_1 + m_2a_2}{m_1 + m_2}$\\
		$m_3 = m_1 + m_2$\\
		$\tau_3 = \tau_1 + \tau_2 + m_1a_1^2 + m_2a_2^2 - m_3a_3^2$\\
		$k_3 = k_1 + k_2 -3$\\


Before proof, let's rewrite likelihood as

		$$NICH(a, m, \tau, k) \propto  \phi^{-1/2}exp\{\frac{-1}{2(\phi/m)}\lambda^2 + \frac{a}{\phi/m}\lambda\} \phi^{-(k/2) - 1} exp\{-\frac{\tau + ma^2}{2\phi}\}$$

\begin{proof}
	By definition 

			\begin{align*}
				NICH_1 * NICH_2 &\propto \phi^{-1/2}exp\{\frac{-1}{2(\phi/m_1)}\lambda^2 + \frac{a_1}{\phi/m_1}\lambda\} \phi^{-(k_1/2) - 1} exp\{-\frac{\tau_1 + m_1a_1^2}{2\phi}\} * \phi^{-1/2}exp\{\frac{-1}{2(\phi/m_2)}\lambda^2 + \frac{a_2}{\phi/m_2}\lambda\} \phi^{-(k_2/2) - 1} exp\{-\frac{\tau_2 + m_2a_2^2}{2\phi}\}\\
					&= \phi^{-1/2}exp\{(\frac{-1}{2(\phi/m_1)} - \frac{1}{2(\phi/m_2)})\lambda^2 + (\frac{a_1}{\phi/m_1} + \frac{a_2}{\phi / m_2})\lambda\} \phi^{-(k_1/2) - 1} \phi^{-(k_2/2) - 1} exp\{-\frac{\tau_1 + m_1a_1^2}{2\phi} - \frac{\tau_2 + m_2a_2^2}{2\phi}\}\\
					&= \phi^{-1/2}exp\{(\frac{-1}{2(\phi/m_3)} )\lambda^2 + (\frac{a_3}{\phi/m_3})\lambda\} \phi^{-(k_3/2) - 1}  exp\{-\frac{\tau_3 + m_3a_3^2}{2\phi} \}
			\end{align*}



\end{proof}


Recall we have that $(\lambda, \phi) \mapsto [S|\phi][T|\lambda, \phi]) \sim NICH(t,n,s, n-3)$. By Lemma 1.2, we assign prior

		$$[\lambda| \phi][\phi] \sim NICH(a,m,\tau, k) $$

This gives us that 

		$$\pi(\lambda, \phi | T, S) \sim NICH(a^*, m^*, \tau^*, k^*) $$


where we have that \\
\\

$a^* = \frac{m a + n T}{m + n}$\\
$m^* = m + n$\\
$\tau^* = \tau + S + m a^2 + nt^2 - (m+n)(\frac{ma + nt}{m+n})^2$\\
$k^* = k + n -3 + 3$\\


So, $[\lambda | \phi, X] \sim N(a^*, \frac{\phi}{m^*})$, and $[\phi|X] \sim \tau^* \chi^{-2}_{k^*}$. 


Note that we can make inference about $\lambda, \phi$ using 

		$$\frac{\lambda - a^*}{\sqrt{\phi/m^*}} | \phi, X \sim N(0, 1) $$

since the RHS doesn't depend on $\phi$ we have that it is independence form $\phi|X$. 

		$$\frac{\lambda - a^*}{\sqrt{\phi/m^*}} | X \sim N(0, 1) $$

\textbf{Wednesday February 1}\\

We want to make inference about 

		$$\frac{\frac{\lambda - a^*}{\sqrt{\phi/m^*}}}{\sqrt{\frac{\tau^*}{\phi}/m^*}} | X \sim t_{m^*} $$

We may reorganize this to be 


		$$\frac{m^*(\lambda - a^*)}{\sqrt{\tau^*/m^*}}\sim t_{m^*} $$


Similarly, for making inference about $\phi$, 

		$$\phi | X \sim \tau^* \chi^{-2}_{m^*} $$

		$$\frac{\tau^*}{\phi}|X \sim \chi^2_{m^*} $$


\section{Noninformative Prior}\index{Noninformative Prior}

\subsection{General Concept}

Even when you don't have a prior distribution it's still beneficial to use Bayesian setting. For example, in dealing with nuisance parameters we deal with high dimension prior. \\

So here, try to use Bayesian to solve "no prior information" proglem. We want to use a flat prior, e.g. the Lebesgue measure. But, in what is this flat? Supppose we impose the Lebesgue measure on $\theta$. Then the prior for monotone transformation of $\theta$, say $\theta^3$, is not Lebesgue anymore. 


\begin{definition}[Improper Prior]
	An infinite, but $\sigma$-finite measure on $\Omega_\Theta$ is called an \textbf{improper prior}.
\end{definition}

\begin{example}
			$$ X|\theta \sim N(\theta, \phi) $$

	Note that here $\phi$ is known. 

	$$\pi(\theta) \equiv 1 $$
	$$f(X|\theta) = \sqrt{2\pi}^{-1} e^{-1/2 * (x - \theta)^2} $$

	$$\pi(\theta|X) = \sqrt{2\pi}^{-1} e^{-1/2 * (x - \theta)^2} * 1  = N(X, 1)$$

	$$f_X(x) = \frac{f(x|\theta) \pi(\theta)}{\pi(\theta|X)} = \frac{f(x|\theta)}{\pi(\theta|x)} = 1$$

	The marginal is Lebesgue improper! But, posterior is important
\end{example}


\begin{example}
	$$X_1, \dots, X_n|\lambda, \phi \sim N(\lambda, \phi) $$

	Here both parameters are random. 

	We have sufficient statistics, $(S = \bar{X}, T= \sum(X_i - \bar{X})^2)$. \\

	\begin{align*}
		[(\lambda, \phi) \mapsto f(t, s| \lambda, \phi)] &\sim NICH(t, n, s, n-3)\\
			&\propto \phi^{-1/2} exp\left\{\frac{-1}{2(\phi/n)}\lambda^2 + \frac{t}{\phi/n}\lambda\right\} \phi^{-((n-3)/2) - 1} exp\{-\frac{s+ nt^2}{2\phi}\}
	\end{align*}\\

	$$[\lambda|\phi] = 1 $$
	$$[\phi] = \frac{1}{\phi} $$

	When we multiply the p.d.f of our NICH by $\frac{1}{\phi}$ we get NICH(t, n, s, n-1) following the same argument in Seciton 1.5 (?) (proper case).


	We can show that 

			$$\frac{\sqrt{n}(\lambda - t(x))}{\sqrt{s(x)/(n-1)}}|X \sim t_{(n-1)} $$

			$$\frac{s(x)}{\phi}|X \sim \chi^2_{(n-1)} $$

	Exactly the same as frequentist sample distribution, except what random has changed. 

\end{example}

\subsection{Invariant Prior} % (fold)
\label{sub:invariant_prior}

What is flat? What is a natural generalization of Lebesgue Measure? \\

Lebesgue measure is invariant under translation. 

		$$\theta \mapsto \theta + c $$

If $\lambda$ is Lebesgue and T is translation, 

		$$\lambda \cdot T^{-1} = \lambda $$

This is natural generalization of flatness. Change T to be some other transformation that some how resembles translation = group of transformation. \\

\begin{remark}
Review definition of group of transformations. 

$\Omega$ (set)\\

$\mathcal{G}$ is a set of bijections (one-to-one on to) on $\Omega$ such that 

\begin{enumerate}
	\item for all $g_1, g_2 \in \mathcal{G}, g_1 \circ g_2 \in \mathcal{G}$
	\item for all $e \in \mathcal{G}$ such that $e \circ g = g \circ e = g$ for all $g \in \mathcal{G}$
	\item $g \in \mathcal{G} \Rightarrow g^{-1} \in \mathcal{G}$
\end{enumerate}	
\end{remark}

With our own problem, 

$$\Omega_\Theta \subseteq \mathbb{R}^P $$

Consider a parametric group, 

		$$\{g_t : t \in \Omega_\theta \} $$

Consider two types of transformations, 

		$$\theta \mapsto g_t(\theta) = L_t$$
		$$\theta \mapsto g_\theta(t) = R_t$$

We may refer to these transformations as Left and Right transformations, respectively. 

Two types of invariant priors or two generalizations of Lebesgue measure, 2-generalization of flatness. 

\begin{definition}
	A measure, $\Pi$, on $\Omega_\Theta$ is the left Haar measure. 

			$$\Pi = \Pi \circ L^{-1}_t \forall t \in \Omega_\Theta $$

	A measure, $\Pi$, is the right Harr measure if 

			$$ \Pi = \Pi \circ R^{-1}_t \forall t \in \Omega_\Theta$$
\end{definition}

\begin{example}
	$$ \Omega_\Theta = \mathbb{R}$$

Translation group;
	$$\mathcal{G} = \{(\theta \mapsto \theta + c = g_c(\theta)): c \in \mathbb{R} \} $$


Suppose that the improper prior density of $\theta$ is $\pi(\theta)$. 

		$$L_t(\theta)= g_c(\theta) = \theta + c = \eta $$

then the density of $\eta$ is 

		$$\phi(g_c^{-1}(\eta) |\frac{\partial g_c^{-1}(\eta)}{\partial\eta}| $$

We want to find

		$$\pi(\cdot)  = \pi(\cdot - c) $$
		$$ \pi(\theta)  = \pi(\theta - c) \quad \forall \theta \in \mathbb{R}$$

If we take $\theta = 0$, 

	$$\pi(0) = \pi(-c) $$


If we take $-c = \theta$, 

		$$\pi(\theta) = \pi(0) \propto \text{ Lebesgue} $$

	Left Haat measure IS the Lebesgue measure. 

Right transformation, 
		
		\begin{align*}
			R_c(\theta) &= g_\theta(c)\\
				&=c + \theta\\
				&= \theta + c\\
				&=g_c(\theta)\\
				&= L_c(\theta)
		\end{align*}

	The right Haar measure is also proportional to the Lebesgue. 
\end{example}

\textbf{Friday February 3}\\

\begin{example}
	Haan measures for this group:

			$$\Omega_\Theta = (0, \infty) $$

			$$\mathcal{G} = \{(\theta \mapsto a\theta): a > 0 \} $$

	We can show that a goup used for distributions like N(0, $\theta$)

	Left tansformation:\\

			$$L_a = a\theta = \eta $$

	if density for $\theta$ is $\pi(\theta)$. 

			\begin{align*}
				\pi_\eta(\eta) &= \pi(\frac{\eta}{a}) |\frac{d \eta/a}{d \eta}|\\
						&=\pi\left(\frac{\eta}{a}\right) \frac{1}{a}\\
				\pi_\eta(1) &= \pi\left(\frac{1}{a}\right)\frac{1}{a}\\
				\pi(\frac{1}{a}) &\propto a\\
				\pi(\theta = \frac{1}{a}) &\propto \frac{1}{\theta}\\
			\end{align*}

	Right transformation:\\

			$$R_a(\theta) = g_\theta(a) = \theta a = L_a(\theta)$$

	Right Haar density is also 

			$$\pi(\theta) = \frac{1}{\theta} $$
\end{example}


\begin{example}
	$N(\mu, \sigma^2), \Omega_\Theta = \{(\mu, \sigma): \mu \in \mathbb{R}, \sigma > 0\}$

	Consider group, 

			$$\mathcal{G} = \{[(\mu, \sigma) \mapsto (c\mu+ b, c\sigma) = (g_{b,c}(\mu, \sigma))]: b \in \mathbb{R}, c > 0 \} $$

	Left transformation:\\

			$$L_{b, c} (\mu, \sigma) = g_{b, c}(\mu, \sigma) = (c\mu + b, c\sigma) = (\tilde{\mu} , \tilde{\sigma}) $$

			\begin{align*}
				\pi_\eta(\tilde{\mu} , \tilde{\sigma}) &= \pi(\frac{\tilde{\mu}- b}{c}, \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
					\frac{\partial \mu}{\partial \tilde{\mu}} & \frac{\partial \mu}{\partial \tilde{\sigma}}\\
					\frac{\partial \sigma}{\partial \tilde{\mu}} & \frac{\partial \sigma}{\partial \tilde{\sigma}}\\
					\end{matrix}\right|\\
				&=\pi(\frac{\tilde{\mu}- b}{c}, \frac{\tilde{\sigma}}{c}) \left|\	\begin{matrix}
					\frac{1}{c} & 0\\
					0 & \frac{1}{c}\\
					\end{matrix}\right|\\
				&=\pi(\frac{\tilde{\mu}- b}{c}, \frac{\tilde{\sigma}}{c}) \frac{1}{		c^2}\\
				\pi_\eta(0,1) &= \pi(\frac{- b}{c}, \frac{1}{c}) \frac{1}{c^2}
			\end{align*}

		So if we consider $(\frac{- b}{c}, \frac{1}{c})  = (\mu, \sigma)$ we get that

				$$\pi(\mu, \sigma) \propto \frac{1}{\sigma^2} $$

		which is the Left Haan measure. 


		Right transformation:\\

				$$ R_{b, c} (\mu, \sigma) = g_{\mu, \sigma}(b, c) = (\mu + \sigma b, c\sigma) = (\tilde{\mu}, \tilde{\sigma}) \neq L_{b,c} $$


				\begin{align*}
					\sigma b + \mu &= \tilde{\mu}\\
					\sigma c &= \tilde{\sigma}\\
					\sigma &= \frac{\tilde{\sigma}}{c}\\
					\mu &= \tilde{\mu} - \sigma b = \tilde{\mu} - \tilde{\sigma}\frac{b}{c}
				\end{align*}
				\begin{align*}
					\pi_\eta(\tilde{\mu}, \tilde{\sigma}) &= \pi(\tilde{\mu} - \	tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
						\frac{\partial \mu}{\partial \tilde{\mu}} & \frac{\partial \mu}{\partial \tilde{\sigma}}\\
						\frac{\partial \sigma}{\partial \tilde{\mu}} & \frac{\partial \sigma}{\partial \tilde{\sigma}}\\
						\end{matrix}\right|\\
					&= \pi(\tilde{\mu} - \tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
						1 & \frac{-b}{c}\\
						0 & \frac{1}{c}\\
						\end{matrix}\right|\\
					&= \pi(\tilde{\mu} - \tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \left|\begin{matrix}
						1 & \frac{-b}{c}\\
						0 & \frac{1}{c}\\
						\end{matrix}\right|\\
					&= \pi(\tilde{\mu} - \tilde{\sigma}(\frac{b}{c}), \frac{\tilde{\sigma}}{c}) \frac{1}{c}
				\end{align*}

				If we replace $(\tilde{\mu}, \tilde{\sigma}) = (0, 1)$ we see that 

						$$ \pi(\mu, \sigma) \propto \frac{1}{\sigma} $$

				Thus the Right and Left Harr are not the same. 
\end{example}

	

% subsection invariant_prior (end)
% %------------------------------------------------

\subsection{Jeffreys' Prior} % (fold)
\label{sub:Jeffrey}

Haar requires natural group of transformations which is not always available in particular appliations. An easily available prior, Jeffrey's prior is constructed using the folloiwng principle. \\

If we assign $\theta$ a measure, $\Pi$ and $\eta = T(\theta)$ is one-to-one transformation, then the prior assigned to $\eta$ should satisfy $\Pi \circ T^{-1}$

Jefferys' Prior\\

Let $f(X|\theta)$ be the likelihood. Let $I(\theta)$ be the Fisher Information

		$$I(\theta) = - E\left[\frac{\partial^2}{\partial\theta \partial \theta^T} \log f(X|\theta) |\theta \right] $$ 

The Jeffreys' Prior is defined as 

		$$\Pi(\theta) \propto \sqrt{det I(\theta)} $$

\begin{theorem}[1.16]
	Let $\pi_\Theta$ be the Jeffreys' prior density of $\theta$. $\Pi_\Phi(\phi)$ is the Jeffreys' prior density for $\phi$ where $\phi = h(\theta)$. Here, $h$ is one-to-one. Then we have that

			$$\pi_\Phi(\phi) = \Phi_\Theta(h^{-1}(\phi)) \left| det\left(\frac{\partial h^{-1} (\phi)}{\partial \phi}\right) \right| $$

	In terms of measure, $\Pi_\Phi = \Pi_\Theta \circ h^{-1}$.
\end{theorem}

\begin{proof}
	Let $f_{X|\Phi}(x|\phi)$ represent the likelihood of $\phi$. Because $\Phi$ and $\Theta$ are one-to-one, 

			$$f_{X|\Phi}(x|\phi) = f_{X|\theta}(x| \theta) = f_{X|\Theta}(X |h^{-1}(\phi)) $$

	Conditional distributions only depends on $\sigma$-field and two one-to-one variables generage same $\sigma$-field. \\

	GET MORE FROM PHOTOS
\end{proof}

\textbf{Monday February 6}\\

\begin{example}
	$$X_1, \dots, X_n \sim N(\theta, \sigma^2)$$

	where both $\theta \& \sigma$ are unknown. \\

	Find Jeffreys' prior for $\theta, \sigma$. \\

			$$f(x_i|\theta, \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\{-\frac{(x_i - \theta)^2}{2 \sigma^2} \} $$

			$$f(x|\theta, \sigma) = \frac{1}{(\sqrt{2 \pi \sigma^2})^n} \exp\{-\frac{\sum_i(x_i - \theta)^2}{2 \sigma^2} \} $$

			$$\log(f(x_i|\theta, \sigma) )= -n \sqrt{2 \pi \sigma^2} -\frac{(x_i - \theta)^2}{2 \sigma^2}  $$

			$$\frac{\partial^2}{\partial \theta^2}\log(f(x_i|\theta, \sigma) )= -\frac{n}{\theta} $$

			$$\frac{\partial^2}{\partial \theta \partial \sigma^2}\log(f(x_i|\theta, \sigma) )= -(\sigma^2)^{-2} \sum(x_i - \theta) $$

			$$\frac{\partial^2}{\partial \sigma^2}\log(f(x_i|\theta, \sigma) )= \frac{n}{2} (\sigma^2)^{-2} - (\sigma^2)^{-3} \sum(x_i - \theta)^2 $$

			Want to take negative expecation of each of theses. 

			$$I(\theta, \sigma^2) = \begin{pmatrix}
				\frac{n}{\sigma^2} & 0 \\
				0 & \frac{n}{2} (\sigma^2)^{-2}\\
			\end{pmatrix} $$

			Want to find the determinent of I. Here it's proportional to $\sigma^{-6}$. 

					$$\sqrt{det(I)} = \sigma^{-3} = \phi^{\frac{3}{2}} $$

			Which gives us our Jeffrey's prior for $(\theta, \sigma^2).$


\end{example}

% subsection jeffrey (end)

\section{Statistical Decision Theory}\index{Decision Theory}

	Rock, Paper, Scissor. \\

	Player One Action Space: $\Omega_{A, 1} = \{R, P, S\}$\\

	Player Two Action Space: $\Omega_{A, 2} = \{R, P, S\}$\\

	Loss Function - $L: \Omega_{A, 1} x \Omega_{A, 2} \rightarrow \mathbb{R}$\\

	Without loss of generality, we may assume the lose to be player one's loss. \\

	Winner gains one dollar, loser loses a dollar, and in the case of a tie, no money is exchanged. 


					\begin{table}[]
				\centering
				\caption{Paper Rock Scissor Loss Table}
				\label{PRS_Loss}
				\begin{tabular}{lllll}
				  & R  & P  & S  &  \\
				R & 0  & -1 & 1  &  \\
				P & 1  & 0  & -1 &  \\
				S & -1 & 1  & 0  & 
				\end{tabular}
				\end{table}


So we have $(\Omega_{A,1}, \Omega_{A, 2}, L)$. In statistics, we may refer to player one as the "statistician" and player two as "nature".\\

Player one's action is called the action space, $\Omega_A$. Player two's action space is called the parameter space, $\Omega_\Theta$.\\


But where does our data come from in this scenario? We are dealing with a statistical decision problem. 

\textbf{Statistical Decision Problem}\\

The statisticain (player one) wants to know nature's action, $\Theta$, but he doesn't. He observes a random variable, whose distribution depends on $\theta$. This is defused information about $\theta$. The random element, X, takes value on 

	$$(\Omega_X, \mathcal{F}_X)  $$

	where $X \sim P_\theta$, $\theta \in \Omega_\Theta$. \\


So we have

			$$ (\Omega_X, \mathcal{F}_X, \{P_\theta: \theta \in \Omega_\Theta\}) $$

on top of $(\Omega_A, \Omega_\Theta, L$. 

Decision rule:

		$$d: \Omega_X \rightarrow \Omega_A $$

and the collection of all decision rule is $\mathcal{D}$. This incurs a loss $L(\theta, d(x))$. \\

Before we take an action, our anticipation of loss is 

		$$E(L(\theta, d(x))|\theta) $$

which we may refer to as \textbf{risk}. \\

Game: $(\Omega_A, \Omega_\Theta, L)$\\
Data: $(\Omega_X, \mathcal{F}_X)$\\
Model: $\{P_\theta: \theta \in \Omega_\Theta\}$\\
Decision Rule: $\mathcal{D}$\\
Risk: $R$

\textbf{Bayesian Statistical Decision Problem}\\

Here, the game, data, model, and decision rule remain the same. The difference is in how we calculate the risk and the use of a prior.

Game: $(\Omega_A, \Omega_\Theta, L)$\\
Data: $(\Omega_X, \mathcal{F}_X)$\\
Model: $\{P_\theta: \theta \in \Omega_\Theta\}$\\
Decision Rule: $\mathcal{D}$\\
Prior: $P_\Theta$\\
Risk: $r$\\

		$$r(d) = E_{\Theta, X}[L(\Theta, d(x))] = \int_{\Omega_\Theta x \Omega_X} L(\theta, d(x)) dP_{\Theta, X} $$  



\textbf{Wednesday February 8}\\


Bayes Rule: Optimal estimation, optimal test. This is (mathematically) easier than frequentist optimal procedures becuase we have a metric in $\Theta$. Don't have to make uniform statements about $\theta$, such as UMVUE, UMP, UMPU, $\dots$. \\


\begin{definition}[Bayes' Rule]
	The Bayes rule is 

			$$d_B = \arg\min\{r(d): d \in \mathcal{D} \} $$

If $P_\Theta$ is improper then $d_B$ is called \textbf{generalized Bayes' Rule}. 

		$$r(d) = \int_{\Omega_\Theta x \Omega_X} L(\theta, d(x))  f_{X, \Theta}(x, \theta) d(\mu_x x \mu_\Theta)(x, \theta)$$


Note that usually $L(\theta, d(x)) \geq 0$ so we have that

		$$f_{x, \Theta} = \left\{\begin{array}{ll}
			f(x|\theta) \pi(\theta)\\
			\pi(\theta|x) f(x)
		\end{array} \right. $$

First way:

		\begin{align*}
			r(d) &= \int_{\Omega_\Theta } \left[\int_{\Omega_X} L(\theta, d(x))  f_{X|\Theta}(x| \theta) d(\mu_x )(x)\right] \pi(\theta) d\mu_\Theta\\
				&= \int_{\Omega_\Theta } R(\theta, d) \pi(\theta) d\mu_\Theta\\
		\end{align*} 

Other way would swap $\Theta, X$. (??)

		\begin{align*}
			r(d) &= \int_{\Omega_X } \left[\int_{\Omega_Theta} L(\theta, d(x))  \pi_{\Theta|X}(x| \theta) d(\mu_\Theta )(\theta)\right]  d\mu_X\\
				&= \int_{\Omega_X } \rho(x, d(x)) d\mu_X(x)\\
		\end{align*} 

Where we have the posterior expected loss, 

		$$\rho(x, a) = E(L(\theta, a)|X) $$

\end{definition}

INSERT PHOTO

How to calculate Bayes's rule, not by definition. \\

\begin{definition}
	

		$$\arg\min(r(d): d \in \mathcal{D}) $$

is the minimum over a set of functions,  $\mathcal{D}$. A member of $\mathcal{D}$ is 

		$$d: \Omega_X \rightarrow \Omega_A $$

But this can be converted into minimization over numbers or vectors by the following theorem. 
\end{definition}

\begin{theorem}[1.7 in Notes]
	Suppose $L(\theta ,a) \geq C < - \infty$ for all $\theta \in \Omega_\Theta, a \in \Omega_A$. Then the decision rule,  

			$$d_B: \Omega_X \rightarrow \Omega_A, x \mapsto \arg\min \{\rho(x, a): a \in \Omega_A \} $$

	is the Bayes' Rule. 
\end{theorem}

$\Omega_A$ is usually $\mathbb{R}, \mathbb{R}^P$ or subsets thereof. So we are not minimizing over functional spaces. 

\begin{proof}
	By Tonelli's theorem, (because we can assume WLoG that $L(\theta, a) \geq 0, \quad \forall \theta, a$) 

			 $$r(d) = \int_{\Omega_X} \rho (x, d(x)) f_X(x) d \mu_X(x)$$

So, for any x, 

		$$\rho (x, d_B(x)) \leq \rho (x, d(x)) $$

Thus, 

		$$  \int_{\Omega_X} \rho (x, d_B(x)) f_X(x) d \mu_X(x) \leq \int_{\Omega_X} \rho (x, d(x)) f_X(x) d \mu_X(x)$$

And we have that $r(d_B) \leq r(d) \forall d \in \mathcal{D}$. 
\end{proof}

In frequentist theory, optimality have to be stated uniformly. \\

"Commonly used optimal criteria" in frequentist decision theory 

		\begin{enumerate}
			\item admissibility
			\item minimax 
		\end{enumerate}

\begin{definition}
	A decision rule is \textbf{inadmissible} if there exists $d' \in \mathcal{D}$ such that 

		\begin{enumerate}
			\item $R(\theta, d') \leq R(\theta, d) \forall \theta \in \Omega_\Theta$
			\item  $R(\theta, d') < R(\theta, d) \text{ for some } \theta \in \Omega_\Theta$
		\end{enumerate}

A decision rule that is not inadmissible is admissible. 
\end{definition}

\begin{definition}
	A decision rule $d\in \mathcal{D}$ is a \textbf{minimax rule} if for all $d' \in \mathcal{D}$, 

			$$\sup_{\theta \in \Omega_\Theta} R(\theta, d) \leq \sup_{\theta \in \Omega_\Theta} R(\theta, d')$$
\end{definition}

The relation between Bayes' rule and admissible rule, generally "all Bayes rule are adimissible". 

\begin{theorem}[1.8 in Notes]

Suppose 

	\begin{enumerate}
		\item for each $d \in \mathcal{D}$, $R(\theta, d)$ is integrable with respect to $P_\Theta$ 
		\item for any $d_1, d_2 \in \mathcal{D}$, 

				$$R(\theta, d_1) - R(\theta, d_2) < 0 $$

		for some $\theta \in \Omega_\Theta$ which implies that

				$$P(R(\theta, d_1) - R(\theta, d_2) < 0 ) > 0$$
	\end{enumerate}

Then any Bayes or generalized Bayes rule is admissiable. 


\end{theorem}

\begin{proof}
	Suppose $d_1 \in \mathcal{D}$ is Bayes and inadmissible. Then there exists $d_2 \in \mathcal{D}$ such that

		\begin{enumerate}
			\item $R(\theta, d_2) \leq R(\theta, d_1) \forall \theta \in \Omega_\Theta$
			\item  $R(\theta, d_2) < R(\theta, d_1) \text{ for some } \theta \in \Omega_\Theta$
		\end{enumerate}

But this gives us that 

		$$R(\theta, d_2) - R(\theta, d_1) \geq 0  \forall \theta$$

		and
			$$P(R(\theta, d_2) - R(\theta, d_1) < 0 ) > 0$$
Recall from STAT 517 that if $f \geq 0, \mu(f> 0) > 0$ then 
		
				$$\int f d\mu > 0 $$

	or that  if $f \leq 0, \mu(f< 0) > 0$ then

			$$\int f d\mu < 0  $$

	This implies that 

		\begin{align*}
			\Rightarrow \int R(\theta, d_2) - R(\theta, d_1) d P_\Theta < 0\\
			\Rightarrow \int R(\theta, d_2)d P_\Theta < \int R(\theta, d_1) d P_\Theta\\
			\Rightarrow r(d_2) < r(d_1)\\
			\Rightarrow d_1 \text{ not Bayes'}
			\end{align*}

\end{proof}


\textbf{Friday February 10}\\

Theorem 1.8 condition 2 is a mild condition which is satifried by the two important cases

		\begin{enumerate}
			\item $R(\theta, d)$ is continuous function for all $d \in \mathcal{D}$, and $P(\Theta \in G) > 0$ for any empty open set. 


			For example, if P << $\lambda$, $\lambda$ << P ($P \equiv \lambda$), then $P(\Theta \in G) < 0$ fro all open $G \neq 0$. To see that the above statement is sufficient for the second condition of Theorem 1.8, 

			FINISH FROM PHOTO

			\item $\Omega_\Theta$ is countable ($\sigma$-finite)

					$$\Omega_\Theta = \{\theta_1, \theta_2, \dots\} $$

			and
					$$ P(\Theta = \theta_i) > 0, \forall i = 1, 2, \dots $$

			If $R(\theta, d_2) - R(\theta, d_1) > 0$ for some $\theta \in \Omega_\Theta$ then this implies that

					$$ R(\theta_i, d_2) - R(\theta_i, d_1) >0 \quad i \in \{1, 2, \dots\}$$

			But we know that $P(\Theta = \theta_i) > 0$ so 

					$$P(R(\theta_i, d_2) - R(\theta_i, d_1) <0)>0 $$
		\end{enumerate}


%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapterimage{chapter_head_2.pdf} % Chapter heading image

\chapter{Estimation \& Inference}

Typically for esitmation, 

	$$\Omega_\Theta = \mathbb{R}^P, \Theta \subseteq \mathbb{R}^P, \Omega_A = \Omega_\Theta $$

so for testing problems, 

		$$\Omega_\Theta = \{0, 1\} = \Omega_A $$

but for clarification problesm, 


$$\Omega_\Theta = \{a_1, \dots, a_k\} = \Omega_A$$

\section{Estimation}\index{Estimation}

		$$\Omega_\Theta = \Omega_A = \mathbb{R}^P $$

One of the most commonly used loss is the squared loss or $L_2$ loss. 

\begin{definition}
	The $L_2$ loss is

			$$L(\theta, a ) = (\theta - a)^T w(\theta)(\theta - a) $$

	where $w(\theta > 0), \forall \theta\in \Omega_\Theta$
\end{definition}


In this case, the Bayes' rule is explicit. 

\begin{theorem}
	Suppose that 

		\begin{enumerate}
			\item $E[\theta^T w(\theta)\theta | X] < \infty$ a.s.
			\item  $E[w(\theta)| X] >0$ a.s
		\end{enumerate}

		Then, 

				$$d_B(x) = \left(E[w(\theta) | X]\right)^{-1} E[w(\theta)\theta|X] $$

		is the Bayes' rule. 
\end{theorem}

Note that if $w(\theta) = I_P$, then Bayes' rule for loss, $||\theta - a||^2$, is simply $E(\theta|X)$. 

\begin{proof}
	Because $L(\theta, a) \geq 0$, we need to minimize

			$$\rho(x, a) = E[(\theta - a)^T w(\theta) (\theta - a) | X] $$

	where a can depend on x. 


	\begin{align*}
		E[(\theta - d_B(x) &+ d_B(x) - a)^T w(\theta) (\theta - d_B(x) + d_B(x) - a) | X] \\
			&= E[( d_B(x) - a)^T w(\theta) (d_B(x) - a) | X] + E[( d_B(x) - a)^T w(\theta) (\theta - d_B(x) ) | X] + E[(\theta - d_B(x))^T w(\theta) ( d_B(x) - a) | X] + E[(\theta - d_B(x) )^T w(\theta) (\theta - d_B(x)| X]\\
			&= (d_B - a)^T E[w(\theta - d_B) | X]\\
			&= (d_B - a)^T \left(E[w\theta | X] - E(w d_B|X)\right)\\
			&= 0
	\end{align*}

	Overview: 

		$$\rho(x, a) = \text{nonneg } + 0 + 0 + \rho(x, d_B)  \Rightarrow \rho(x, a) \geq \rho(x, d_B)  $$

		So we have that $d_B$ is the Bayes' rule. 
\end{proof}

Another commonly used loss is the $L_1$ loss. 

\begin{definition}
	If $p = 1$ then the $L_1$ loss is 

			$$L(\theta, a) = |\theta - a| $$

What is the Bayes rules here?
\end{definition}

\begin{definition}[Median]
	Let  U be a random varaible with c.d.f. F. Then any n umber m that satisfies 

			$$F(m-) \leq \frac{1}{2} \leq F(m) $$

	is called the \textbf{median}. Note that this definition does not require F to be monotone. 
\end{definition}


\begin{theorem}
	If U is integrable and m is a median of U, then 

		$$\int |U- m | dP \leq \int |U - a| dP \quad \forall a \in \Omega_U $$ 
\end{theorem}



\begin{proof}
	Case 1. m < a\\

	\begin{align*}
		\int_{\Omega_U} |U-m| - |U-a|dP &= \int_{U \leq m} + \int_{m< U \leq a} + \int_{U < a}\\ 
			&= \int_{U \leq m} m-a dP+ \int_{m< U \leq a} 2U - m - a dP + \int_{U < a} a - m dP\\
			&= (m-a)P(U \leq m) + (**) + (a- m) P(U > a) 
	\end{align*}

	For (**), 

		\begin{align}
			\int_{m< U \leq a} 2U - m - a dP &\leq (2a - m - U) P(m < U \leq a)\\
			&= (m-a)[P(U \leq m) - P(U > a) - P(m < U \leq a)]\\
			&= (m-a)[P(U \leq m) - P(U> m)]\\
			&= (m-a)[P(U\leq m) - (1 - P(U \leq m))]
			&= (m - a) [2 F(m) - 1]
		\end{align}

		So when a > m, 

			$$E|U- m| - E|U- a| \leq (2 F(m) - 1)(m-a) $$

	Case 2. $a < m$\\

	By the same argument, 

			$$ E|U- m| - E|U- a| \leq (1 - 2 F(m-))(a- m) $$

	But, because m is a median, 

			$$ F(m-) \leq \frac{1}{2} \leq F(m)$$

			FINISH FROM PHOTO	
\end{proof}

\textbf{Monday February 13}\\

\textbf{Corollary 2.1}\\

The posterior median $M(\theta|X)$ is a Bayes rule with respect to 

		$$L(\theta, a) = |\theta - a| $$


More generally, 

		$$L(\theta, a) = \left\{ \begin{array}{ll}
			\alpha_1(\theta - a) & \theta \geq a\\
			\alpha_2(a - \theta) & \theta < a
		\end{array} \right. $$

Here, Bayes rule is quantile. 


Besides Bayes estimations, another populat estimation is the generalized MLE. This is simply, 

		$$\hat{\theta} = \arg\max \{\pi(\theta| X) : \theta \in \Omega_\Theta\} $$

the posterior mode. MLE is a special case where $\pi(\theta)$ is a constant. \\

\begin{example}
 	Bayes estimation under 

 			$$L(\theta, a) = (\theta - a)^2 = |\theta  - a|$$

 	Also generalized MLE. \\

 	We have a linear regression, 

 			$$Y_i = \theta x_i + \epsilon_i \quad i = 1, \dots, n $$
 			$$\epsilon_1, \dots, \epsilon_n | \theta \stackrel{iid}{\sim} N(\theta, \sigma^2) $$

 	We have prior knowledge that $\theta \geq 0$ (e.g. growth rate). how do we incorporate their information for estimation.\\

 			$$\Pi(\theta) = 1 \quad \theta \geq 0 = I(\theta \geq 0) $$

 	Here we find the imporoper prior.  

 			$$f(x|\theta) \propto \exp\{- \frac{x^T x}{2 \sigma^2} \theta^2 + \frac{x^T y}{\sigma^2}\theta\} $$ 


where $x = \begin{pmatrix}
	x_1\\
	\vdots\\
	x_n
\end{pmatrix}, y = \begin{pmatrix}
	y_1\\
	\vdots\\
	y_n
\end{pmatrix}$\\

By Lemma 1.1, if $f(\theta) \propto \exp\{-a \theta^2 + b \theta\}$ then 

		$$\theta \sim N(\frac{b}{2a}, \frac{1}{2a}) $$

So, as a function of $\theta$, $\theta \mapsto f(x|\theta)$ has the form 

		$$N(\frac{x^T y}{x^T x}, \frac{\sigma^2}{x^T x}) $$

and we can then see that the posterior density would be 

		$$\pi(\theta|x) = \frac{N(\mu(x, y), \tau^2(x, y)) I(\theta \geq 0)}{\int^\infty_0 N(\mu, \tau^2) d\mu} $$

Note that this integral integrated with repsect to $\theta$ becomes 

		$$\int^\infty_0 N(\mu, \tau^2) d\theta = \int ^\infty_{\mu / \tau} N(0,1)d\theta = \Phi(\mu/\tau) $$

Now with the conjugate Bayes rule and $L(\theta, a) = (\theta- a)^2$ we may let $\gamma = \frac{\theta - \mu}{\tau}$ and get that

		$$\pi(\gamma | x) = \frac{N(0,1)}{\Phi(\frac{\mu}{\tau})} I (\gamma \geq - \frac{\mu}{\tau}) $$

		\begin{align*}
			E(\gamma|X) &= \frac{1}{\Phi(\mu/ \tau)} \int^\infty_{- \mu/\tau} \gamma N(0, 1) d\gamma \\
				&= \frac{\exp\{- \mu^2 / (2\tau^2)\}}{\sqrt{2 \pi} \Phi(\mu/ \tau)}
		\end{align*}

		$$E(\theta | X) = \mu + \tau * \frac{\exp\{- \mu^2 / (2\tau^2)\}}{\sqrt{2 \pi} \Phi(\mu/ \tau)} $$

where, as above,  $\mu = \frac{x^Ty}{x^T x}$, and $\tau^2 = \frac{\sigma^2}{x^T x}$. \\



 \end{example} 

 Bayes rule under $L(\theta, a ) = |\theta - a|$ is the median. So the solution would be 

 		$$\int^m_0 N(\mu, \tau^2) d\theta = \frac{1}{2} \int^\infty_0 N(\mu, \tau^2) d\theta $$

 So with the change of variables, 

 		$$\int^{(m-\mu)/ \tau}_{-\mu/ \tau} N(0, 1) d\gamma = \frac{1}{2} \int^\infty_{- \mu/ \tau} N(0, 1) d\gamma $$

 and after some algebra we get that 

 		$$m = \tau \Phi^{-1}(1 - \frac{1}{2}\Phi(\mu / \tau)) + \mu $$

 and so here the generalized MLE is the MLE. \\

 This is simply maximizing $N(\mu, \tau^2)$ over $\theta \geq 0$. 

So the generalized MLE over $\theta \geq 0$ is 

		$$\max(0, \frac{x^T y}{x^T x}) $$

\section{Bayes Rule and Unbiasedness}\index{Bayes Rule and Unbiasedness}

Another relation between Bayesian and Frequentist criteria. Whereas bayes rule and admissibility are somewhat consistant, unibased is a fundamentally unBayesian idea. 

Recall that d(x) is unbiased for $\theta$ if and only if 

		$$E(d(x) | \theta) = \theta \forall \theta $$

\begin{theorem}[2.3 in notes]
	Suppose $d_B(x)$ is Bayes rule with respect to 

			$$ L(\theta, a) = (\theta - a)^T W(\theta)(\theta - a) $$

	then $d_B(x)$ is biased unless

			$$d(x) = \theta a. s. P_{\theta, X} $$
\end{theorem}


\begin{proof}
	Recall that 

			$$d_B(x) = \left[E(W(\theta)|X)\right]^{-1} E(W(\theta)\theta|X) $$

	 In the follwoig, write $W(\theta) = W$, $d_B(x) = d_B$. We want to show that if $d_B$ is unbiased then 

	 		$$r(d_B) = 0 \Leftrightarrow \theta = d_B(x) a.s. P $$

	 		\begin{align*}
	 			r(d_B) &= E\left[(\theta - E(W|X)^{-1}E(W\theta|X)^T W(\dots)\right]\\
	 				&= E\left[ E(\theta^T W \theta | X) - E(W\theta|X)^TE(W|X)^{-1} E(W\theta|X)\right]\\
	 				&=E\left[ E(\theta^T W \theta | X) - d_B^T E(W\theta|X)\right]\\
	 				&=E\left[ E(\theta^T W \theta  - d_B^T W\theta|X)\right]\\
	 				&= E(\theta^T W \theta  - d_B^T W\theta)\\
					&= E(\theta^T W \theta  - E(d_B|\theta)^T W\theta)\\
					&= 0
	 		\end{align*}
\end{proof}


\textbf{Wednesday February 15}\\

\section{Error Assessment in Bayesian Setting}

In frequentist setting, X is nothing but one realizeation of a random variable that potentially takes many values. So we may use

		$$\Var_\theta(x) = \Var(x|\theta) $$

or 

		$$\MSE(X|\theta) = \E(X - \E(X|\theta))^2 $$

In the Bayesian context, X is fixed (i.e. it's being conditional on). Once you condition on something it's out of the probability picture. 

		$$\Var(\theta|X) = \E[(\theta - \E(\theta|X))(\theta- \E(\theta|X))^T]$$
		$$\MSE_d(\theta|X) = \E(\theta - d(x)|x)^2 = \E [(\theta - d(x))(\theta - d(x))^T] $$

Note that $\Var(\theta|X)$ is a special case of $\MSE_d(\theta|X)$ when $d = d_B$ under squared loss. It's easy to show the identity 

		$$\MSE_d(\theta|X) = \Var(\theta|X) + [\E(\theta|X) - d(x)][\E(\theta|X) - d(x)]^T $$

\begin{example}
	Suppose that

			$$X_1, \dots, X)n|\phi \stackrel{iid}{\sim} N(0, \phi), \phi\sim \tau \chi^{-2}_{(\nu)}$$

	Then as in your HW \#2, you can show that 

			$$\phi | X_1, \dots, X_n \sim (s + \tau) \chi^{-2}_{(\nu + n)} $$



	where $s = \sum^n_{i=1} X_i^2$. 

	Also, as shown in the homework, 

			$$\E(\phi|X_1, \dots, X_n) = \frac{s+ \tau}{\nu + n - 2} $$

			$$\Var(\phi|X_1, \dots, X_n) = \frac{2(s+ \tau)^2}{(\nu + n - 2 )^2 (\nu + n - 4)} $$

	Suppose that $d_1(x) = \frac{1}{n} S$, unbiased estimator ($\mu$ = 0). Also that $d_2(x) = \frac{s + \tau}{\nu + n +2}$ is the GMLE (HW 2). 

			$$\MSE_{d_1}(\phi|X_1, \dots, X_n) = \frac{2(s + \tau)^2}{(\nu + n - 2)^2 (\nu + n - 4)} + \left(\frac{s + \tau}{\nu + n - 2} - \frac{1}{n}S\right)^2 $$

			$$\MSE_{d_2}(\phi|X_1, \dots, X_n) = \frac{2(s + \tau)^2}{(\nu + n - 2)^2 (\nu + n - 4)} + \left(\frac{s + \tau}{\nu + n - 2} - \frac{s + \tau}{\nu + n + 2}S\right)^2 $$

	So you can present $d \pm \sqrt{\MSE_d (\theta | X_1, \dots, X_n)}$. 
\end{example}

\section{Credible Set (or Interval)}

In the frequentist setting, CI is a random interval. C(X):

		$$P_\Theta(\theta \in C(X)) = 1 - \alpha $$

In the Bayesian setting, C(X) is fixed, we want: 

		$$P(\theta \in C(X)| X) = 1 - \alpha $$

\begin{definition}
	A $(1 - \alpha)$-credible set is any $C \in \mathcal{F}_\theta$ such that 

			$$P(\Theta^{-1}(C) || X) \geq 1 - \alpha\quad a.s.$$
\end{definition}

Shortest interval is preferred. HPD: Highest Posterior Density Credible Set. 

\begin{definition}
	The $(1 - \alpha)$-highest posterior density for $\theta$is a $C \in \mathcal{F}_\theta$ such that

			$$C = \{\theta \in \Omega_\theta | \pi(\theta|X) \geq k_\alpha \} $$

	where

			$$k_\alpha = \sup \{k| P(\pi(\theta|X) \geq k) \geq 1 - \alpha \} $$
\end{definition}

Intuitively, 

PHOTO

Continuous $\pi(\theta|X)$ then in this case (PHOTO) 

		$$P(\pi(\theta|X) \geq k_\alpha) = 1 - \alpha $$
		$$P(\{\theta| pi(\theta|X) \geq K_\alpha \}|| X) $$

As we move K up, $P(\pi(\theta|x) \geq K)$ takes only three values. What if $1-\alpha$ is not one of the three?

PHOTO

HPD is the shortest credible set as proved in next theorem.

\begin{theorem}[2.4]
	Suppose that $\pi(0) > 0 $, for all $\theta \in \Omega_\Theta$. Let $C_\alpha^*$ be a $(1 - \alpha)$ HPD credible set and $C_\alpha$ be any $(1-\alpha)$ credible set. 

	Furthermore, assume that $P_{\Theta|X}(C_\alpha^*|X) = 1 - \alpha$. Then, 

			$$\mu_\Theta(C^*_\alpha) \leq \mu_\Theta(C_\alpha) $$
\end{theorem}

\begin{proof}

Want to show: 

	$\mu_\Theta (c) < \mu_\Theta(C_\alpha^*) \Rightarrow P_{\Theta|X}(C|X) < P_{\Theta|X}(C^*_\alpha| X) = 1 - \alpha $

Let $C \subseteq \Omega_\Theta, \mu_\Theta(C) < \mu_\Theta(C^*_\alpha) \Rightarrow \mu(C\setminus C_\alpha^*) < \mu(C_\alpha^* \setminus C)$

	
\textbf{Friday February 17}\\

FINSIH PROOF

We know that 

$\pi(\theta|X) \geq K_\alpha$ on $C^*_\alpha \Rightarrow$ on $C^*_\alpha \setminus C$.\\

$\pi(\theta|X) \leq K_\alpha$ on $C(^*_\alpha)^C \Rightarrow$ on $C \setminus C^*_\alpha$.\\

\begin{align*}
	P_{\Theta|X} (C_\alpha^* \setminus C |X) &= \int_{C_\alpha^* \setminus C} \pi(\theta | x) d\mu_\Theta(\theta)\\
		&\geq K_\alpha \mu_\Theta(C_\alpha^* \setminus C)\\
		&> K_\alpha \mu_\Theta(C \setminus C_\alpha^*)\\
		&\geq \int_{C \setminus C_\alpha^*} \pi(\theta|X) d\mu_\Theta\\
		&= P_{\Theta | X) }(C\setminus C_\alpha^*) 
\end{align*}

\end{proof}

INCLUDE PICTURE

\begin{example}
	$$X_1, \dots, X_n |\lambda, \phi \stackrel{iid}{\sim} N(\lambda, \phi)$$
	$$\lambda | \phi \sim N(a, \frac{\phi}{m}) $$
	$$\phi \sim \tau \chi^2_{(k)} $$

	So we know that 

			$$\frac{\sqrt{n +  m} (\lambda - a(x))}{\sqrt{\tau(x) \setminus (n + k)}} \left| \right. X \sim t_{(n_k)}$$

	where, 

	$a(x) = (n \bar{x} + ma) \setminus (n + m)$\\
	$\tau(x) = \sum(X_i - \bar{X})^2 + \tau + (\bar{X} - a)^2(m^{-1} + n^{-1})$\\

	INSERT PHOTO

	So the $(1 - \alpha)*100\%$ credible set for $\lambda$ is

			$$\{\lambda: -t_{(n+k)}(\frac{\alpha}{2}) < \frac{\sqrt{n +  m} (\lambda - a(x))}{\sqrt{\tau(x) \setminus (n + k)}} < t_{(n+k)} (\frac{\alpha}{2})\} $$

	Here we get, 

			$$a(x) \pm t_{(n+k)}(\frac{\alpha}{2} \sqrt{\tau(x) [(n+k)(n+m)]}) $$


	Recall that we also know that

			$$\frac{\phi}{\tau(x)}|x \sim \chi^{-2}_{(n+k)} $$

	PHOTO

	$C_1, C_2$ are solution to 

			$$h(C_1) = h(C_2)$$
			$$\int^{C_2}_{C_1} h(t) d t = 1 - \alpha $$

	where h is pdf of $\chi^{-2}_{k+m}$

	HDD set of $\{\phi: C_1 < \frac{\phi}{\tau(x)} < C_2 \}$

\end{example}



\section{Hypothesis Test}\index{Hypothesis Test}

PHOTO 

Common we use the following losses\\

	\begin{itemize}
		\item 0-1 <- if wrong lose 1
		\item 0 - $C_1$ - $C_2$ <- more nuanced. If wrong one way, $C_1$. If wrong the other way, $C_2$. 
	\end{itemize}


	$$L(\theta, a) = \left\{ \begin{array}{ll}	
		0 & 	(\theta, a) \in (\Omega_\Theta^{(0)} x \{ a_0\}) \cup (\Omega_\Theta^{(1)} x \{a_1\})\\
		C_1 & (\theta, a) \in (\Omega_\Theta^{(0)} x \{ a_1\}) \\
		C_2 & (\theta, a) \in (\Omega_\Theta^{(1)} x \{a_0\})\\
	\end{array}	\right. $$

	\begin{theorem}
		Suppose that 
				$$0 < P_{\Theta| X} (\Omega_\Theta^{(0)} | X) < 1 $$

		then the Bayes' rule for 0 - $C_1$ - $C_2$ Loss is 

				$$d_B(x) = \left\{ \begin{array}{ll}
					a_0 & C_1 P_{\Theta|X} (\Omega_\Theta^{(1)} | X) \leq C_0 P_{\Theta|X} (\Omega_\Theta^{(0)} | X)\\
					a_1 & C_1 P_{\Theta|X} (\Omega_\Theta^{(1)} | X) > C_0 P_{\Theta|X} (\Omega_\Theta^{(0)} | X)\\
				\end{array} \right. $$
	\end{theorem}

	\begin{proof}
		Since the loss is bad, need to minimize 

				$$\rho(x, a)$$

	 over $(a_0, a_1)$. 

	 	\begin{align*}
	 		\rho(x, a) &= E(L(\theta, a) | X)\\
	 			&= \int_{\Omega_\Theta^{(0)}} L(\theta, a) dP_{\Theta|X} + \int_{\Omega_\Theta^{(1)}} L(\theta, a) dP_{\Theta|X}\\
	 		\rho(x, a_0) &= 0 + C_1 P_{\Theta|X} (\Omega_\Theta^{(1)})\\
	 		\rho(x, a_1) &=  C_2 P_{\Theta|X} (\Omega_\Theta^{(0)}) + 0\\
	 	\end{align*}


	\end{proof}

There is a problem in the frequentist setting (e.g. $H_0: \theta = \theta_0$)

		$$\Omega_\Theta^{(0)} = \{\theta_0 \} $$

So, if we put an absolute continous prior, then there is no mass assigned on $\Omega_\Theta^{(0)}$. So, the above Bayes rule does not work. 

\begin{definition}
	Let $(\Omega, \mathcal{F})$ be a measureable space. Then the Divas measure for $a \in \Omega$ is the set function 

			$$\delta_a(B) = \left\{\begin{array}{ll}
				1 & a \in B\\
				0 & a \notin B
			\end{array} \right. $$

	This is a measure (proved in 517). 
\end{definition}


Note that is also can be shown that for any function $f: \Omega \rightarrow \mathbb{R}, B \in \mathcal{F}$

		$$\int_B f(\omega) \delta_a (d \omega) = f(a) \delta_a(B) $$


\textbf{Monday February 20}\\

\textbf{Lemma 2.1} Suppose that $(\Omega, \mathcal{F})$ is a measureable space. We have that $a \in \Omega, B \in \mathcal{F}, \delta_a(B)$ is the Dirac measure about a. Suppose that for any a, $\{a\}\in \mathcal{F}$, (true for Borel). Suppose that

		$$f:\Omega \rightarrow \mathbb{R}, \textcircled{m} \mathcal{F}\setminus\mathbb{R} $$

then we have that

		$$\int_B f(\omega) d\delta_a(\omega) = f(a) \delta_a(B)$$

\begin{proof}
			
		$$\int_B f d\delta_a = \sup_{\{A_i\} \in \mathcal{P}} \sum^k_{i=1}[\inf_{\omega < A_i} f(\omega)] \delta_a(A_i) $$

If $a \notin B$, then the above summation part of the equality is 0 for all $\{A_i\} \in \mathcal{P}$.\\

Now suppose that $a \in B$, then there exists a unique $A_i \in \{A_i\}$ such that$a \in A_i$.\\

So for any $\{A_i\} \in \mathcal{P}$ 

		$$ \sum^k_{j=1}[\inf_{\omega < A_j} f(\omega)] \delta_a(A_j) = [\inf_{\omega < A_i} f(\omega)] \delta_a(A_i), \quad a \in A_i$$

We know that this term is less than f(a), so this implies that

		$$\int_B f d\delta_a \leq f(a) $$

From here we want to take the special partition

		$$(\{a\}, B\setminus \{a\}) \in \mathcal{P} $$

		\begin{align*}
			\int f d \delta_a \geq [\inf_{\omega \in \{a\}} f(\omega)] \delta_a(\{a\}) + [\inf_{\omega \in B \setminus \{a\}} f(\omega)] \delta_a(B\setminus \{a\})\\
				&= f(a)*1 + 0\\
				&=f(a)
		\end{align*}

So we have that $\int f d \delta_a = f(a)$ and thus we may conclude that 

		$$\int f d \delta_a = \left\{\begin{array}{ll}
			0 & a \notin B\\
			f(a) & a \in B
		\end{array} \right. \Rightarrow \int f d \delta_a = f(a) \delta_a(B)  $$
\end{proof}

Suppose that $Q_\Theta$ is a measure on $(\Omega_\Theta, \mathcal{F}_\Theta)$ such that 

		$$Q_\Theta << \nu_\Theta << \lambda $$

	Let $P_\Theta = (1 - \epsilon)Q_\Theta + \epsilon \delta_{\Theta_0}$ and $\Pi_\Theta(\theta) = \frac{d Q_\Theta}{d \nu_\Theta}$. 

\begin{theorem}[2.6]
	Suppose that $P_\Theta$ is defined as above. Then, for $a \in A_i$, 

		$$P_{\Theta|X}(\{\theta_0\}|x) = \frac{\epsilon f(x|\theta_0)}{(1 - \epsilon)\int_{\Omega_\Theta} f(x|\theta) \pi_\Theta(\theta) d \nu_\Theta(\theta) + \epsilon f(x|\theta_0)} $$

\end{theorem}

\begin{proof}
	Let $\mu_\Theta = (1 - \epsilon) \nu_\Theta + \epsilon \delta_{\Theta_0}$. We want to show that $P_\Theta << \mu_\Theta$ and find $\frac{d P_\Theta}{d \mu_\Theta}$. But in order to find this derivative we'll have to guess and check!


			$$\tau_\Theta(\theta) = \left\{ \begin{array}{ll}
				\pi_\Theta(\theta) & \theta \neq \theta_0\\
				1 & \theta = \theta_0
			\end{array} \right.$$

	We want to check  $\tau_\Theta = \frac{d P_\Theta}{d \mu_\Theta}$. 

	Let $B \in \mathcal{F}_\Theta$, 

	\begin{align*}
		\int_B \tau_\Theta(\theta) d\mu_\Theta(\theta) &= \int_B \tau_\Theta(\theta) d((1 - \epsilon) \nu_\Theta + \epsilon \delta_{\theta_0})\\
			&=(1 - \epsilon)\int_B \tau_\Theta d \nu_\Theta + \epsilon\int \tau_\Theta d \delta_{\theta_0}\\
			&=\dots\int_{B \setminus \{\theta_0\}} \tau_\Theta d \nu_\Theta + \dots\\
			&=\dots \int_{B \setminus \{\theta_0\}} \Pi_\Theta d \nu_\Theta + \dots\\
			&=\dots \int_{B} \Pi_\Theta d \nu_\Theta + \dots\\
			&=(1 - \epsilon)\int_{B} \Pi_\Theta d \nu_\Theta + \epsilon\int \tau_\Theta d \delta_{\theta_0}\\
			&= (1 - \epsilon)\int_{B} \Pi_\Theta d \nu_\Theta + \epsilon \delta_{\theta_0} (B)\\
			&= (1 - \epsilon) Q_\Theta(B) + \epsilon \delta_{\theta_0}(B)\\
			&= P_\Theta(B)
	\end{align*}

	Therefore by RN Theorem 

			$$P_\Theta << \mu_\Theta, \frac{d P_\Theta}{d \mu_\Theta}$$


	the posterior density from $\tau_\Theta(\theta)$ (the prior density) and the likelihood gives us, 

			$$\tau_{\Theta|X}(\theta|x) = \frac{f_{X|\Theta}(x|\theta) \tau_\Theta(\theta)}{\int_{\Omega_\Theta} f_{X|\Theta}(x|\theta) \tau_\Theta d \mu_\Theta(\theta)} $$

	So we have that 

		\begin{align*}
			P_{\Theta|X}(\{\theta_0\}|x) = \int_{\{\theta_0\}} \tau_{\Theta|x}(\theta|x) d\mu_\Theta(\theta) \\
				&=  \frac{\int_{\{\theta_0\}} f(x|\theta) \tau_{\Theta}(\theta) d\mu_\Theta(\theta)}{\int_{\Omega_\Theta} f_{X|\Theta}(x|\theta) \tau_\Theta d \mu_\Theta(\theta)} \\
		\end{align*}

By Lemma 2.1, 

		\begin{align*}
			\int_{\{\theta_0\}} f(x|\theta) \tau_{\Theta}(\theta) d\mu_\Theta(\theta) &= (1 - \epsilon) \int_{\{\theta_0\}} \dots d\nu_\Theta(\theta) + \epsilon \int_{\{\theta_0\}} \dots d \delta_{\theta_0}\\
			&= 0 + \epsilon f(x|\theta_0)\\
		\end{align*}

		\begin{align*}
			\int_{\Omega_\Theta} f_{X|\Theta}(x|\theta) \tau_\Theta d \mu_\Theta(\theta) &= (1 - \epsilon) \int f(x|\theta) \pi(\theta) d\mu(\theta) + \epsilon \int f(x|\theta) \pi(\theta) d \delta_{\theta_0}(\theta)
		\end{align*}

	Therefore, 

			$$P_{\Theta|X}(\{\theta_0\}|x)  =\frac{\epsilon f(x|\theta_0)}{(1-\epsilon) \int f(x| \theta) \pi(\theta) d \nu(\theta) + \epsilon f(x|\theta_0) }$$
\end{proof}

\textbf{Corollary 2.2} Consider setting

		$$H_0: \theta = \theta_0, H_1: \theta \neq \theta_0 $$

and action $\mathcal{A} = \{a_0, a_1 \}$ where we accept and fail to reject $H_0$, respectively. 

Suppose the loss function is 

		$$L(\theta, a) = \left\{ \begin{array}{ll}
			0 & (\theta, a) \in (\{\theta_0\} x \{a_0 \}) \bigcup (\{\theta_0\}^C x \{a_1\})\\
			C_0 & (\theta, a) \in (\{\theta_0\} x \{a_1 \}) \\
			C_1 & (\theta, a) \in (\{\theta_0\}^C x \{a_0\})\\
		\end{array}\right. $$

and the prior is 

		$$P_\Theta = (1 - \epsilon) Q_\Theta + \epsilon \delta_{\theta_0} $$

the Bayes' rule is 

		$$\frac{\epsilon f(x|\theta_0)}{(1-\epsilon) \int f(x| \theta) \pi(\theta) d \nu(\theta) + \epsilon f(x|\theta_0) } < \frac{C_1}{C_0 + C_1} $$


\begin{example}
	Suppose that 

			$$X_1, \dots, X_n | \theta \stackrel{iid}{\sim} Exp(\theta) $$

	Then the likelihood 

			$$f(x|\theta) = \theta^{-n} e^{- t(x)/ \theta}, \quad \theta > 0 $$
			$$t(x) = \sum^n_{i=1} X_i $$

	Suppose that $\theta \sim \tau \chi^{-2}_{(m)}$. Then (you may check) 

			$$\theta| \underline{X} \sim (2 t(x) + \tau) \chi^{-2}_{(m+2n)} $$


	Suppose we want to test 

			$$H_0: \theta \leq a, H_1: \theta > a $$

	The Bayes' rule: reject if

			$$P(\Omega_\Theta^{(1)}|x) > \frac{C_0}{C_0 + C_1} $$


	Note that 

			\begin{align*}
			 	P(\Omega_\Theta^{(1)}|x) = P(\theta > a| x)\\
			 		&=1 - P(\theta \leq a| x)\\
			 		&= 1 - P\left(\frac{\theta}{2 t(x) + \tau} \leq \frac{a}{2 t(x) + \tau} |x\right)\\
			 		&= 1 - F(\frac{a}{2 t(x) +\tau})
			 \end{align*} 

where F is the c.d.f. of $\chi^{-2}_{(2n+m)}$. 

We want to solve

		$$a = (2t(x) + \tau) F^{-1}(\frac{C_1}{C_0 + C_1}) $$


\end{example}



\textbf{Wednesday February 22}\\


Suppose we want to test 

			$$H_0: \theta =\theta_0, H_1: \theta \neq \theta_0 $$

Under the $0-C_0-C_1$ loss and the slab \& spike prior. 

	The Bayes' rule: reject if

			$$\frac{\epsilon f(x|\theta_0)}{(1-\epsilon) \int_{\Omega_\Theta} f(x| \theta) \pi(\theta) d \nu(\theta) + \epsilon f(x|\theta_0) } < \frac{C_1}{C_0 + C_1} $$

			$$f(x|\theta_0) = \theta_0^{-1}e^{-t(x)/\theta_0} $$

			$$\pi(\theta) = \tau \chi^{-2}_{(\nu)} \quad \text{ slab}$$

			$$\pi(\theta) = \frac{1}{\Gamma(\frac{\nu}{2}) 2^{\frac{\nu}{2}}} (\frac{\theta}{\tau})^{-\frac{\nu}{2} - 1} e^{- \frac{\tau}{2 \theta}} \tau^{-1} $$

	Take the integral from the rejection denominator, and using the information above

			\begin{align*}
				\int^\infty_{-\infty} \theta^{-n} e^{-t(x)/\theta}  \frac{1}{\Gamma(\frac{\nu}{2}) 2^{\frac{\nu}{2}}} (\frac{\theta}{\tau})^{-\frac{\nu}{2} - 1} e^{- \frac{\tau}{2 \theta}} \tau^{-1} d\theta &= \text{Algebra to isolate new pdf (integrates to 1)}\\
					&=\frac{\Gamma(\frac{2n + 2}{2})}{\Gamma(\frac{\nu}{2})} \tau^{\frac{\nu}{2}}(2 t(x) + \tau) \frac{2n + \nu}{2} 2^n
			\end{align*}

\section{Classificaiton}\index{Classification}

$\Omega_\Theta$ is a finite set true lables of classes. 

		$$\Omega_\Theta = \{1, \dots, k\} $$
		$$\Omega_A = =\{1, \dots, k\} $$

PHOTO
	
The loss function, 

	$L:\{1, \dots, k\} x \{1, \dots, k\}$

PHOTO

The $0 - C_0 - C_1$ is a sepcial case in this. 

PHOTO

The 0-1 loss is also a further special case where $C_{01} = C_{10} = 1 $


\begin{theorem}
	The Bayes' Rule for ($\Omega_\Theta, \Omega_A, L$) is 

			$$d_B(x) - \arg\min\left\{\sum^k_{\theta=1}C_{\theta a} f_{x|\theta}(x|\theta) \pi_\Theta(\theta): a = 1, \dots, k \right\} $$
\end{theorem}

\begin{proof}
	Because the los if inte we need to minimize, 

			$$\rho(x, a): a = 1, \dots, k $$

	Recall that, 

			\begin{align*}
				\rho(x, a) &= \E(L(\theta, a)|X)\\
					&= \sum^k_{\theta=1} L(\theta, a) \pi(\theta|x)\\
					&= \sum^k_{\theta = 1} C_{\theta a} \pi(\theta|x)\\
					&= \sum^k_{\theta = 1} C_{\theta a} f(x|\theta)\pi(\theta)
			\end{align*}

	Above, note that 

			$$\pi(\theta|x) \propto f(x|\theta) \pi(\theta) $$ 
\end{proof}

usually, there is a training set and a testing test. In the training set, we know the 'labels', whereas in the testing set we do not. 

We have for label $\theta$, 

		$$X_{\theta 1}, \dots, X_{\theta n_{\theta}} \stackrel{iid}{\sim} P_{X|\theta}$$

where $\theta = 1, \dots, k$. An example would be handwritting for letters/numbers. 

Generally, there are extra parameters $\Psi_\theta$ that determines the shape of clan $\theta$. 

		$$X_{\theta 1}, \dots, X_{\theta n_\theta} \sim P_{X|\theta, \Psi_\theta} $$

Most commonly used models for $P_{X|\theta, \Psi_\theta}$ is 

		\begin{enumerate}
			\item LDA - $N(\mu_\theta, \Sigma)$
			\item QDA - $N(\mu_\theta, \Sigma_\theta)$
		\end{enumerate}

The parameter, $\Phi_\theta$ is estimated from training sets, whereas $\pi(\theta)$ is estimated by 

		$$\frac{n_\theta}{\sum^k_{\theta = 1} n_\theta} $$

Plug these into the Bayes then you have LDA, QDA. So, under first model if we use UMVUE to estimate $\mu_\theta, \Sigma$, then 

		$$\hat{\mu_\theta} = \frac{1}{n_\theta} \sum^{n_\theta}_{i = 1} X_{\theta i} $$

		$$\hat{\Sigma} = \frac{1}{\sum^k_{\theta = 1} n_\theta - k} \sum^k_{\theta = 1} \sum^{n_\theta}_{i = 1} (x_{\theta i} - \hat{\mu}_\theta )(x_{\theta i} - \hat{\mu}_\theta)^T $$

Plug in Bayes rule and we get LDA. 

For the second model, 

		$$\hat{\mu_\theta} = \frac{1}{n_\theta} \sum^{n_\theta}_{i = 1} X_{\theta i} $$

		$$\hat{\Sigma} = \frac{1}{n_\theta - 1} \sum^k_{\theta = 1} \sum^{n_\theta}_{i = 1} (x_{\theta i} - \hat{\mu}_\theta )(x_{\theta i} - \hat{\mu}_\theta)^T $$

Plug this into Bayes rule and get QDA. 


Suppose we make a plot using QDA, 

		PHOTO


\section{Stein's Estimate}{Stein's Estimate}

Stein (1956) shows that if the dimension is greater than 3 and sample size is 1, then MLE is not admissible. 

\textbf{Friday February 24}\\

\textbf{Lemma 2.2} Suppose that 
		
		$$X \sim N(\mu, \sigma^2), g:\mathbb{R} \rightarrow \mathbb{R} $$

where g is differentiable, 

		$$\E |g(x)| < \infty $$

Then we have that

		$$\Cov (X, g(X)) = \Var (x) \E g^\cdot(x) $$


\begin{proof}
	First we assume that 

			$$X \sim N(0, 1) $$

	Let $\phi$ be the pdf of $N(0, 1)$. Recall that 

			$$\phi(x) = - x \phi(x)$$

			$$\E[g^\cdot(x)] = \int^\infty_{-\infty}g^\cdot(x) \phi(x)  dx $$


	Let $a \in \mathbb{R}$, then this integral above becomes 

			\begin{align*}
				\int^a_{-\infty}g^\cdot(x) \phi(x)  dx + \int^\infty_{a}g^\cdot(x) \phi(x)  dx &= \int^a_{-\infty}g^\cdot(x) (\int_{-\infty}^x  \phi^\cdot(z) dz)  dx + \int^\infty_{a}g^\cdot(x) (\int_{x}^\infty  \phi^\cdot(z) dz)   dx\\
					&=\int^a_{-\infty} \int_{-\infty}^x  g^\cdot(x)   \phi^\cdot(z) dz  dx + \int^\infty_{a}  \int_{x}^\infty g^\cdot(x)   \phi^\cdot(z) dz   dx\\
					&= \int^a_{-\infty} \int_{z}^a  g^\cdot(x)   \phi^\cdot(z) dx  dz + \int^\infty_{a}  \int_{z}^a g^\cdot(x)   \phi^\cdot(z) dx   dz\\
					&=\int^a_{-\infty} \phi^\cdot(z) (g(a) - g(z))dz + \int_a^\infty \phi^\cdot(z)((g(z) - g(z)) dz)\\
					&=\int^\infty_{-\infty} \phi^\cdot(z)(g(a) - g(z)) dz\\
					&= g(a) \int^\infty_{-\infty} \phi^\cdot(z) dz - \int^\infty_{-\infty} \phi^\cdot(z)g(z) dz\\
					&= \int z g^\cdot(z) \phi(z) d z \\
					&= \E (X g(x))\\
					&=\Cov(X, g^\cdot(x))\\
					&= E(g^\cdot(x))
			\end{align*}


	More generally, if $X \sim N(\mu, \sigma^2)$ then $X = \sigma Z + \mu$ where $Z \sim N(0, 1)$.


			$$\Cov (X, g(X)) = \sigma \Cov(Z, g_1(z))$$

	Note that

	\begin{align}
		g_1 (z) &= g(\sigma z + \mu)\\
		&= \sigma \E(g^\cdot_1(z))\\
		&= \sigma \E(\sigma g^\cdot_1(\sigma Z + \mu))\\
		&= \sigma^2 \E (g^\cdot(\sigma Z + \mu))\\
		&= \sigma^2 \E(g^\cdot(x))\\
		&= \Var (x) \E g^\cdot(x)
	\end{align}




\end{proof}


\textbf{Lemma 2.3} (HW \#4). If 

		$$X \sim N(\mu, \Sigma), g: \mathbb{R}^P \rightarrow \mathbb{R}^p $$

where g is differentiable such that $\frac{\partial g}{\partial x^T}$ has integrable components. Then 

		$$\Cov(X, g(X)) = \Sigma \E \left(\frac{\partial g^T(X)}{\partial X}\right) $$


\begin{theorem}[2.8]
	Suppose that 

			$$X \sim N(\theta, I_P) $$

	where $P \geq 3$. 

	Let 

			$$ L(\theta, a ) = ||\theta - a ||^2$$

	and d(x) = X. Then we have that d is inadmissable. ("Very weird")
\end{theorem}


\begin{proof}
	Let $d_1(x) = (1 - g(X)) X$ where $h: \mathbb{R}^P \rightarrow \mathbb{R}$ is to be specified later. 

		\begin{align*}
			R(\theta, d_1) &= \E ||\theta - d_1(X)||^2 \\
				&= \E ||X - \theta - h(x) X ||^2\\
				&= \E||X - \theta||^2 - 2\E [(X- \theta)^T h(X)X] + \E [h^2(X) ||X|]\\
				&= \E||X - \theta||^2 - 2 tr[\Cov(X, h(X)X)] + \E [h^2(X) ||X|]\\
				&= \E||X - \theta||^2 - 2 tr [I_P \E \frac{\partial h(X)}{\partial X}] + \E [h^2(X) ||X|]\\
				&=\E||X - \theta||^2 - 2\E(X^T (\frac{\partial h}{\partial X} + ph(X))) + \E [h^2(X) ||X|]\\
				&=R(\theta, d) -  2\E(X^T (\frac{\partial h}{\partial X} + ph(X))) + \E [h^2 (x) ||X||^2]
		\end{align*}

Let $h(x) = \frac{\alpha}{||x||^2}$. Then 

		\begin{align}
			\frac{\partial h}{\partial X} &= \alpha \frac{\partial}{\partial X} (||X||^2)^{-1} \\
				&=\alpha(-1)(||X||^2)^{-2} \frac{d X^T X}{d X}
				&= -\alpha ||X||^{-4} 2x\\
				&= -\alpha 2X ||X||^{-2}
		\end{align}


So if we plug this into equations above we get 

		$$||X||^{-2} \alpha(4 - 2P + \alpha) $$

So we have that

		$$R(\theta, d_1) - R(\theta, d) = \alpha(4 - 2 P + \alpha ) \E ||X||^2 $$

If $4 - 2 P + \alpha < 0$ then d is inadmissible.

Any $0 < \alpha < 2P - 4$ makes this happen. Note we need $P \geq 3$. 
\end{proof}

\textbf{Monday February 27}\\

We've said that

		$$(1 - \frac{\alpha}{||X||^2})X $$

is better than X for $0 < \alpha < 2p - 4, p \geq 3$. So if we take $\alpha = p - 2$ then we have

		$$(1 - \frac{p - 2}{||X||^2})X $$

which is called the James-Stein estimator. 

\section{Empirical Bayes}\index{Empirical Bayes}

Somestimes we have parameters in $\Pi$ such as 

		$$X|\mu \sim N(\mu, 1) $$

		$$\mu \sim N(a, \phi) $$

Where before we treated a and $\phi$ as known hyperparameters. 

For Empirical Bayes these parameters will also be estimated, $\pi(\theta|b)$.

The likelihood is 

		$$\pi(\theta|x) \propto f(x|\theta) \pi(\theta|b)$$

We can integrate

		$$f(x|b) = \int f(x|\theta)\pi(\theta|b) d\mu_\theta $$

and treat this as our likelihood with b as the parameter. So then you can estimate b by frequentist methods, such as MLE or UMVUE. Once you have $\hat{b}$ you can plug it into the prior, $\pi(\theta|b)$. With the 'pretend' prior we may carry out Bayesian analysis, such as Bayes rule, generalized MLE, etc. This precess is called Empirical Bayes. 


 \begin{example}
 	Let $\theta = \theta_1, \dots, \theta_P$.  And that $X_1, \dots, X_P|\theta $. Assume that

 			\begin{enumerate}
 				\item $\theta_1 \indep \dots \indep \theta_P$
 				\item $X_i|\theta_1, \dots, \theta_P \sim N(\theta_i, 1) $

 			This condition also implies that

 			$$X_i \indep \theta^{-i} | \theta^i $$
 			
 				\item $\theta_i \sim N(0, b)$ 
 			\end{enumerate}

 	Here the likelihood is, 

 				$$f(\underline{X}|\underline{\theta}) = f(X_1|\underline{\theta}) \cdots f(X_P|\underline{\theta}) = f(X_1|\theta_1) \cdots f(X_P|\theta_P) $$

 	And our prior becomes

 			
 			$$\pi(\underline{\theta}|b) = \pi(\theta_1|b) \cdots \pi(\theta_P|b)$$

 	We may integrate to get $f(x|\theta)$

 			\begin{align}
 				\int f(x|\theta)\pi(\theta|b) d\theta &= \int \dots \int f(x_1|\theta_1) \cdots f(X_P|\theta_P) \pi(\theta_1|b) \cdots \pi(\theta_P|b) d\theta\\
 					&=\int f(X_1|\theta_1) \pi(\theta_1|b) d\theta_1 \cdots \int f(X_P|\theta_P) \pi(\theta_P|b) d\theta_P\\
 					&= f(X_1|b)\cdots f(X_P|b)
 			\end{align}

 	We weill derive UMVUE for b. Before that, we first will do Bayes rule for $\theta$ under $||\theta - a||^2$. 

 			$$d_B(\theta) = E(\theta|X) = \begin{pmatrix}
 				E(\theta_1|X_1, \dots, X_P)\\
 				\vdots\\
 				E(\theta_1|X_1, \dots, X_P)
 			\end{pmatrix} $$

 	Where

 			$$E(\theta_i|X_1, \dots, X_P) = \int \theta_i \pi(\theta_i|X_1, \dots, X_P) d\theta_i $$

 			\begin{align*}
 				\pi(\theta_i|\underline{X}) &= \frac{f(\underline{X}|\theta_i) \pi(\theta_i)}{f(\underline{X})} \\
 					&=\frac{f(X_1|\theta_i) \cdots f(X_P|\theta_i) \pi(\theta_i)}{f(\underline{X})}
 			\end{align*}

 	Note that conditional indepedence satisfies 4 axioms (see below). 
 			
 \end{example}


\textbf{Axioms of Conditional Independence}\\

		$$X \indep Y | Z $$

	Visualized as:

		$$\textcircled{X} - \textcircled{Z} - \textcircled{Y}$$

 Where X only can "communicate" with Y through Z. 

 \begin{enumerate}
 	\item $X \indep Y | Z \Rightarrow Y \indep X | Z$
 	\item $X \indep (Y, Z) |W \Rightarrow X\indep Y | W$ 
 	\item $X \indep (Y, Z) |W \Rightarrow X \indep Z | (W, Y)$
 	\item $X \indep W | (Y, Z) \& X \indep Y|Z \Rightarrow X \indep (Y, W) |Z$
 \end{enumerate}

\textbf{Wednesday March 1}\\

MIDTERM EXAM - Content up to (and including) Section 2.2. Note: 2016 Midterm covered more content. 

 \textbf{Friday March 3}\\

\begin{example}
	
	$$X_1, \dots, X_P |\theta_1, \dots, \theta_P \sim N(\theta_i, 1)$$
	$$\theta_i \sim N(0, b) $$

where, 

		$$\theta_1 \indep \dots \indep \theta_P $$

and b is known. 

Bayes rule: $\E (\theta_i| \underline{X})$\\

So we know that 

		\begin{align*}
		\pi(\underline{\theta}|\underline{X}) &= \frac{f(\underline{X}|\underline{\theta}) \pi(\underline{\theta})}{f(\underline{X})}	\\
			&= \frac{f(X_1|\theta_1) \pi(\theta_1)}{f(X_1)} \cdots \frac{f(X_P|\theta_P) \pi(\theta_P)}{f(X_P)}
		\end{align*}		
\end{example}

Now we can find, 

		\begin{align*}
			\E (\theta_i|\underline{X}) &= \int \cdots \int \theta_i \pi(\underline{\theta}|\underline{X}) d\underline{\theta}\\
				&=\int \cdots \int \theta_i \pi(\theta_1|X_1) \cdots \pi(\theta_P|X_P) d\underline{\theta}\\
				&= \int \theta_i \pi(\theta_i|X_i) d\theta_i\\
				&= \E (\theta_i|X_i)
		\end{align*}

But, 

		$$X_i |\theta_i \sim N(\theta_i, 1) $$
		$$\theta_i \sim N(0, b) $$
		$$\theta_i |X_i \sim N\left(\frac{X_i}{\frac{1}{b} + 1}, \frac{1}{\frac{1}{b} + 1} \right) $$

Using Bayes' Rule, 

		$$\E (\theta_i | X_i) = \frac{X_i}{\frac{1}{b}+ 1} = (1 - {\frac{1}{1 + b}}) X_i $$

and if we know b, 
		$$d_B(x) = \begin{pmatrix}
			\frac{X_1}{b^{-1} + 1}\\
			\vdots\\
			\frac{X_P}{b^{-1} + 1}\\
		\end{pmatrix} $$


We want to estimate b using UMVUE of $\frac{1}{b + 1}$. 

Note that $X_i \sim N(0, b + 1)$. So, a complete, sufficient statistic would be

		$$S = \sum^n_{i = 1} X_i^2 $$

We know that

		$$\frac{S}{b+1} \sim \chi^{2}_P $$

and the inverse, 

		$$\frac{b + 1}{S} \sim \chi^{-2_P} $$

where we know the expectation is $\frac{1}{P- 2}$, rearranging we get 

		$$\E (\frac{p - 2}{S}) = \frac{1}{b+1}$$

giving us the UMVUE of $\frac{1}{b+ 1}$. Plugging this into Bayes' rule, 

		$$d_B(x) = (1 - \frac{p - 2}{||X||^2})X $$

Note that this is the James Stein estimate. 



% %----------------------------------------------------------------------------------------
% %	CHAPTER 3
% %----------------------------------------------------------------------------------------

\chapterimage{chapter_head_1.pdf} % Chapter heading image

\chapter{Testing Hypothesis for Simple-VS-Simple}

\section{Basic Concepts}\index{Basic Concepts}

$(\Omega, \mathcal{F})$ is measurable space. \\
$(\Omega, \mathcal{F}_X)$ is measurable space. \\
$X: \Omega \rightarrow \Omega_X \textcircled{m} \mathcal{F} \setminus \mathcal{F}_X$\\


Let $\mathcal{P}_0, \mathcal{P}_1$ be two disjoint families of probability measure on $(\Omega, \mathcal{F})$. 

P is the true distrubiton. Assume that $P \in \mathcal{P}_0 \cup \mathcal{P}_1 = \mathcal{P}$

\textbf{Hypothesis Test}\\

		$$H_0: P \in \mathcal{P}_0, H_1: P \in \mathcal{P}_1 $$

We want to decide whether to reject $H_0$. \\


If everything is deterministic, 

		$$A:Theory \Rightarrow B_{Obs} $$

So if B doesn't happen, then A is false. 

But in non-deterministic world, 

		$$A:\text{True} \Rightarrow P(\text{B occurs}) = \text{large} $$
		$$A:\text{Prob not True} \Leftarrow \text{B doesn't happen} $$

Stasticial Inference!

We reject $H_0$ if X falls in a certain region, $C \subseteq \Omega_X$. This region of rejection is called the...rejeciton region. 

\begin{definition}
	Let 

			$$ \alpha = \sup_{P \in \mathcal{P}_0} P(X \in C) $$

	be the Type I error. 

			$$\beta(P) = P(X \in C), P \in \mathcal{P}_1 $$

	is the Power at P. Note that

			$$1 - \beta(P) $$

	is the Type II error.
\end{definition}


If we introduce, 

		$$\phi: \Omega_X \rightarrow \{0,1\} $$
		$$ \phi(x) = \mathbb{I}_{\{C\}}(X) $$

then $P(X \in C) = \E (\phi(x))$. 

So, more generally, we don't have to restrict range of $\phi$ to {0, 1}. 

		$$\phi: \Omega_X \rightarrow [0, 1] $$

So, $\phi(x)$ is the probability of rejection given X. 

In this general setting the function 

\textbf{Monday March 13}\\

\section{Neyman-Pearson Lemma}\index{Neyman-Pearson Lemma}

Scenario: You are in a boat arriving at Treasure Island. Your boat can carry 100 pounds of bounty, but no more. On the island there are 1 lb units of gold, diamonds, watermelon, and bananas. Clearly, with each unit being equal in weight you want to take the most valuable goods. Likely diamonds, then gold, then the food. If they are not equal weight you still want to determine which is the most valuable per unit weight. 

This is the essence of the Neyman-Pearson Lemma with analogies to likelihood and rejection regions. 

Weight is analogus to null probability. \\
Value is to power. \\
Weight limit is to alpha.\\


When we test simple versus simple hypothesis, 

		$$H_0: P = P_0 \quad H_1: P = P_1 $$

where $\mathcal{P}_0  = \{P_0\}, \mathcal{P}_1 = \{P_1\}$. 


\begin{definition}
	A test $\phi$ for the simple versus simple hypothesis is Most Powerful size $\alpha$ test (MP-$\alpha$) if

	\begin{enumerate}
		\item $\E _{P_0} \phi(x) = \alpha$
		\item If $\phi'$ is level $\alpha$. (i.e $\E _{P_0} \phi' \leq \alpha$)
	\end{enumerate}

	then $\E _{P_1} \phi \geq \E _{P_1} \phi'$
\end{definition}

We can assume that $P_0, P_1$ are dominated by a measure $\mu$. For example, let 

	$$\mu = P_1 + P_2 $$

	then, $P_1 << \mu, P_2 << \mu$.

\textbf{Lemma 3.1} (Neyman-Pearson: Optimality) Let $f_0, f$ be $\frac{d P_0}{d \mu}, \frac{d P_1}{d \mu}$. Then for any test of the form: 

		$$ \phi(x) = \left\{ \begin{array}{ll}
			1 & f_1(x) > k f_0(x)\\
			0 & f_1(x) < k f_0(x)\\
		\end{array} \right.$$

where $K \geq 0$ is MP-$\alpha$ where $\alpha = \E_{P_0} \phi$.


\begin{proof}
	Let $\phi'$ be any test of level $\E_{P_0} \phi$. We want to show that

			$$\E_{P_1} \phi \geq \E_{P_1} \phi' $$

	Consider the formula, 

			$$(\phi - \phi')(f_1 - k f_0) $$

	and look at which term is postive or negative and note that together the terms are always positive. 

	So, 

			\begin{align*}
				\int (\phi - \phi')(f_1 - k f_0) d\mu &> 0\\
				\int (\phi - \phi')f_1 d\mu &\geq k \int (\phi - \phi')f_0 d\mu\\
				\int (\phi - \phi') f_1 d\mu &\geq k \E_{P_0} \phi - \E_{P_0} \phi'\\
				\E_{P_0} \phi &\geq \E_{P_0} \phi
			\end{align*}

\end{proof}

\textbf{Existence \& Uniqueness Part of N-P Lemma}\\

\textbf{Proposition 3.1} Suppose that $\rho:\mathbb{R} \rightarrow \mathbb{R}$ is right continuous, nonincreasing such that

		\begin{enumerate}
			 \item $\rho(0-) = 1$
			 \item  $\lim_{x \rightarrow \infty} \rho(x) = 0$
		\end{enumerate}

Then for any $0 < \alpha < 1$ there exists $k \in [0, \infty]$ such that $\alpha \in [\rho(k), \rho(k-)]$. 

We may think of this as a discontinuous version of intermediate value theorem. 

\begin{proof}
	If $\alpha \equiv 1$ then we may take $k = 0$ and then 

			$$\rho(0-) = 1 \geq \rho(0) $$

	Now we may assume that $0 < \alpha < 1$. Let $S_\alpha = \{x : \rho(x) \leq \alpha \}$. Let $k = \inf S_\alpha$. Now, because $\alpha > 0, \rho(x) \rightarrow 0, S_\alpha \neq 0$ for any $ 1> \alpha > 0, 0 < \inf S_\alpha < \infty$. Let $\{x_n \} \subset S_\alpha$ such that $x_n \downarrow k$ by right continuity, $\rho(x_n) \rightarrow \rho(k)$. Since we have that $\rho(k) \geq \alpha$ for all n, this implies that $\rho(k) \leq \alpha$. 

	Since $\rho(x) < \alpha$ for all $x < k$, then $\rho(k-) \geq \alpha$. 
\end{proof}
 

 \begin{theorem}
 	For any $0 < \alpha \leq 1$ there exists a test $\phi$ of size $\alpha$ for the NPL form, i.e., 

 			$$\phi(x) = \left\{\begin{array}{ll}
 				1 & f_1 > k f_0\\
 				0 & f_1 < k f_0\\
 			\end{array} \right. $$

 	such that $\phi(x) \equiv \gamma$ for $f_1 = k f_0$. For some $\gamma \in [0, 1], 0 \leq k < \infty$ (the existance part of NPL).
 	
 	Further more, if $\phi'$ is MP-$\alpha$ then it has to have the form above (almost surely $P_0, P_1$). 		
 \end{theorem}

 \textbf{Wednesday March 15}\\

 \begin{proof}
 	Existance. For a test of the form: 

 			$$\phi(x) = \left\{\begin{array}{ll}
 				1 & f_1 > k f_0\\
 				\gamma & f_1 = k f_0\\
 				0 & f_1 < k f_0\\
 			\end{array} \right. $$

 			\begin{align*}
 				\E_{P_0} \phi(x) &= \int_{f_1 > k f_0} \phi f_0 d\mu + \int_{f_1 = k f_0} \phi f_0 d\mu + \int_{f_1 < k f_0} \phi f_0 d\mu\\
 					&=P_0(f_1 > k f_0) + \gamma P_0(f_1 = k f_0) + 0\\
 			\end{align*}

 	Let 

 			\begin{align*}
 				\rho(k) &= P_0(f_1 > k f_0)\\
 					&= P_0(f_1 > k f_0, f_0 > 0)\\
 					&= P_0 (\frac{f_1}{f_0} > k, f_0> 0)
 			\end{align*}

 	We claim that $\rho(k)$ satisfies the four conditions in Proposition 3.1

 			\begin{itemize}
 				\item Right Continuous\\

						Let $k_n \downarrow k$, 

								$$\bigcup^\infty_{n = 1} \{f_1 > k_n f_0\} = \{f_1 > k f_0\} $$ 
								
						And by continuity of probability, 

								$$P_0( f_1 > k_n f_0) \rightarrow P_0(f_1 > k f_0) $$

				\item Nonincreasing
				\item $\rho(0-) = 1$

						Let $k_n \uparrow 0$. Then

								$$\bigcap^\infty_{n = 1} \{f_1 > k_n f_0\} = \{f_1 > 0\} $$ 
								
						And by continuity of probability, 

								$$P_0( f_1 > k_n f_0) \rightarrow P_0(f_1 > 0) $$
						So,
								$$\rho(0-) = 1 $$ 				

				\item $\lim_{k \rightarrow \infty} \rho(k) = 0$

						Let $k_n \rightarrow \infty$. Then

						$$\bigcap^\infty_{n = 1} \{f_1 > k_n f_0\} = \emptyset$$ 
								
						And by continuity of probability, 

								$$P_0( f_1 > k_n f_0) \rightarrow P_0(\emptyset) $$ 	
						So,
								$$\rho(k_n) \rightarrow 0 $$ 	
 			\end{itemize}

So by Proposition 3.1, there exists $0 \leq k_0 < \infty$ such that

		$$\rho(k_0) \leq \alpha \leq \rho(k_0-) $$

which is to say, 

		$$P_0(f_1 > k f_0) \leq \alpha \leq P_0(f_1 \geq k_0 f_0) $$

Let 

		$$\gamma = \left\{\begin{array}{ll}
			\frac{\alpha - P_0(f_1 > k_0 f_0)}{P_0(f_1 = k_0 f_0)} & P(f_1 = k_0 f_0) = 0\\
			0 & o.w.\\
		\end{array} \right. $$

Now let 

		$$\phi(x) = \left\{\begin{array}{ll}
 				1 & f_1 > k f_0\\
 				\gamma & f_1 = k f_0\\
 				0 & f_1 < k f_0\\
 			\end{array} \right. $$

 	If $P_0(f_1 = k f_0) > 0$ then, 

 			\begin{align*}
 				\int \phi f_0 d\mu &= P_0(f_1 > k f_0) + \frac{\alpha - P_0(f_1 > k_0 f_0)}{P_0(f_1 = k_0 f_0)}\\
 					&=\alpha
 			\end{align*}

 	And if $P_0(f_1 = k f_0) = 0$ then, $\gamma = 0$ and thus,

 			\begin{align*}
 				\int \phi f_0 d\mu = P_0(f_1 > k f_0) \\
 				 	&= P_0(f_1 \geq k f_0)	\\
 					&=\alpha
 			\end{align*}

 And now for the uniqueness part. 

 First let us understand what is the meaning of $\phi'$ with the form, 

 			$$\phi'(x) = \left\{\begin{array}{ll}
 				1 & f_1 > k f_0\\
 				0 & f_1 < k f_0\\
 			\end{array} \right. $$
 almost surely $P_0, P_1$. 

 This means that $\phi' = \phi$ on $f_1 \neq k f_0$ almost surely. Then 

 		$$P_{0,1}(\{\omega: f_1(\omega) \neq k f_0(\omega) \rightarrow \phi'(\omega) = \phi(\omega) \}) = 1 $$

 		$$P_{0,1}(\{\omega: f_1(\omega) \neq k f_0(\omega) \rightarrow \phi'(\omega) \neq \phi(\omega) \}) = 0 $$

So we want to show that 

		$$P_{0, 1}( f_1 \neq k f_0, \phi' \neq \phi) = 0 $$

Since $\phi'$ is another MP-$\alpha$ tst

		$$\int \phi' f_0 d\mu = \int \phi f_0 d\mu = \alpha $$

Because both are MP, 

		PHOTO


 \end{proof}


 \section{Uniformly Most Powerful Test for One-Sided Hypothesis}\index{UMP Test - One Sided}

 \subsection{General Definition}

 $(\Omega, \mathcal{F}, \mathcal{P})$ is our probability measure where

 		$$\mathcal{P} = \mathcal{P}_0 \cup \mathcal{P}_1,  \mathcal{P}_0 \cap \mathcal{P}_1 = \emptyset$$

 We hypothesis that

 		$$H_0: P \in \mathcal{P}_0 \quad H_1: P \in \mathcal{P}_1 $$

 \begin{definition}
 	A test $\phi$ for 

 	 		$$H_0: P \in \mathcal{P}_0 \quad H_1: P \in \mathcal{P}_1 $$

is UMP of size $\alpha$ (UMP-$\alpha$) if 

		\begin{enumerate}
			\item it is of size $\alpha$ 

					$$\sup_{P \in \mathcal{P}_0} \int \phi dP = \alpha $$

			\item if $\phi'$ is ove level $\alpha$, 

					$$\sup_{P \in \mathcal{P}_0} \int \phi' dP \leq \alpha $$

		then we have that

				$$\int \phi dP \geq \int \phi' dP \quad \forall P \in \mathcal{P} $$

		In the parametric setting, 

				$$\mathcal{P} = \{P_\theta : \theta \in \Theta\} $$

				$$\Theta = \Theta_0 \cup \Theta_1, \Theta_0 \cap \Theta_1 = \emptyset $$

				$$H_0: \theta \in \Theta_0 \quad H_1: \theta \in \Theta_1 $$

			\begin{enumerate}
				\item $\sup_{\theta \in \Theta_0} \int \phi d P_\theta$
				\item $$\int \phi dP_\theta \geq \int \phi' dP_\theta \quad \forall \theta \in \Theta $$
			\end{enumerate}
		\end{enumerate}
 \end{definition}


 \subsection{Monotone Likelihood Ratio Assumption}

 \begin{definition}
 	Let $\mathcal{P} = \{P_\theta: \theta \in \Theta \}$ be a one parameter family of probability measures, dominated by a $\sigma$-definite measure $\mu$. Also let 

 			$$f_\theta = \frac{d P_\theta}{d \mu} $$

 	The family of densities, 

 			$$\{f_\theta: \theta \in \Theta\} $$

 	has Monotone Likelihood Ratio (MLR) in Y(x) if for any $\theta_1 < \theta_2$. 

 			$$L(X) = \frac{f_{\theta_2}(x)}{f_{\theta_1}(x)} $$

 	is monotone (nondecreasing or nonincreasing) funciton of Y(x) on the set on which Y(x) is defined. 
 \end{definition}

 \textbf{Friday March 17}\\

 GET NOTES FROM DHANUSHI

 \textbf{Monday March 20}\\

 \subsection{General Form of UMP-Test for One-Sided Hypothesis}

 		$$H_0: \theta \leq \theta_0 \quad H_1: \theta \geq \theta_0 $$

 		$$ H_0: \theta \geq \theta_0 \quad H_1: \theta \leq \theta_0$$

 \begin{theorem}[3.2 in class]
 	Suppose that $\{f_\theta: \theta \in \Theta \subseteq \mathbb{R}\}$ has MLR (without loss of generality, nondecreasing in Y(x)). 

 	\begin{enumerate}
 		\item For $\alpha < 0$ the test of the form

 				$$\phi(x) = \left\{ \begin{array}{ll}
 					1 & Y(x) > k\\
 					\gamma & Y(x) = k\\
 					0 & Y(x) < k
 				\end{array}\right.$$

 		combined with,  $\int \phi f_{\theta_0} d\mu = \alpha$ is UMP-$\alpha$ for testing (I).

 		\item If $\alpha < 1$ then the test,

				$$\phi'(x) = \left\{ \begin{array}{ll}
 					1 & Y(x) > k'\\
 					\gamma' & Y(x) = k'\\
 					0 & Y(x) < k'
 				\end{array}\right.$$ 	

 		combined with $\int \phi' f_{\theta_0} d\mu = \alpha$ is UMP-$\alpha$ for testing (II).			
 	\end{enumerate}
 \end{theorem}

 The idea is that you want the smoothed $\E_\theta \phi$ to be extreme. So you want $\phi$ to be as extreme as possible. 

 \begin{proof}
 	Step 1. Show test (I) is UMP for testing

 			$$H_0: \theta = \theta_0 \quad H_1: \theta > \theta_0 $$

 	It suffices to show this test is of N-P form for any $\theta > \theta_0$. 

 	So we wnat to show that test I is of the form, 


 		$$\phi_1(x) = \left\{ \begin{array}{ll}
 					1 & f_\theta(x) > k_1 f_{\theta_0} (x)\\
 					0 & f_\theta(x) < k_1 f_{\theta_0} (x)
 				\end{array}\right.$$

 for any $\theta < \theta_0$. 

 Because $\{f_\theta: \theta \in \Theta\}$ has MLR, $\{x: Y(x) \leq k\} \subseteq \{x: L_0(Y(x)) \leq L_0(k)\}$. Thus, $L_0(Y(x)) = L(x)$. 

 PHOTO

 For the same reason, $\phi$ is 0 on $\{x: Y(x) < k\}$. So $\phi$ has form $\phi_1$. 

 Step 2. Want to show that $\phi$ has size $\alpha$ because $\phi$ is of form I, nondecreasing in $Y(x)$. 

 So $\beta_\phi(\theta)$ is nondecreasing in $\theta$. 

 		$$\sup_{\theta \leq \theta_0} \beta_\phi (\theta) = \beta_\phi(\theta_0) = \alpha $$


 Step 3. Show that $\phi$ is UMP-$\alpha$ for 

 		$$H_0: \theta \leq \theta_0 \quad H_1: \theta > \theta_0 $$

 We know that $\phi $ is UMP among all tests such that $\E_{\theta_0} \phi' \in \alpha$ (**) by Step 1. 

 Now we want $\phi$ to be UMP among all $\phi'$ such that 

 		$$\sup_{\theta \leq \theta_0} \E_\theta \phi'(x) \leq \alpha $$

 But (**) is true because statement above implies (**). 

 The second part of the proof follows by considering 

 			$$\phi = 1 - \psi $$

 where $\psi$ is the first test form with size $1 - \alpha$. 

 \end{proof}

 \subsection{Properites of UMP-$\alpha$ Test for One Sided Hypotheses}

 Intuitively, UMP property tells us that the test's (I) power most extreme on higher side.

 PHOTO

 But it also has to be most extreme on other side.

 \textbf{Corollary 3.1} Suppose that $0 < \beta_\phi (\theta_0) <1$. If $\phi$ is of the I form, then for any $\psi$ satisfying

 		$$\beta_\psi (\theta_0) \geq \beta_\phi(\theta_0) $$

 we have 

 		$$ \beta_\phi (\theta) \leq \beta_\psi(\theta) $$

 \begin{proof}
 	From the proof of Theorem 3.2, $1 - \phi$ is UMP for testing form (II) and has size 

 			$$\beta_{1 - \phi}(\theta_0) = \E_{\theta_0} (1 - \phi) = 1 - \beta_\phi(\theta_0) $$

 	Thus, for $\theta< \theta_0$,

 			\begin{align*}
 				\beta_{1 - \phi}(\theta) &\geq \beta_{1 - \psi}(\theta)\\
 				1 - \beta_{\phi}(\theta) &\geq 1 - \beta_{\psi}(\theta)\\
 				\beta_{\phi}(\theta) &\leq \beta_{\psi}(\theta)\\
 			\end{align*}
 \end{proof}

 Next property says that if we have test of form (I) then under MLR and indentifiability, then $\beta_\phi(\theta)$ is strictly increasing. 

 \begin{theorem}[3.4]
 	Suppose that $\{f_\theta:\theta \in \Theta \subseteq \mathbb{R}\}$ has MLR (nondecreasing in Y(x)) and is identifiable. Let us use $\phi$ of form I such that

 			$$\E_{\theta_0} \phi(x) = \alpha $$

 	then $\beta_\phi (\theta)$ is strictly increasing on the set $\{\theta: 0 < \beta_\phi(\theta) < 1\}$. 
 \end{theorem}

 \begin{remark}
 	Review of identifiability. Consider a family of probability measures, 

 			$$\{P_\theta: \theta \in \Theta\} $$

 	This family is identifiable if whenever $\theta_1 \neq \theta_2$, then $\P_{\theta_1} \neq P_{\theta_2}$. 


 \end{remark}


 Suppose GET NOTES


\textbf{Wednesday March 22}\\

Let $\theta_1 < \theta_2$. As proved in Theorem 3.2, $\phi$ is of form (\_.-):

		$$\phi(x) = \left\{ \begin{array}{ll}
			1 & f_{\theta_2} < k' f_{\theta_1}\\
			0 & f_{\theta_2} > k' f_{\theta_1}\\
		\end{array}\right. $$

for some $0 \leq k' \leq \infty$. 

So by NPL, $\phi$ is MP test for 

		$$H_0: \theta_1 \quad H_1: \theta_2 $$

and has size $\beta_\phi(\theta_1)$. We know that

		$$\beta_\phi(\theta_2) \geq \beta_\phi (\theta_1) $$

So we need to show 

		$$\beta_\phi(\theta_2) \neq \beta_\phi(\theta_1) $$


Suppose that $\beta_\phi(\theta_2) = \beta_\phi(\theta_1) $ then let $\phi^*(x) \equiv \beta_\phi(\theta_1)$. Then

		$$(\phi - \phi^*)(f_{\theta_2} - k' f_{\theta_1)}) \geq 0 $$

		$$\int (\phi - \phi^*)(f_{\theta_2} - k' f_{\theta_1)}) d\mu $$

But since we know that the integral is zero, so much the integrand (inside part) be zero. This, combined with $0 < \beta_\phi(\theta_1) < 1$ gives us that

		$$\phi \neq \phi^* \quad f_{\theta_2} \neq k' f_{\theta_1} $$


So $\{f_{\theta_2} \neq k' f_{\theta_1} \} \subseteq \{(\phi - \phi^*) (f_{\theta_2} - k; f_{\theta_1}) \neq 0 \}$. Note that

		$$\mu(f_{\theta_2} \neq k' f_{\theta_1}) = 0 $$

GET NOTES


\section{Uniformly Most Powerful Unbiased Test - 2 Sided Hypothesis}\index{UMPU Test - 2 sided Hypothesis}

Suppose we want to test


		$$H_0: \theta_1 \leq \theta \leq \theta_2 \quad H_1: \theta > \theta_2, \theta < \theta_1  $$

PHOTO

Bing Li's Notation: -.\_.-\\


2nd Case

    	$$ H_0: \theta \geq \theta_2, \theta \leq \theta_1 \quad H_1: \theta_1 < \theta < \theta_1$$

Notation: \_.-.\_ \\



We are going to study three types of 2 sided hypotheses

		\begin{enumerate}[label = \Roman*]
			\item $H_0: \theta = \theta_0 \quad H_1: \theta \neq \theta_0$
			\item $H_0: \theta_1 \leq \theta \leq \theta_2 \quad H_1: \theta > \theta_2, \theta < \theta_1 $
			\item $ H_0: \theta \geq \theta_2, \theta \leq \theta_1 \quad H_1: \theta_1 < \theta < \theta_1$
		\end{enumerate}

The most commonly used is I (then II then III). But the most fundamental proof is III so we'll work backwards to I. 

First we show, in the case of I and II that UMP does not exist. 

\begin{example}
	(No UMP for 2-sided Hyp I)

		$$X \sim Bin(n, \theta) $$

$0 < \theta < 1$\\
$0< \alpha< 1$\\

		$$H_0: \theta = .5 \quad H_1: \theta \neq .5 $$

	First consider, $H_1^+: \theta > .5$. By Theorem 3.2, 

			$$\phi_+ = \left\{ \begin{array}{ll}
				1 & x > c_+\\
				\gamma_+ & x = c_+\\
				0 & x < c_+

			\end{array}\right. $$

with $\E_{\theta = .5} (\phi_+) = \alpha$ is UMP-$\alpha$. 


Can similarly, set up $\phi_-$ for testing $H_1^-: \theta < .5$. 

Suppose that $\phi_0$ is UMP - $\alpha$ for given hypothesis. Then $\phi_0$ is UMP0$\alpha$ for $H_0$ vs $H_1^+$. 


Consequently, 

		$$\E_\theta(\phi_0) = \E_\theta(\phi_+) $$

GET NOTES

\end{example}


\textbf{Friday March 24}\\

\subsection{Uniformly Most Powerful Unbiased Test (UMPU)}

		$$H_0: \mathcal{P}_0 \quad H_1: \mathcal{P}_1 $$


\begin{definition}
	A test, $\phi$, is unbiased for above hypothesis if $\forall P_0 \in \mathcal{P}_0, P_1 \in \mathcal{P}_1$, 

			$$\E_{P_0} \phi(x) \leq \E_{P_1} \phi(x) $$

	So rejectiong probability is greater if $P \in \mathcal{P}_1$.  That is, the probability of rejeciton is higher when you should reject than when you shouldn't. 


\end{definition}

\begin{definition}
	A test, $\phi$, is called UMPU of size $\alpha$ (UMPU-$\alpha$) if 

			\begin{enumerate}
				\item It has size $\alpha$
				\item For any size-$\alpha$ unbiased $\psi$, we have

						$$\beta_\phi (P) \geq \beta_\psi (P) \quad \forall P \in \mathcal{P}_0$$ 
			\end{enumerate}
\end{definition}


Several simple facts. \\


Fact 1.  UMP-$\alpha$ test is always unibased. 

	\begin{proof}
		Suppose $\phi$ is UMP-$\alpha$. Let $\psi(x) = \alpha$. (Here we are CWIST-ing, comparing with stupid test.)

		Since $\phi$ is UMP-$\alpha$, $\psi$ is a tset of size $\alpha. $

				$$\E_P  \phi(x) \geq \alpha \quad \forall P \in \mathcal{P}_1 $$

		but for $\alpha = \sup_{P \in \mathcal{P}_0} \E_P \phi(x)$, 

				$$\E_{P_1} \phi \geq \E_{P_0} \phi \quad \forall P_i \in \mathcal{P}_i, \quad i = \{0, 1\} $$

	\end{proof}

Fact 2. UMPU-$\alpha$ test is always unbaised, CWIST take $\psi(x) = \alpha$. Then we have that 

		$$\E_P \psi(x) \equiv \alpha \quad \forall P $$

So we have that $\psi $ is unbiased of size $\alpha$. 

So 		

		$$\E_P \phi(x) \geq \E_P \psi(x) = \alpha \quad \forall P \in \mathcal{P}_1 $$ 

But 

		$$\E_P \phi(x) \leq \alpha \quad \forall  P \in \mathcal{P}_0$$

So $\phi$ is unbiased. \\


Fact 3. A UMP-$\alpha$ test is always a UMPU-$\alpha$ test. (one is larger class/group)

\textbf{Proposition 3.2} Suppose on $\mathcal{P} = \mathcal{P}_0 \cup \mathcal{P}_1$ is defined a metric, say P, such that

		$$P \mapsto \E_P \phi(x), \mathcal{P} \rightarrow \mathbb{R} $$

is continuous with respect to  P. 

If $\phi$ is size - $\alpha$ unbiased, then

		$$\E_P \phi(x) \equiv \alpha \quad \forall P \in \bar{\mathcal{P}_0} \cap \bar{\mathcal{P}_1} $$

	\begin{proof}
		Let $P \in \bar{\mathcal{P}_0} \cap \bar{\mathcal{P}_1}$ Because $P \in \bar{\mathcal{P}_0}$, there exists $\{P_k\} \subseteq \mathcal{P}_0$ such that

				$$\rho(P_k, P) \rightarrow 0, \quad k \rightarrow \infty $$

		which implies that

				$$\E_{P_k} \phi(x) \rightarrow \E_p \phi(x) $$

		Note that the LHS is less than $\alpha$ since $\phi$ is unbiased. This implies that

				$$ \E_p \phi(x) \leq \alpha$$

		and similarly

				$$\E_p \phi(x) \geq \alpha $$

		so, 

				$$ \E_p \phi(x) = \alpha$$
	\end{proof}

	\subsection{Exponential Family}

	Recall that $\{f_\theta (x): \theta \in \Theta \}$ is an exponential family with repsect to $\mu$ if 

			$$f_\theta(x) = \frac{\exp{\eta(\theta) Y(x)}}{\int \exp{\eta(\theta) Y(x) d\mu(x)}} $$

	where $\eta = \Theta \rightarrow \Theta$ is one-to-one. 

	Another review, analytic functions:

	\begin{definition}
		A function $f: \mathbb{R} \rightarrow \mathbb{R}$ is analytic on an open set G ison any $x \in G$ there exists a neighborhood $0_x$ such that for all $x' in 0_x$, 

				$$f(x') = \sum^\infty_{n = 0} a_n (x' - x)^n $$

		which generalizes a polynomial. 
	\end{definition}

\begin{theorem}
	If f is analytic, on G and $\{x: f(x) = 0 \}$ has a limit point in G then $f(x) = 0$ on G. 
\end{theorem}

\begin{definition}
	A point x is a limit point of set A, if every neighborhood of x contains a point in A that is not x itself. 
\end{definition}

Another review, 

\textbf{Lemma 3.3} Suppose $g(x) e^{\theta x}$ is integrable with repsect to $\mu$ for all $\theta \in \Theta$. Then, in the interior of $\theta$, 

		$$\theta \mapsto \int e^{\theta x} d\mu(x) $$

Futhermore, the derivatives of all order can be taken inside in integral. 


Fact: For one parameter exponential family, 

		$$\dot{\beta}_\psi (\theta) = \Cov_\theta (Y(X), \psi(X)) $$

\begin{proof}
 	
GET NOTES 

 \end{proof} 
 
\textbf{Monday March 27}\\

\subsection{Generlized Neyman Pearson Lemma}

Recall island example for intuition. 

\textbf{Lemma 3.4} Suppose $f_1, \dots, f_m, f_{m + 1}$ are funcitons measureable and integrable with respect to $\mu$. Let $C_1, \dots, C_m \in \mathbb{R}$, and

		$$\phi^*(x) = \left\{ \begin{array}{ll}
			1 & f_{m+1} > C_1 f+1 + \cdots + C_m f_m\\
			0 & f_{m+1} < C_1 f+1 + \cdots + C_m f_m\\
		\end{array}\right. $$

Then 

	\begin{enumerate}
		\item For any $\phi$ satisfying

				$$\int \phi f_i d\mu = \int \phi^* f_i d\mu \quad i = 1, \dots, m $$ 

		we have

				$$\int \phi^* f_{m+1} d\mu \geq \int \phi f_{m + 1} d\mu$$

		\item If $C_1 \geq 0, C_m \geq 0$ then for any $\phi$ satisfying

				$$\int \phi f_i d\mu \leq \int \phi^* f_i d\mu \quad i = 1, \dots, m $$
		we have

				$$\int \phi^* f_{m+1} d\mu \geq \int \phi f_{m+1} d\mu $$
	\end{enumerate}

\begin{proof}
Consider 	

		$$(\phi^* - \phi)(f_{m+1} - C_1 f_1 - \cdots - c_m f_m) $$

Not if second term is greater than zero, then the first must be greater than or equal to zero. Similarly, if second term is less than zero, then so must the first be less than or equal to. 

Thefore,

GET NOTES - lots of stupid arrows across the board
	
\end{proof}

\subsection{UMP-Test for Test III}

		$$H_0: \theta \leq t_1, \theta \geq t_2 \quad H_1: t_1 < \theta < \theta_2$$


We will show that $\phi$ of the form (\_.-.\_) is UMP-$\alpha$ for hyposthesis III above. 

		$$\phi(x) = \left\{ \begin{array}{ll}
			1 & t_1 < Y(x) < t_2\\
			\gamma_i & Y(x) = t_i \quad i = 1, 2\\
			0 & Y(x) < t_1 \text{ or } Y(x) > t_2
		\end{array} \right. $$


\begin{theorem}[3.6]
	Let X be a random variable with density 

			$$f_\theta (x) = c(\theta)e^{\theta Y(x)} $$

	Then, 

	\begin{enumerate}
		\item for any $\theta_1 < \theta_2$, $0 < \alpha < 1$, there exists $0 < t_1 < t_2 < \infty, 0 \leq \gamma_1, \gamma_2 \leq 1$ such that this form of $\phi$ (\_.-.\_) satisfies, 

			$$\E_{\theta_1} \phi(x) = \alpha $$
			$$\E_{\theta_2} \phi(x) = \alpha $$

		\item Also, for any $\psi$ with 

			$$\E_{\theta_1} \psi \leq \E_{\theta_1}\phi, \E_{\theta_2} \psi \leq \E_{\theta_2}\phi $$

	we have that for $\theta_1 < \theta< \theta_2$

			$$\E_\theta \phi(x) \geq \E_{\theta}\psi $$

		\item Also, also, let $\phi$ be (\_.-.\_) in part 1. Then for any $\psi$ such that

			$$\E_{\theta_1} \psi(x) = \E_{\theta_2} \psi(x) = \alpha $$

	we have that for all $\theta < \theta_1$ or $\theta < \theta_2$,

			$$\E_\theta \phi(x) \leq \E_\theta \psi(x) $$

		\item	Also, also, also, This $\phi$ is UMP-$\alpha$ for III.

	\end{enumerate}
	
	
\end{theorem}


\begin{proof}
	\begin{enumerate}
		\item Proof is similar to existence of \_.- such taht

				$$\E_{\theta_0} (\_.-) = \alpha $$

		using intermediate value theorem for P. This is tedius, so omitted. 
		\item Idea: for any $\theta \in (\theta_1, \theta_2)$, $\phi = \_.-$ is of the GNP form. 

		GET NOTES


		\item \textbf{Wednesday March 29}\\

			GET NOTES
		\item  
	\end{enumerate}
\end{proof}
 
 \subsection{UMPU-$\alpha$ Test for II, III}

 First, we will address hypothesis II. 

 \begin{theorem}[3.7]
 	Suppose X has density 

 			$$\frac{e^{\theta Y(x)}}{\int e^{\theta Y(x)} d\mu(x)} = c(\theta e^{\theta Y(x)}) $$

 	and we know that $\theta_1 < \theta_1$, and $0 < \alpha < 1$. Then

 			\begin{enumerate}
 				\item there exists test of the form 

 						$$\phi(x) = \left\{\begin{array}{ll}
 							1 & Y(x) < t_1 \text{ or } Y(x) > t_2\\
 							\gamma_1 & Y(x) = t_1\\
 							\gamma_2 & Y(x) = t_2 \\
 							0 & t_1 < Y(x) < t_2
 						\end{array} \right. $$

 			where $-\infty < t_1 < t_2 < \infty$ and $0 \leq \gamma_1, \gamma_2 < 1$ such that

 					$$\E_{\theta_1} \phi = \E_{\theta_2} \phi = \alpha  $$


				\item this test is UMPU-$\alpha$ test for form II

						$$H_0: \theta_1 \leq \theta \leq \theta_2 \quad H_1: \theta< \theta_1 \text{ or } \theta > \theta_2$$

 			\end{enumerate}
 \end{theorem}

\begin{proof}
	\begin{enumerate}
		
		\item Get notes. 

		\item 

	\end{enumerate}
\end{proof}

Theorems 3.6 and 3.7 (the previous two) are logically equivalent. But how come we get UMP for 3.6, but UMPU for 3.7?

For hypothesis II, we need to exclude the test such as

PHOTO

that is why we consider unbiased test. 

For hypothesis I, these test are automatically excluded. 

Only difference. 



Now we may consider hyposthesis I. 

\begin{theorem}[3.8]
	Suppose that X has density,

		$$\frac{e^{\theta Y(x)}}{\int e^{\theta Y(x)} d\mu(x)} = c(\theta) e^{\theta Y(x)} $$

 	Let $\theta_0 \in \Theta^0$ interior of $\Theta$ and $0 < \alpha < 1$. 

 	Then, 

 		\begin{enumerate}
 			\item there exists $\phi = -.\_.-$ such that

 					$$\E_{\theta_0} \phi(x) = \alpha $$  
 					$$\E_{\theta_0} [Y(x) \phi(x)] = \alpha \E{\theta_0} (Y(x))$$

 			\begin{remark}
 				Note that unbiasedness for hyp I implies that (SEE PHOTO)

 						$$\beta_\phi(\theta_0) = \alpha $$
 						$$\dot{\beta}_{\phi} (\theta_0) = 0 $$

 				But recall that 

 						$$\dot{\beta}_\phi (\theta_0) = \Cov_{\theta_0} (Y(x), \phi(x)) $$

 				which implies that

 						$$\E_{\theta_0} (Y\phi) = \alpha \E_{\theta_0} Y $$
 			\end{remark}
 			\item  $\phi$ is UMPU-$\alpha$ for testing hypothesis  I.
 		\end{enumerate}

\end{theorem}

\begin{proof}
	\begin{enumerate}
		
		\item  Part 1 is omitted.
		\item  GET NOTES
	\end{enumerate}
\end{proof}


\textbf{Friday March 31}\\

FINISHED PROOF. 

\begin{example}
	Let $X_1, \dots, X_n$, iid, $N(\theta, 1)$. We want to test hypotheses I, II, and III. 

	by elementary calculation (BEC) 

			$$f_\theta (X_1, \dots, X_n) = c(\theta) \exp\{\theta \sum^n_{i = 1} X_i \} $$

	So, let $Y(X_1, \dots, X_n) = \sum^n_{i=1} X_i$. 

	\textbf{Testing hypothesis III}\\

	By Theorem 3.6, we have that the test, $\phi$, for hypothesis III is (\_.-.\_) such that

			$$\E_{\theta_1} (\_.-.\_) = \E_{\theta_2} (\_.-.\_) = \alpha $$

	But, 

			$$Y \sim N(n\theta, n) $$

	Note that here, (\_.-.\_) is approximately the same as (\_-\_).

	We have that 

			$$P_{\theta_1} (t_1 < Y < t_2) = \alpha $$

			$$P_{\theta_2} (t_1< Y < t_2) = \alpha $$


	Again, by elementary calculations, we know that above are equivalent to 

		$$ \left\{ \begin{array}{l}
			\Phi(\frac{t_2 - n \theta_1}{\sqrt{n}}) - \Phi(\frac{t_1 - n \theta_1}{\sqrt{n}}) = \alpha\\
			\Phi(\frac{t_2 - n \theta_1}{\sqrt{n}}) - \Phi(\frac{t_1 - n\theta_2}{\sqrt{n}}) = \alpha\\

		\end{array} \right.$$


	where $\Phi$ is the CDF of a Standard Normal distribution. 

	In principe, we can always solve these two variables equation numerically (such as Newton Raphson). However, in this, the two equations actually reducing to one equation by symmetry. 

	So, 

			$$t_1 = n \frac{\theta_1 + \theta_2}{2} - t_0 $$
			$$t_2 = n \frac{\theta_1 + \theta_2}{2} + t_0 $$

	And thus, we only have to solve this equation, 

			$$\Phi (\frac{n \frac{\theta_1 + \theta_2}{2} + t_0 - n\theta_1}{\sqrt{n}}) - \Phi (\frac{n \frac{\theta_1 + \theta_2}{2} - t_0 - n\theta_1}{\sqrt{n}}) $$

	which reduces to,

			$$\Phi \left(\frac{n (\theta_1 + \theta_2) + 2t_0 }{2\sqrt{n}}\right) - \Phi \left(\frac{n (\theta_1 + \theta_2) - 2t_0 }{2\sqrt{n}}\right) $$

\textbf{Testing Hypothesis II}\\

$\phi$ = -.\_.- = -\_-

We have that 

			$$P_{\theta_1} (Y < t_1) + P_{\theta_1} (Y > t_2) = \alpha $$

			$$P_{\theta_2} (Y < t_1) + P_{\theta_2} (Y > t_2) = \alpha $$


	Again, by elementary calculations, we know that above are equivalent to 

		$$ \left\{ \begin{array}{l}
			\Phi(\frac{t_1 - n \theta_1}{\sqrt{n}}) +1 - \Phi(\frac{t_2 - n \theta_1}{\sqrt{n}}) = \alpha\\
			\Phi(\frac{t_1 - n \theta_1}{\sqrt{n}}) +1 - \Phi(\frac{t_1 - n\theta_2}{\sqrt{n}}) = \alpha\\

		\end{array} \right.$$

	pHOTOS

	So, 

			$$t_1 = n(\frac{\theta_1 + \theta_2}{2}) + t_0 $$
			$$t_2 = n(\frac{\theta_1 + \theta_2}{2}) - t_0 $$

	And thus we only need to solve, 


		$$\Phi \left(\frac{n (\theta_2 - \theta_1) - 2t_0 }{2\sqrt{n}}\right) + 1 - \Phi \left(\frac{n (\theta_2 - \theta_1) + 2t_0 }{2\sqrt{n}}\right) $$

FINISH FOR HYP III


\end{example}


\textbf{Monday April 3}\\

Need to finish for Hyp III. 

Our test is $\phi$ = -.\_.- which has to stisfy


		$$ \Phi (\frac{t_1 - n\theta_0}{\sqrt(n)}) + 1 - \Phi (\frac{t_2 - n \theta_0}{\sqrt(n)}) = \alpha$$

		$$\Cov _{\theta_0} Y, \phi(y)) = 0 $$


Now, hope to solve $(t_1, t_2)$ but we may reduce to equation by symettry

		$$\Cov _{\theta_0} (Y, \phi(Y)) = \E _{\theta_0} ((Y - n\theta_0) \phi(Y)) = 0 $$

We want the product of thre functions to integrate to zero which would imply that $\phi(Y)$ is symetric about $n\theta_0$.

So we take 

		$$t_1 = n\theta_0 - t_0 $$
		$$t_2 = n\theta_0 + t_0 $$

Then we only need to satifsy 

		$$\Phi(\frac{-t_0}{\sqrt(n)}) + 1 - \Phi(\frac{t_0}{\sqrt(n)}) = \alpha $$

We can solve

		$$t_0 = \sqrt(n) \Phi^{-1} (1 - \frac{\alpha}{2}) $$

So our UMPU-$\alpha$ test is 

PHOTO


\chapter{Testing Hypotheses in the Presense of Nuisance Pamameters}\index{Testing Hypotheses in the Presense of Nuisance Pamameters}


$\Theta \subseteq \mathbb{R}^p$\\
$\mathscr{P} = \{P_\theta: \theta \in \Theta \}$\\
$\{\Theta_0, \Theta_1\}$ partition of $\Theta$\\

		$$H_0: \theta \in \Theta_0 \quad H_1: \theta \in \Theta_1 $$

Want the form of the UMPU-test (in this case we don't have a general UMP form). 

More sepecifically	interested in testing one parameter (without loss of generality assume $\theta_1$ but we will treat $\theta_{2:p}$ as unknown).

That is

		$$H_0: \theta_1 \in A \quad H_1: \theta_1 \notin A $$

For example, 

		$$H_0: \theta_1 \in a \quad H_1: \theta \notin a $$

where $\theta = \begin{pmatrix}
	\theta_1 \\
	\theta_2
\end{pmatrix}$\\

We may call $\theta_1$ the parameter of interest, and the others the nuisance parameters. 

\begin{example}
	$$X_1, \dots, X_n \sim N(\mu, \sigma^2) $$

where $\sigma^2$ is unknown and we want to test
	
		$$H_0: \mu = 0 \quad H_1: \mu \neq 0 $$


\end{example}

These are the types of problems concerned in this chapter


\section{Unbiased and Similar Tests}\index{Unbiased and Similar Tests}

Recall that if $\E_\theta \phi$ is in $\theta$ then compact on $\bar{\mathscr{P}_0} \cap \bar{\mathscr{P}_1}$. Specialized to the current setting this result is: 

\textbf{Proposition 4.1}: If $\theta \mapsto \E_\theta \phi(x)$ is continuous and $\phi$ is unbaised of size $\alpha$ then 

		$$\E_\theta \phi(x) = \alpha \quad \forall \theta \in \bar{\mathscr{\Theta}}_0 \cap \bar{\mathscr{\Theta}}_1 = \Theta_B $$


\begin{definition}
	A test $\phi$ satisfying $\beta_\phi(\theta) = \alpha \quad \forall \theta, \theta \in \Theta_B$ is called an $\alpha$ similar test. 

	A test $\phi$ is called UMP Similar-$\alpha$ test (or UMPS-$\alpha$ test) if

			\begin{enumerate}
			 	\item $\phi$  is $\alpha$ similar
			 	\item if $\psi$ is $\alpha$-similar then 

			 			$$\E_\theta \phi \geq \E_\theta \psi \quad \forall \theta \in \Theta $$ 
			 \end{enumerate} 

	Let $\mathscr{U}_\alpha$ be the collection of all unbiased test of size $\alpha$. Then we may say that $\mathscr{S}$ represent the collection of all $\alpha$-similar tests. 
\end{definition}


By Propsition 4.1, under the condition that $\E_\theta \phi(x)$ continuous in $\theta$, 

		$$\mathscr{U}_\alpha \subseteq \mathscr{S}_\alpha $$


\begin{theorem}
	Suppose that $\beta_\phi (\theta)$ is continuous in $\theta$ for all $\phi(x)$. If $\phi$ is UMPS-$\alpha$ and also $\phi$ has size $\alpha$ then $\phi$ is UMPU-$\alpha$.
\end{theorem}

We need the notion of sufficiency, completeness, and later ancillary. Recall that T is sufficient if $P_\theta(\cdot| T)$ doesnt depend on $\theta$. Also that T is complete if $\E_\theta g(T) = 0 \quad \forall \theta$. 


\textbf{Wednesday April 5}\\

Sufficiency, $\{P_\theta, \theta \in \Theta \}, (\Omega, \mathcal{F})$. A statistic T, measurable on $\mathcal{F}$, is sufficient if for all $A \in \mathcal{F}$, $P_\theta(A||T)$ is the same for all $\theta \in \Theta$. 

We say that T is sufficient on $G \subseteq \Theta$ if $P_\theta (A||T)$ is the same for all $\theta \in G$. 

This is important when we need to talk about sufficiency for nuisance parameters. 


Completeness. A statistic T, measurable on $\mathcal{F}$, is complete on $G \subseteq \Theta$ if for any $g \textcircled{m} T$, 

		$$\E_\theta g(T) = 0 \quad \forall \theta \in G $$

which implies that

		$$g(T) = 0 \quad a.s. P_\theta \quad \forall \theta \in G $$

T is bddly (bounded?) complete if the above holds for all bounded g measurable T. 


\begin{remark}
	In the presence of nuisance parameter, UMP may not exist even for one-sided hypothesis. So we'll always talk about UMPU test here. 
\end{remark}

Intuition about the theory:

Suppose we want to test 

		$$H_0: \theta_1 = a \quad H_1: \theta_1 \neq a $$

Suppose there exists a statistic T that is sufficient for $\theta_{2:p}$ for each fixed $\theta$. Then the distribution of $P_\theta(X|T)$ is only a function of $\theta_1$. 

		$$P_\theta (X \in A |T ) = P_{\theta_1} (X \in A | T) $$ 

Now we turned k-parameter problem for ($\theta$) into one-parameter problem $(\theta_1$). That is, if we condition on statistic sufficient for nuisance parameter, then we get rid of the nuisance parameter. 

Futhermore, suppose that the distribution of X|T ~ Exponential Family. Then we may use results to say that the tests above (the dah dah dah tests) are best for hypotheses I, II, III. 

But the power that we maximized is the conditional probability, $P_{\theta_1} (X | T)$, while what we want is $P_\theta (X)$. 

This is why we need completeness. 

First, the notion of Neyman - Structure

\begin{definition}
	A test, $\phi(X)$ has $\alpha$-Neyman Structure with respect to a statistic, T, if 

			$$\E_\theta [\phi(X)|T] = \alpha \text{ (a.s. } P_\theta) \quad \forall \theta \in \Theta_B $$

	where $\Theta_B = \bar{\Theta}_0 \cap \bar{\Theta}_1$. 

	We may let $N_\alpha (T)$ denote the collection of all $\phi$ with Neyman Structure with respect to T. 

	Note that $N_\alpha (T) \subseteq \mathscr{S}_\alpha$ because, 

			\begin{align*}
				\phi \in N_\alpha(T) &\Rightarrow \E_\theta [\phi(X) | T] = \alpha\\
					&\Rightarrow \E_\theta \phi(X) = \alpha
			\end{align*}


\end{definition}

\begin{theorem}[4.2]
	If T sufficient and bddly complete for $\theta \in \Theta_B$, then 

			$$\mathscr{S} \subseteq N_\alpha (T)$$
\end{theorem}

\begin{proof}
	Suppose $\phi \in \mathscr{S}_\alpha$. Then, 

			$$\E_\theta \phi(X) = \alpha \quad \forall \theta \in \Theta_B$$

			$$\E_\theta [\E_{\theta} (\phi(X) |T) - \alpha]= 0 $$

			FINISH PROOF
\end{proof}

We can make a sligtly more general structure, 

\textbf{Corollary 4.1} Suppose that

		$$\Theta_B = \bigcup_{r = 1}^s \Theta_B^{(r)} $$

and T is SBC on each $\Theta^{(r)}$. Then 

		$$\mathscr{S}_\alpha \subset \mathscr{N}_\alpha (T) $$

\begin{proof}
	
\end{proof}

Recall that, 

		$$\mathscr{U}_\alpha \subseteq \mathscr{S}_\alpha \supseteq \mathscr{N}_\alpha (T) $$

Where first requires continuous power condition. 

If we have that condition that T SCB either on $\Theta_B$ or each $\Theta_B^{(r)}$ then we get, 

		$$\mathscr{U}_\alpha \subseteq \mathscr{S}_\alpha \subseteq \mathscr{N}_\alpha (T) $$

In either case, under SBC, 

		$$\mathscr{U}_\alpha \subseteq \mathscr{N}_\alpha (T) $$

\section{Sufficient and Complete}

For components of $\theta$ under exponential family, suppose that 

		$$X \sim \frac{e^{\theta^T t_0 (x)}}{\int e^{\theta^T t_0 (x)} d\mu_0} $$

and that $X_1, \dots, X_n \sim X$. 
	


\textbf{Friday April 7}\\


Let 

		$$X \sim \frac{e^{\theta^T t_0(x)}}{\int e^{\theta^T t_0(x)} d\mu_0 (X)} $$

		$$(X_1, \dots, X_n) \sim \frac{e^{\theta^T t(x_1, \dots, x_n)}}{\int e^{\theta^T t(x_1, \dots, x_n)} d\mu_0 (x_1, \dots, x_n)} $$


\begin{theorem}
	Supppose we have $X_1, \dots, X_n$ from a exponentail family. Then, $t(X_1, \dots, X_n)$ are complete and sufficient for $\theta$. 

			$$t(X_1, \dots, X_n) = \sum^n_{i=1} t_0(X_i) $$
\end{theorem}

\begin{proof}
	We get sufficiency form Fisher's factorization theorem. Must show completeness. 

	FINISH PROOF
\end{proof}
 
\textbf{Exponential Family}\\

Marginal distribution is exponential family, so must conditional distribution be. 

"Closed" under marginalization and conditioning.

Let (U, V) be a pair of random vectors defined on ($\Omega_U x \Omega_V, \mathcal{F}_U x \mathcal{F}_V$) for any distribution, $P_{UV}$. Let $P_{U}$ be the marginal distribution and $P_{V|U}$ be the conditional distribution. 

\textbf{Lemma 4.1} Let Q be a probability measure on ($\Omega_U x \Omega_V, \mathcal{F}_U x \mathcal{F}_V$) and P is another probability measure defined by 

		$$dP = a(U) b(V) dQ $$

where $a(\cdot) \geq 0, b(\cdot) \geq 0$ . 

		$$\int a(U) b(V) dQ(U,V) = 1 $$

Then 

		\begin{enumerate}
			\item $dP_U(u) = a(U) [\int b(V) dQ_{V|U} (v|u)]dQ_U(u)$ 
			\item $dP_{V|U}(v|u) = \frac{b(V) dQ_{V|U} (v|u)}{\int b(V') dQ_{V|U} (v'|u) }$
		\end{enumerate}


\begin{proof}
	\begin{enumerate}
		\item FINISH PROOF
		\item  
	\end{enumerate}
\end{proof}


Now let 

		$$t(\underline{X}) = \begin{pmatrix}
			U(\underline{X})\\
			V(\underline{X})
		\end{pmatrix} $$

and 

		$$\theta = \begin{pmatrix}
			\eta\\
			\xi
		\end{pmatrix} $$

where $\theta^T t(X) = \eta^T U(\underline{X}) + \xi^T V(\underline{X})$

\begin{theorem}
	Suppose

			$$\underline{X} \sim \frac{e^{\theta^T t(\underline{X})}}{\int e^{\theta^T t(\underline{X})} d\mu(\underline{X})} $$

	Then 

	\begin{enumerate}
		\item For each fixed $\xi$ there exists a measure $\mu_\xi$ on $(\Omega_U, \mathcal{F}_U)$ such that

				$$\frac{d P_U}{d\mu_\xi} = \frac{e^{\eta^T \eta}}{\int e^{\eta^T \eta} d\mu(U)} $$
		\item For each u there exists a measure on $(\Omega_\nu, \mathcal{F}_\nu)$ such that

				$$\frac{d P_{V|U}(v|u)}{d \mu_u} = \frac{e^{\xi^TV}}{\int e^{\xi^T V} d\mu_u} $$
	\end{enumerate}


\end{theorem}

\textbf{Monday April 10}\\

\begin{proof}
	\begin{enumerate}
		\item
		\item  
	\end{enumerate}
\end{proof}

\textbf{Corollary 4.2} Suppose that

		$$X_1, \dots, X_n) \sim \frac{e^{\theta^T t(X_{1:n})}}{\int e^{\theta^T t(X_{1:n})} d\mu(X_{1:n})} $$

then for each fixed $\theta_1 = a$, $T_{2:p} = t_{2:p}(x_1, \dots, x_n)$ is complete sufficient for 

		$$\Theta_{(a)} = \{\theta: \theta_1 \leq a \} $$

\section{UMPU test in the presence of Nusience Parameter}\index{UMPU with nusience parameter}

\textbf{Lemma 4.2} Suppose that $T = t(X_1, \dots, X_n)$ is sufficient for $\Theta$. Then, for any $\phi(X_1, \dots, X_n)$ there exists a 

		$$\psi(t(x_1, \dots, x_n)) = \psi_0 t(x_1, \dots, x_n) $$

such that

		$$\beta_{\psi_{0t}}(\theta) = \beta_{\phi}(\theta)  \quad \forall \theta $$


\begin{proof}
	

\end{proof}

\begin{theorem}[4.6]
	Suppose we have $X_1, \dots, X_n$ are iid ~ Exp Family. Then we have

			$$\phi_0(z) = (\_.-|t_{2:p}) $$

	such that 

			$$\E_n (\phi_0(T) | T_{2:p}) = \alpha $$

	is UMPU-$\alpha$ for testing $H_0: \theta_1 \leq a \quad H_1: \theta_1 > a$
\end{theorem}

\begin{theorem}[4.7]
	We we have $X_1, \dots, X_n$ is distributed as Exponential Family then for testing:

		\begin{enumerate}
			\item Hypothesis III: $H_0: \theta_1 \in (a, b)^c \quad H_1: \theta_1 \in (a, b)$

			The test $\phi_0(T) = \_.-.\_|t_{2:p}$ with $\E_a(\phi_0|t_{2:p}) = \E_a (\phi_0|t_{2:p}) = \alpha$ is UMPU-$\alpha$ test. 
			
			\item Hypothesis II: $H_0: \theta_1 \in [a,b] \quad H_1: \theta_1 \in [a,b]^c$

			The test $\phi_0(T) = -.\_.-|t_{2:p}$ with $\E_a(\phi|t_{2:p}) = \E_a (\phi|t_{2:p}) = \alpha$ is UMPU-$\alpha$ test.

			\item Hypothesis I: $H_0: \theta_1 =a \quad H_1: \theta_1 \neq a$

			The test $\phi_0(T) = -.\_.-|t_{2:p}$ with $\E_a(\phi|t_{2:p}) = \E_a (\phi|t_{2:p}) = \alpha$ and $\E_a (\phi_0 (T) T_1| t_{2:p}) = \alpha \E_a(T_1 |t_{2:p})$ is UMPU-$\alpha$ test. 
		\end{enumerate}
\end{theorem}


\section{Using Basu's Theorem to construct UMPU in presence of nuisance parameters}\index{Using Basu's Theorem to construct UMPU in presence of nuisance parameters}

A technique simplifying UMPU construction so we need the thresheholds for the different hypothesis test forms, $t_1, t_2, \gamma_1, \gamma_2$. 

These are determined by conditional probability such as 

		$$\E_a (\phi_0|t_{2:p}) = \alpha $$
In special cases, these can be reduced to conditional probabilities which leads to all of the famous tests, student -t, Pearsons test, F-test...

\begin{theorem}[Basu's Theorem]
	Suppose $X \sim P_\theta$, T(x) is sufficient and bddly complete for $\theta$; U(x) is ancillary for $\theta$. (This means that the distrubiton of U(x) doesn't depend on $\theta$. ) Then $T(x) \indep U(x)$.
\end{theorem}

\begin{proof}
	
\end{proof}


But how do we find ancillary statistics?

One -way: by invariance. 

Let $\mathcal{G}$ be a group of transformations $\Omega \rightarrow \Omega$. Let $\mathcal{P}$ be a family of distrubitons on $(\Omega, \mathcal{F})$. For each $g \in \mathcal{G}$
it induces a transformation, $\tilde{g}: \mathcal{P} \rightarrow \mathcal{P}$, $p \mapsto P_0 g^{-1}$, that is to say $\tilde{g}(P) = P \circ g^{-1}$. It is easy to show that $\tilde{\mathcal{G}} = \{\tilde{g} : g \in \mathcal{G} \}$ is a group of transformations. 

A group is transitive if for any $p \in \mathcal{P}$, $\{\tilde{g}(p):\tilde{g} \in \tilde{\mathcal{G}} \} = \mathcal{P}$. 

A statistic V is ancillary if the distribution of V under P is the same for all $p \in P$. 


\textbf{14 April 2017}\\

\begin{theorem}
	Suppose V is a random element on $(\Omega, \mathcal{F})$. $\mathcal{P}$ is a family of probability measure on $(\Omega, \mathcal{F})$; $\mathcal{G}$ is a group of transformations $\Omega \rightarrow \Omega$. 

	Suppose that 

			\begin{enumerate}
				\item $\mathcal{P}$ is invarience under $\mathcal{G}$
				\item V is invarient under $\mathcal{G}$
				\item $\tilde{\mathcal{G}}$ is transitive 
			\end{enumerate}

	then V is ancillary. 
\end{theorem}

\begin{proof}
	
\end{proof}

\textbf{Lemma 4.3} Suppose that $(\Omega, \mathcal{F}, P)$ is probability space, $(\Omega_v, \mathcal{F}_v)$ is measure space.

		$$v: \Omega \rightarrow \Omega_v \textcircled{m} \mathcal{F}/\mathcal{F}_v $$
		$$g: \Omega \rightarrow \Omega $$

then $(p \circ g^{-1} \circ v^{-1}(A) = P_0 (v \circ g) ^{-1} (A)$. 

\textbf{17 April 2017}\\





 %----------------------------------------------------------------------------------------
% %	CHAPTER 5
% %----------------------------------------------------------------------------------------

% \chapterimage{chappter_head_1.pdf} % Chapter heading image

% \chapter{Convergence in Probability/Limit Theorem}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

% \chapter*{Bibliography}
% \addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}}
% \section*{Books}
% \addcontentsline{toc}{section}{Books}
% \printbibliography[heading=bibempty,type=book]
% \section*{Articles}
% \addcontentsline{toc}{section}{Articles}
% \printbibliography[heading=bibempty,type=article]

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage
\phantomsection
\setlength{\columnsep}{0.75cm}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}}
% \printindex

%----------------------------------------------------------------------------------------

\end{document}